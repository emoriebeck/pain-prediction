[["index.html", "Idiographic prediction of loneliness and procrastination Chapter 1 Workspace 1.1 Packages 1.2 Directory Path 1.3 Codebook 1.4 Demographics", " Idiographic prediction of loneliness and procrastination Emorie D Beck 2021-06-21 Chapter 1 Workspace 1.1 Packages library(knitr) # creating tables library(kableExtra) # formatting and exporting tables library(rio) # importing html library(readxl) # read excel codebooks and documentation library(psych) # biscuit / biscwit library(glmnet) # elastic net regression library(glmnetUtils) # extension of basic elastic net with CV library(caret) # train and test for random forest library(vip) # variable importance library(Amelia) # multiple imputation (of time series) library(lubridate) # date wrangling library(gtable) # ggplot friendly tables library(grid) # ggplot friendly table rendering library(gridExtra) # more helpful ggplot friendly table updates library(plyr) # data wranging library(tidyverse) # data wrangling library(ggdist) # distributional plots library(ggridges) # more distributional plots library(cowplot) # flexibly arrange multiple ggplot objects library(tidymodels) # tidy model workflow and selection # library(modeltime) # tidy models for time series library(furrr) # mapping many models in parallel 1.2 Directory Path res_path &lt;- &quot;https://github.com/emoriebeck/behavior-prediction/raw/main&quot; local_path &lt;- &quot;~/Box/network/other projects/idio prediction&quot; 1.3 Codebook Each study has a separate codebook indexing matching, covariate, personality, and outcome variables. Moreover, these codebooks contain information about the original scale of the variable, any recoding of the variable (including binarizing outcomes, changing the scale, and removing missing data), reverse coding of scale variables, categories, etc. # list of all codebook sheets ipcs_codebook &lt;- import(file = sprintf(&quot;%s/01-codebooks/codebook.xlsx&quot;, res_path), which = 2) %&gt;% as_tibble() ipcs_codebook ## # A tibble: 98 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait long_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BFI-2 E scblty Sociabil… Was outgoing,… Likert scale 1 to 5… 1. Is outgoing, s… E1 no Extravers… Sociabil… ## 2 BFI-2 E scblty Sociabil… Was talkative. Likert scale 1 to 5… 46. Is talkative. E2 no Extravers… Sociabil… ## 3 BFI-2 E scblty Sociabil… Tended to be … Likert scale 1 to 5… 16r. Tends to be qu… E3 yes Extravers… Sociabil… ## 4 BFI-2 E scblty Sociabil… Was sometimes… Likert scale 1 to 5… 31r. Is sometimes s… E4 yes Extravers… Sociabil… ## 5 BFI-2 E assert Assertiv… Had an assert… Likert scale 1 to 5… 6. Has an asserti… E5 no Extravers… Assertiv… ## 6 BFI-2 E assert Assertiv… Was dominant,… Likert scale 1 to 5… 21. Is dominant, a… E6 no Extravers… Assertiv… ## 7 BFI-2 E assert Assertiv… Found it hard… Likert scale 1 to 5… 36r. Finds it hard … E7 yes Extravers… Assertiv… ## 8 BFI-2 E assert Assertiv… Preferred to … Likert scale 1 to 5… 51r. Prefers to hav… E8 yes Extravers… Assertiv… ## 9 BFI-2 E enerL… Energy L… Was full of e… Likert scale 1 to 5… 41. Is full of ene… E9 no Extravers… Energy L… ## 10 BFI-2 E enerL… Energy L… Showed a lot … Likert scale 1 to 5… 56. Shows a lot of… E10 no Extravers… Energy L… ## # … with 88 more rows outcomes &lt;- ipcs_codebook %&gt;% filter(category == &quot;outcome&quot;) %&gt;% select(trait, long_name) ftrs &lt;- import(file = sprintf(&quot;%s/01-codebooks/codebook.xlsx&quot;, res_path), which = 3) %&gt;% as_tibble() 1.3.1 Measures Participants responded to a large battery of trait and ESM measures as part of the larger study. The present study focuses on ESM measures whose use we preregistered. A full list of the collected measures for the study can be found in supplementary codebooks in the online materials on the OSF and GitHub. The measures collected at each wave were identical. ESM measures were used to estimate idiographic personality prediction models. 1.3.1.1 ESM Measures 1.3.1.1.1 Personality Personality was assessed using the full BFI-2 (Soto &amp; John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 “disagree strongly” to 5 “agree strongly.” Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants. ipcs_codebook %&gt;% filter(category == &quot;BFI-2&quot;) ## # A tibble: 60 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait long_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BFI-2 E scblty Sociabil… Was outgoing,… Likert scale 1 to 5… 1. Is outgoing, s… E1 no Extravers… Sociabil… ## 2 BFI-2 E scblty Sociabil… Was talkative. Likert scale 1 to 5… 46. Is talkative. E2 no Extravers… Sociabil… ## 3 BFI-2 E scblty Sociabil… Tended to be … Likert scale 1 to 5… 16r. Tends to be qu… E3 yes Extravers… Sociabil… ## 4 BFI-2 E scblty Sociabil… Was sometimes… Likert scale 1 to 5… 31r. Is sometimes s… E4 yes Extravers… Sociabil… ## 5 BFI-2 E assert Assertiv… Had an assert… Likert scale 1 to 5… 6. Has an asserti… E5 no Extravers… Assertiv… ## 6 BFI-2 E assert Assertiv… Was dominant,… Likert scale 1 to 5… 21. Is dominant, a… E6 no Extravers… Assertiv… ## 7 BFI-2 E assert Assertiv… Found it hard… Likert scale 1 to 5… 36r. Finds it hard … E7 yes Extravers… Assertiv… ## 8 BFI-2 E assert Assertiv… Preferred to … Likert scale 1 to 5… 51r. Prefers to hav… E8 yes Extravers… Assertiv… ## 9 BFI-2 E enerL… Energy L… Was full of e… Likert scale 1 to 5… 41. Is full of ene… E9 no Extravers… Energy L… ## 10 BFI-2 E enerL… Energy L… Showed a lot … Likert scale 1 to 5… 56. Shows a lot of… E10 no Extravers… Energy L… ## # … with 50 more rows 1.3.1.1.2 Affect Items capturing affect were initially pulled from the PANAS-X (Watson &amp; Clark, 1994). In order to reduce redundancy, these were cross-referenced with the BFI-2 and duplicated items (e.g., “excited” were only asked once. Because we were not interested in scale score but in items, we further had research participants examine remaining items and asked them to indicate items that were not relevant to their experience. Finally, we added two “neutral” affect-related terms – goal-directed and purposeful. Each of these were rated on a 1 “disagree strongly” to 5 “agree strongly.” ipcs_codebook %&gt;% filter(category == &quot;Affect&quot;) ## # A tibble: 10 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait long_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Affect angry angry angry Angry Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A1 no Angry Negative ## 2 Affect afraid afraid afraid Afraid Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A3 no Afraid Negative ## 3 Affect happy happy happy Happy Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A5 no Happy Positive ## 4 Affect excited excited excited Excited Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A7 no Excited Positive ## 5 Affect proud proud proud Proud Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A9 no Proud Positive ## 6 Affect guilty guilty guilty Guilty Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A10 no Guilty Negative ## 7 Affect attent… attent… attentive Attentive Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A11 no Attentive Positive ## 8 Affect content content content Content Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A12 no Content Positive ## 9 Affect purpos… purpos… purposef… Purposef… Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A13 no Purposeful Neutral ## 10 Affect goaldir goaldir goaldir Goal-dir… Likert scale 1 to 5; 1 = … &lt;NA&gt; &lt;NA&gt; A14 no Goal-dire… Neutral 1.3.1.1.3 Binary Situations Binary situation indicators were derived by asking undergraduate research assistants to provide list of the common social, academic, and personal situations in which they tended to find themselves. From these, we derived a list of 19 unique situations. Separate items for arguing with or interacting with friends or relatives were composited in overall argument and interaction items. Participants checked a box for each event that occurred in the last hour (1 = occurred, 0 = did not occur). ipcs_codebook %&gt;% filter(category == &quot;sit&quot;) ## # A tibble: 18 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait long_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sit study study study Was studying or … 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_01 no Studying &lt;NA&gt; ## 2 sit argume… argume… argFrnd Had an argument … 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_02 no Argument Argument wi… ## 3 sit argume… argume… argFam Had an argument … 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_03 no Argument Argument wi… ## 4 sit intera… intera… IntFrnd Interacted with … 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_04 no Interacted Interacted … ## 5 sit intera… intera… IntFam Interacted with … 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_05 no Interacted Interacted … ## 6 sit lostSm… lostSm… lostSmth… Lost something 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_06 no Lost somethi… Lost someth… ## 7 sit late late late Was late for som… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_07 no Late Late ## 8 sit frgtSm… frgtSm… frgtSmth… Forgot something 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_08 no Forgot somet… Forgot some… ## 9 sit brdSWk brdSWk brdSWk Was bored with s… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_09 no Bored with s… Bored with … ## 10 sit excSWk excSWk excSWk Was excited abou… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_10 no Excited abou… &lt;NA&gt; ## 11 sit AnxSWk AnxSWk AnxSWk Was anxious abou… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_11 no Anxious abou… &lt;NA&gt; ## 12 sit tired tired tired Felt tired 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_12 no Tired Tired ## 13 sit sick sick sick Felt sick 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_13 no Sick Sick ## 14 sit sleepi… sleepi… sleeping Was sleeping 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_15 no Sleeping Sleeping ## 15 sit class class class Was in class 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_16 no In Class In Class ## 16 sit music music music Was listening to… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_17 no Listening to… Listening t… ## 17 sit intern… intern… internet Was on the Inter… 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_18 no On the inter… On the inte… ## 18 sit TV TV TV Was watching TV 0 = did not… &lt;NA&gt; &lt;NA&gt; sit_19 no Watching TV Watching TV 1.3.1.1.4 DIAMONDS Situation Features Psychological features of situations were measured using the ultra brief version of the “Situational Eight” DIAMONDS (Duty, Intellect, Adversity, Mating, pOsitivity, Negativity, Deception, and Sociality) scale (S8-I; Rauthmann &amp; Sherman, 2015). Items were measured on a 3-point scale from 1 “not at all” to 3 “totally.” ipcs_codebook %&gt;% filter(category == &quot;S8-I&quot;) ## # A tibble: 8 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait long_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 S8-I Duty Duty Duty Work has to be done. 1 (= not a… &lt;NA&gt; &lt;NA&gt; D1 no Duty Duty ## 2 S8-I Intell… Intell… Intellect Deep thinking is require… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D2 no Intellect Intellect ## 3 S8-I Advers… Advers… Adversity Somebody is being threat… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D3 no Adversity Adversity ## 4 S8-I Mating Mating Mating Potential romantic partn… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D4 no Mating Mating ## 5 S8-I pOsiti… pOsiti… pOsitivi… The situation is pleasan… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D5 no pOsitivity pOsitivi… ## 6 S8-I Negati… Negati… Negativi… The situation contains n… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D6 no Negativity Negativi… ## 7 S8-I Decept… Decept… Deception Somebody is being deceiv… 1 (= not a… &lt;NA&gt; &lt;NA&gt; D7 no Deception Deception ## 8 S8-I Sociab… Sociab… Sociabil… Social interactions are … 1 (= not a… &lt;NA&gt; &lt;NA&gt; D8 no Sociality Sociality 1.3.1.1.5 Timing Features The final set of features were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (Fisher &amp; Soyster, 2019; . To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = {cumulative time}_t and 1 period sine = {cumulative time}_t). 1.3.2 Procedure Participants in this study were drawn from a larger personality study. All responded to two types of surveys: trait and state (Experience Sampling Method; ESM) measures, for which they were paid separately. Participants completed three waves of trait measures and two waves of state measures. For the first two waves, trait surveys were collected immediately before beginning the ESM protocol. 1.3.2.1 Main Sample For the main sample, participants were recruited from the psychology subject pool at Washington University in St. Louis. Participants were told that the study posted on the recruitment website was the first wave of a longer longitudinal study they would be offered the opportunity to take part in. Participants were brought into the lab between October 2018 and December 2019, where a research assistant or the first author explained the study procedure to them and walked them through the consent procedure. If they consented, participants were led to a room where they could fill out a form to opt into the ESM portion of the study. They then completed baseline trait measures using the Qualtrics Survey Platform. After, the participants were debriefed, paid $10 in cash and, if they opted into the ESM portion of the study, the ESM survey procedure was explained to them. Participants then received ESM surveys four times per day for two weeks (target n = 56). The survey platform was built by the first author using the jsPsych library (De Leeuw, 2015). Additional JavaScript controllers were written for the purpose of this study and are available on the first author’s GitHub. Start times were based on times that participants indicated they would like to receive their first survey based on their personal wake times. Surveys were sent every 4 hours, meaning that the surveys spanned a 12-hour period from the start time participants indicated. Participants received their first survey at their chosen time on the Monday following their in-lab session. They were compensated $.50 for each survey completed for a maximum of $28. To incentivize responding, participants who completed at least 50 surveys received a “bonus” for a total compensation of $30, which was distributed as an Amazon Gift Card. 1.3.3 Analytic Plan The present study tested three methods of machine learning classification models, some of which have been used for idiographic prediction in other studies (Fisher &amp; Soyster, 2019; Kaiser &amp; Butter, 2020): (1) Elastic Net Regression (Friedman, Hastie, &amp; Tibshirani, 2010), (2) The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT; Elleman, McDougald, Condon, &amp; Revelle, 2020), and (3) Random Forest Models (Kim et al., 2019). Because we have a large number of indicators to test, each of the methods used have variable selection features and, in some instances, other methods for reducing overfitting, as detailed below. To both reduce the number of indicators used in each test and to test which group of indicators are the most predictive of procrastination and loneliness, we will also test these in several sets: (1) Personality indicators (15), (2) Affective indicators (10), (3) Binary situation indicators (16), (4) DIAMONDS situation indicators (8), (5) Psychological indicators (personality + affect) (25), (6), Situation indicators (binary + DIAMONDS) (24), and (7) Full set (personality + affect + binary situations + DIAMONDS) (49). We will additionally test each of these with and without the 18 timing indicators, for a total set of 14 combinations of the 67 features. In each of these methods, we used cumulative rolling origin forecast validation, which was comprised of the first 75% of the time series, and held out the remaining 25% of the data set for the test set. In the rolling origin forecast validation, we used the first one-third of the time series as the initial set, five observations as the validation set, and set skip to one, which roughly resulted in 10-15 rolling origin “folds.” Out of sample prediction was tested based on classification error and area under the ROC (receive operating characteristic) curve (AUC). Classification error is a simple estimate of the percentage of the test sample that was correctly classified by the model. In addition, the AUC will capture the trade-off between sensitivity and specificity across a threshold. In the present study, we used an AUC threshold of .5, which indicates binary classification at chance levels. ROC visualizations plot 1 - specificity (i.e. false positive rate: false positives / (false positives + true negatives)) against sensitivity (i.e. true positive rate: true positives / (true positives + false positives)). 1.4 Demographics 1.4.0.1 Trait participants &lt;- googlesheets4::sheets_read(&quot;https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit?usp=sharing&quot;, sheet = &quot;ESM&quot;) %&gt;% select(SID, Name, Email) %&gt;% mutate(new = seq(1, n(), 1), new = ifelse(new &lt; 10, paste(&quot;0&quot;, new, sep = &quot;&quot;), new)) 1 old_names &lt;- trait_codebook$`New #` # wave 1 trait baseline &lt;- sprintf(&quot;%s/04-data/01-raw-data/baseline_05.07.20.csv&quot;, res_path) %&gt;% read_csv() %&gt;% filter(!row_number() %in% c(1,2) &amp; !is.na(SID) &amp; SID %in% participants$SID) %&gt;% select(SID, StartDate, gender, YOB, race, ethnicity) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% mutate(wave = 1, gender = factor(gender, c(1,2), c(&quot;Male&quot;, &quot;Female&quot;)), YOB = substr(YOB, nchar(YOB)-4+1, nchar(YOB)), race = mapvalues(race, 1:7, c(0,1,3,2,3,3,3)), ethnicity = ifelse(!is.na(ethnicity), 3, NA), race = ifelse(is.na(ethnicity), race, ifelse(ethnicity == 3, ethnicity))) %&gt;% select(-ethnicity) save(baseline, file = sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path)) load(url(sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path))) dem &lt;- baseline %&gt;% select(SID:race) %&gt;% mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB), StartDate = as.Date(ymd_hms(StartDate)), race = factor(race, 0:3, c(&quot;White&quot;, &quot;Black&quot;, &quot;Asian&quot;, &quot;Other&quot;))) %&gt;% select(-YOB) dem %&gt;% summarize(n = length(unique(SID)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;), sum(gender == &quot;Female&quot;)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 8 ## n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 208 154 (71.96%) 19.51 (1.27) 69 (32.24%) 34 (15.89%) 67 (31.31%) 30 (14.02%) 2019-03-29 (2018-10-17 - 2019-12-05) dem %&gt;% kable(., &quot;html&quot; , col.names = c(&quot;ID&quot;, &quot;Start Date&quot;, &quot;Gender&quot;, &quot;Race/Ethnicity&quot;, &quot;Age&quot;) , align = rep(&quot;c&quot;, 5) , caption = &quot;&lt;strong&gt;Table S1&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Participants at Baseline&lt;em&gt;&quot;) %&gt;% kable_styling(full_width = F) %&gt;% scroll_box(height = &quot;900px&quot;) Table 1.1: Table S1Descriptive Statistics of Participants at Baseline ID Start Date Gender Race/Ethnicity Age 02 2018-10-17 Female White 18 01 2018-10-17 Female Black 19 03 2018-10-17 Female Asian 19 04 2018-10-18 Male Other 19 05 2018-10-18 Male White 19 06 2018-10-18 Female Asian 20 07 2018-10-18 Female Black 20 08 2018-10-18 Female Black 18 09 2018-10-18 Female 18 10 2018-10-19 Female 19 11 2018-10-19 Female Asian 18 12 2018-10-19 Female White 20 13 2018-10-19 Female White 18 14 2018-10-19 Female Black 19 16 2018-10-19 Female White 18 15 2018-10-19 Female 20 17 2018-10-19 Male Asian 18 18 2018-10-22 Female White 19 19 2018-10-22 Female Black 20 20 2018-10-22 Female Asian 18 21 2018-10-22 Female Asian 19 22 2018-10-22 Female Black 19 23 2018-10-22 Male White 21 24 2018-10-22 Male Black 20 25 2018-10-22 Female White 18 27 2018-10-23 Female 18 26 2018-10-23 Female 18 28 2018-10-23 Female Other 21 29 2018-10-23 Female Asian 19 30 2018-10-23 Female Asian 20 31 2018-10-24 Female White 18 32 2018-10-24 Female Black 20 33 2018-10-24 Female Asian 18 34 2018-10-24 Female Black 19 35 2018-10-26 Female Black 18 36 2018-10-29 Female Asian 21 37 2018-10-29 Male Other 18 38 2018-10-29 Male Asian 19 36 2018-10-29 Female Other 20 37 2018-10-29 Female Asian 18 41 2018-10-29 Male White 19 38 2018-10-29 Female Black 19 43 2018-10-29 Female White 18 44 2018-10-30 Female Asian 18 45 2018-11-01 Female 18 46 2018-11-01 Female Asian 22 48 2018-11-01 Female Asian 21 47 2018-11-01 Male Asian 23 49 2018-11-02 Female Asian 20 51 2018-11-02 Female White 20 50 2018-11-02 Female Other 19 52 2018-11-05 Male Other 21 53 2018-11-05 Female Asian 19 52 2018-11-05 Male Asian 21 53 2018-11-05 Female White 19 56 2018-11-05 Male Asian 18 58 2018-11-05 Female Asian 21 57 2018-11-05 Female Asian 59 2018-11-06 Female Asian 21 60 2018-11-06 Male White 20 61 2018-11-06 Male White 18 62 2018-11-06 Male Black 18 63 2018-11-06 Female White 20 64 2018-11-07 Female Other 21 65 2018-11-07 Male White 20 67 2018-11-07 Female Black 19 66 2018-11-07 Male White 20 68 2018-11-07 Female Asian 18 69 2018-11-07 Female Asian 18 70 2018-11-08 Female 19 72 2018-11-08 Female Other 18 71 2018-11-08 Female Other 19 74 2018-11-08 Female Other 22 73 2018-11-08 Female White 18 75 2018-11-08 Female Asian 19 76 2018-11-08 Female Black 20 77 2018-11-08 Male Black 21 79 2018-11-08 Male Asian 18 80 2018-11-08 Female White 21 82 2018-11-09 Female Black 20 81 2018-11-09 Female Asian 22 83 2018-11-09 Female White 18 84 2018-11-09 Female White 20 85 2018-11-14 Female White 18 86 2018-11-14 Female Asian 21 87 2018-11-14 Female Asian 21 89 2018-11-14 Female White 19 88 2018-11-14 Female Asian 18 90 2018-11-20 Female Other 21 91 2018-11-20 Female Black 21 93 2018-11-28 Female White 20 92 2018-11-28 Male Black 20 94 2018-11-28 Female Asian 21 95 2018-11-28 Female Black 21 96 2018-11-28 Female White 21 97 2018-11-28 Male Asian 19 98 2018-11-29 Female Asian 18 99 2018-11-29 Female Other 21 100 2018-11-29 Female Black 19 101 2018-11-29 Female White 18 102 2018-11-29 Female White 19 103 2019-03-15 Female White 20 104 2019-03-15 Female 22 106 2019-03-22 Female Asian 21 105 2019-03-22 Female White 20 107 2019-03-22 Female White 19 109 2019-03-29 Female White 19 108 2019-03-29 Female White 20 111 2019-04-05 Female White 19 110 2019-04-05 Male Other 22 112 2019-04-05 Female Black 113 2019-04-05 Female Asian 23 114 2019-04-05 Male White 20 116 2019-04-12 Female White 21 115 2019-04-12 Female White 19 118 2019-04-12 Female Asian 21 117 2019-04-12 Female White 20 119 2019-04-12 Male Asian 19 121 2019-04-12 Female White 20 122 2019-04-12 Male White 20 123 2019-04-12 Female Asian 20 124 2019-04-12 Female Black 23 126 2019-04-19 Male Other 22 125 2019-04-19 Female Asian 21 128 2019-04-19 Male Black 23 127 2019-04-19 Female White 21 129 2019-04-19 Female Other 20 130 2019-04-19 Female White 19 131 2019-04-19 Male White 20 133 2019-04-26 Male Asian 20 132 2019-04-26 Female White 21 134 2019-04-26 Female Asian 136 2019-09-11 Male Asian 20 135 2019-09-11 Male White 19 138 2019-09-13 Female White 19 137 2019-09-13 Female White 19 139 2019-09-17 Female Black 19 141 2019-09-18 Female White 21 142 2019-09-13 Male Asian 18 143 2019-09-19 Female 18 146 2019-09-19 Female Asian 20 148 2019-09-20 Female Black 20 147 2019-09-20 Female White 20 149 2019-09-20 Female White 22 150 2019-09-20 Female Asian 18 151 2019-09-20 Female Other 18 152 2019-09-20 Male White 18 153 2019-09-27 Female Asian 21 154 2019-09-27 Male White 18 155 2019-09-27 Male White 19 156 2019-09-27 Male White 20 157 2019-09-27 Female White 21 158 2019-09-27 Female Asian 21 159 2019-09-27 Female Other 18 160 2019-09-27 Female Asian 18 161 2019-09-27 Female White 18 162 2019-09-27 Female Black 19 164 2019-10-04 Female 18 163 2019-10-04 Male Black 19 165 2019-10-04 Female White 22 166 2019-10-04 Male Other 19 168 2019-10-04 Female Asian 19 167 2019-10-04 Male Asian 19 169 2019-10-18 Male Asian 21 170 2019-10-18 Female Asian 19 171 2019-10-18 Male Other 18 172 2019-10-18 Female Other 19 174 2019-10-18 Male White 19 173 2019-10-18 Female Asian 18 175 2019-10-28 Male Black 19 176 2019-10-28 Male Other 19 177 2019-10-30 Male Asian 21 179 2019-11-02 Male Other 21 181 2019-11-02 Male Black 21 180 2019-11-02 Female White 20 182 2019-11-03 Female Black 19 184 2019-11-03 Female Asian 20 183 2019-11-03 Female Black 21 185 2019-11-07 Female Other 19 186 2019-11-07 Female Asian 21 188 2019-11-08 Female Other 18 187 2019-11-08 Male White 20 190 2019-11-08 Male White 21 189 2019-11-08 Female Other 20 192 2019-11-08 Female Black 18 191 2019-11-08 Male Other 18 199 2019-11-11 Female Asian 19 200 2019-11-11 Female Asian 18 201 2019-11-11 Female Asian 19 202 2019-11-11 Male Asian 18 203 2019-11-14 Male White 18 204 2019-11-14 Female Other 20 205 2019-11-14 Female Asian 19 206 2019-11-15 Female White 19 197 2019-11-15 Female 18 207 2019-11-15 Female Other 18 193 2019-11-15 Male White 18 196 2019-11-15 Female White 21 195 2019-11-15 Male Black 21 197 2019-11-15 Male White 18 209 2019-11-20 Female Asian 19 210 2019-11-20 Female 18 211 2019-11-20 Female 19 212 2019-11-20 Male White 20 213 2019-11-22 Male Asian 19 214 2019-11-22 Female Other 19 216 2019-11-22 Female Asian 19 215 2019-11-22 Female White 19 217 2019-12-02 Male 19 218 2019-12-03 Male Asian 19 219 2019-12-05 Male White 19 220 2019-12-05 Female Other 20 221 2019-12-05 Female Asian 21 222 2019-12-05 Female Black 20 "],["cleaning.html", "Chapter 2 Data Cleaning 2.1 Pre-Share Cleaning 2.2 ESM Data Setup 2.3 Setup for Idiographic Machine Learning Models 2.4 Demographics", " Chapter 2 Data Cleaning 2.1 Pre-Share Cleaning For the purposes of this study, we are only sharing the data used in this study (for both items and participants). ESM data were pre-cleaned because data imported from jsPsych are in a format not easily interpretable within R. An R script that shows the full procedure for extracting jsPscyh data is available on the OSF page for this study. Trait data were downloaded from Qualtrics and directly imported and cleaned as shown below. In addition, in order to anonymize participants, we have changed their ID’s using our master list, which we cannot make available because it contains identifying information. participants &lt;- googlesheets4::sheets_read(&quot;https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit#gid=16299281&quot;, sheet = &quot;ESM&quot;) %&gt;% select(SID, Name, Email) %&gt;% mutate(new = seq(1, nrow(.), 1), new = ifelse(new &lt; 10, paste(&quot;0&quot;, new, sep = &quot;&quot;), new)) 1 # wave 1 esm load(sprintf(&quot;%s/04-data/01-raw-data/clean_data_w1_2020-06-08.RData&quot;, res_path)) # combine waves bfi &lt;- BFI %&gt;% distinct() %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) old.names &lt;- ipcs_codebook$orig_itemname emo &lt;- emotion %&gt;% select(-trait, -facet) %&gt;% mutate(item = str_remove_all(item, &quot;E_&quot;)) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(trait = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, reverse_code)) sit &lt;- sit %&gt;% filter(item %in% old.names) %&gt;% select(-trait, -facet, -answer) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) ds8 &lt;- DS8 %&gt;% select(-trait, -facet, -answer) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) save(bfi, emo, sit, ds8, file = sprintf(&quot;%s/04-data/esm_cleaned_combined_2021-04-07.RData&quot;, res_path)) rm(list = ls()) train_fun &lt;- function(x) { if(length(unique(x[!is.na(x)]))==1){ replace &lt;- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]] x[sample(1:length(x), 1)] &lt;- replace } else if (any(table(x) == 1)){ replace &lt;- c(0,1)[which(table(x) &lt;= 1)] x[sample(1:length(x), 1)] &lt;- replace } x } test_fun &lt;- function(x) { if(length(unique(x[!is.na(x)]))==1){ replace &lt;- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]] x[sample(1:length(x), 1)] &lt;- replace } x } Now, we’ll load in the cleaned and de-identified data. load(url(sprintf(&quot;%s/04-data/01-raw-data/esm_cleaned_combined_2021-04-07.RData&quot;, res_path))) 2.2 ESM Data Setup Next, we need to make sure that all time information for IPCS is available. Specifically, this will allow us to control for overnight periods and unequal spacing between measurement occasions. 2.2.1 Timing First, we need to create empty rows in the data where there are missing assessments from the four target surveys per day as well as for the overnight periods. The function below uses the time stamp to figure out which blocks are missing and add those empty rows by indexing the time stamp of collected surveys as well as participants chosen start times. missing_fun &lt;- function(d){ first_day &lt;- unique(d$StartDate) # get first day hourBlock &lt;- unique(d$`Hour Block 1`) # get first hour block max_day &lt;- max(d$Day); max_day &lt;- ifelse(max_day &lt; 14, 14, max_day) # get number of days d2 &lt;- d %&gt;% #mutate(StartDate = ifelse(is.na(StartDate), min(Date, na.), StartDate)) full_join(crossing( Day = seq(0,max_day,1), HourBlock = 1:6, StartDate = first_day, `Hour Block 1` = hourBlock)) %&gt;% # cross existing data with &quot;perfect&quot; data arrange(Day, HourBlock) %&gt;% mutate(Date = StartDate + Day, Hour = ifelse(is.na(Hour), `Hour Block 1` + (HourBlock-1)*4, Hour), Minute = ifelse(is.na(Minute), &quot;00&quot;, Minute)) d2$Date[d2$Hour &gt; 23] &lt;- d2$Date[d2$Hour &gt; 23] + 1 # some day blocks span days d2$Hour[d2$Hour &gt; 23] &lt;- d2$Hour[d2$Hour &gt; 23] - 24 # some day blocks span days d2 &lt;- d2 %&gt;% mutate( Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute)) %&gt;% select(-`Hour Block 1`, -StartDate) } # create a data frame of timing info ipcs_times &lt;- emo %&gt;% select(SID, StartDate, Date, Hour, Minute, Day, `Hour Block 1`, HourBlock) %&gt;% distinct() %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute)) %&gt;% arrange(SID, Date) %&gt;% group_by(SID) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, missing_fun)) %&gt;% unnest(data) %&gt;% arrange(SID, Date, Hour) %&gt;% group_by(SID) %&gt;% mutate(all_beeps = seq(1, n(), 1)) %&gt;% ungroup() Here’s the result ipcs_times ## # A tibble: 23,615 x 8 ## SID Date Hour Minute Day HourBlock Full_Date all_beeps ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 2018-10-22 15 23 0 1 2018-10-22 15:23 1 ## 2 01 2018-10-22 19 00 0 2 2018-10-22 19:00 2 ## 3 01 2018-10-22 23 23 0 3 2018-10-22 23:23 3 ## 4 01 2018-10-22 23 25 0 3 2018-10-22 23:25 4 ## 5 01 2018-10-23 3 00 0 4 2018-10-23 3:00 5 ## 6 01 2018-10-23 7 00 0 5 2018-10-23 7:00 6 ## 7 01 2018-10-23 11 00 0 6 2018-10-23 11:00 7 ## 8 01 2018-10-23 17 50 1 1 2018-10-23 17:50 8 ## 9 01 2018-10-23 19 39 1 2 2018-10-23 19:39 9 ## 10 01 2018-10-23 23 00 1 3 2018-10-23 23:00 10 ## # … with 23,605 more rows 2.2.2 Personality Now it’s time to wrangle the personality data. As a reminder, personality was assessed using the full BFI-2 (Soto &amp; John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 “disagree strongly” to 5 “agree strongly.” Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants. 2.2.2.1 Wrangle Raw Data # join with codebook, reverse code, composite within facets and spread to wide format bfi_wide &lt;- bfi %&gt;% select(SID, Date, Hour, Minute, trait, facet, value = responses2, reverse_code) %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute), Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute)) %&gt;% mutate(value = as.numeric(value), value = ifelse(!is.na(reverse_code) &amp; reverse_code == &quot;yes&quot;, reverse.code(-1, value, mini = 1, maxi = 5), value)) %&gt;% select(-reverse_code) %&gt;% group_by(SID, trait, facet, Full_Date) %&gt;% summarize(value = mean(value, na.rm = T)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;trait&quot;, &quot;facet&quot;) , values_from = &quot;value&quot; , names_sep = &quot;_&quot;) %&gt;% group_by(SID) %&gt;% arrange(SID, lubridate::ymd_hm(Full_Date)) %&gt;% mutate(all_beeps = seq(1, n(), 1)) %&gt;% ungroup() 2.2.2.2 Multiple Imputation These data were collected using a planned missing design, so we need to impute data for the planned missing components. # run MI bfi_mi &lt;- data.frame(unclass(bfi_wide %&gt;% select(-Full_Date))) set.seed(5) bfi_mi &lt;- amelia(bfi_mi, m = 1, ts = &quot;all_beeps&quot;, cs = &quot;SID&quot;)$imputations[[1]] %&gt;% as_tibble() %&gt;% full_join(bfi_wide %&gt;% select(SID, Full_Date, all_beeps)) %&gt;% select(-all_beeps) ## -- Imputation 1 -- ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 bfi_mi ## # A tibble: 8,672 x 17 ## SID agreeableness_Co… agreeableness_Re… agreeableness_Tr… conscientiousnes… conscientiousnes… conscientiousne… extraversion_As… extraversion_En… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 1.45 1 3 4 4 4.79 4 2 ## 2 01 2.43 3 2 1 1 3.68 4 3 ## 3 01 2.86 1.5 1.70 1.14 3.03 3 2 2 ## 4 01 3.39 2 4 4.33 4 4 2 4 ## 5 01 5 2 3.85 3.04 3 3.04 4.36 3 ## 6 01 4 4.59 4 1.91 5 5 2.76 4 ## 7 01 2 1 3.88 2.18 3 1.61 2.75 2.5 ## 8 01 2 6.30 2 4 2.96 5 4 5.05 ## 9 01 3.26 2.80 3 3.17 2 2 1.07 3 ## 10 01 4.5 3.23 2.39 3.5 1.63 3.09 2 2 ## # … with 8,662 more rows, and 8 more variables: extraversion_Sociability &lt;dbl&gt;, neuroticism_Anxiety &lt;dbl&gt;, neuroticism_Depression &lt;dbl&gt;, ## # neuroticism_Emotional.Volatility &lt;dbl&gt;, openness_Aesthetic.Sensitivity &lt;dbl&gt;, openness_Creative.Imagination &lt;dbl&gt;, ## # openness_Intellectual.Curiosity &lt;dbl&gt;, Full_Date &lt;chr&gt; Personality data: bfi_mi ## # A tibble: 8,672 x 17 ## SID agreeableness_Co… agreeableness_Re… agreeableness_Tr… conscientiousnes… conscientiousnes… conscientiousne… extraversion_As… extraversion_En… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 1.45 1 3 4 4 4.79 4 2 ## 2 01 2.43 3 2 1 1 3.68 4 3 ## 3 01 2.86 1.5 1.70 1.14 3.03 3 2 2 ## 4 01 3.39 2 4 4.33 4 4 2 4 ## 5 01 5 2 3.85 3.04 3 3.04 4.36 3 ## 6 01 4 4.59 4 1.91 5 5 2.76 4 ## 7 01 2 1 3.88 2.18 3 1.61 2.75 2.5 ## 8 01 2 6.30 2 4 2.96 5 4 5.05 ## 9 01 3.26 2.80 3 3.17 2 2 1.07 3 ## 10 01 4.5 3.23 2.39 3.5 1.63 3.09 2 2 ## # … with 8,662 more rows, and 8 more variables: extraversion_Sociability &lt;dbl&gt;, neuroticism_Anxiety &lt;dbl&gt;, neuroticism_Depression &lt;dbl&gt;, ## # neuroticism_Emotional.Volatility &lt;dbl&gt;, openness_Aesthetic.Sensitivity &lt;dbl&gt;, openness_Creative.Imagination &lt;dbl&gt;, ## # openness_Intellectual.Curiosity &lt;dbl&gt;, Full_Date &lt;chr&gt; Now back to long format bfi_long &lt;- bfi_mi %&gt;% pivot_longer(cols = c(-SID, -Full_Date) , names_to = c(&quot;trait&quot;, &quot;facet&quot;) , values_to = &quot;value&quot; , names_sep = &quot;_&quot;) %&gt;% mutate(category = &quot;BFI-2&quot;) 2.2.3 Other Measured Features Emotion and Situation (Binary and DIAMONDS) Data (not planned missing, so no need to impute): features &lt;- emo %&gt;% full_join(sit) %&gt;% full_join(ds8) %&gt;% select(SID, category, trait, facet, itemname, Date, Hour, Minute, Day, HourBlock, value = responses2) %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute), Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute), value = as.numeric(value)) %&gt;% group_by(SID, category, trait, facet, Full_Date) %&gt;% summarize(value = max(value)) %&gt;% ungroup() features ## # A tibble: 312,010 x 6 ## SID category trait facet Full_Date value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 Affect afraid afraid 2018-10-22 15:23 2 ## 2 01 Affect afraid afraid 2018-10-22 23:23 3 ## 3 01 Affect afraid afraid 2018-10-22 23:25 3 ## 4 01 Affect afraid afraid 2018-10-23 17:50 4 ## 5 01 Affect afraid afraid 2018-10-23 19:39 2 ## 6 01 Affect afraid afraid 2018-10-24 0:00 2 ## 7 01 Affect afraid afraid 2018-10-24 11:44 3 ## 8 01 Affect afraid afraid 2018-10-24 15:37 2 ## 9 01 Affect afraid afraid 2018-10-24 20:46 3 ## 10 01 Affect afraid afraid 2018-10-25 21:07 4 ## # … with 312,000 more rows 2.2.4 Timing Features Finally, we’ll create the timing features. These were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (e.g., Fisher &amp; Soyster, 2019). To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = {cumulative time}_t and 1 period sine = {cumulative time}_t). time_features &lt;- ipcs_times %&gt;% mutate(wkday = wday(Full_Date, label = T) , Mon = ifelse(wkday == &quot;Mon&quot;, 1, 0) , Tue = ifelse(wkday == &quot;Tue&quot;, 1, 0) , Wed = ifelse(wkday == &quot;Wed&quot;, 1, 0) , Thu = ifelse(wkday == &quot;Thu&quot;, 1, 0) , Fri = ifelse(wkday == &quot;Fri&quot;, 1, 0) , Sat = ifelse(wkday == &quot;Sat&quot;, 1, 0) , Sun = ifelse(wkday == &quot;Sun&quot;, 1, 0) , morning = ifelse(Hour &gt;= 5 &amp; Hour &lt; 11, 1, 0) , midday = ifelse(Hour &gt;= 11 &amp; Hour &lt; 17, 1, 0) , evening = ifelse(Hour &gt;= 5 &amp; Hour &lt; 22, 1, 0) , night = ifelse(Hour &gt;= 22 &amp; Hour &lt; 5, 1, 0)) %&gt;% ## sequential time differences for each persn group_by(SID) %&gt;% mutate(tdif = as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = &quot;hours&quot;))) %&gt;% filter(is.na(tdif) | tdif &gt; 1) %&gt;% mutate(tdif = as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = &quot;hours&quot;)) , tdif = ifelse(is.na(tdif), 0, tdif) , cumsumT = cumsum(tdif)) %&gt;% ungroup() %&gt;% ## timing variables mutate(linear = as.numeric(scale(cumsumT)) , quad = linear^2 , cub = linear^3 , sin1p = sin(((2*pi)/24)*cumsumT) , sin2p = sin(((2*pi)/12)*cumsumT) , cos1p = cos(((2*pi)/24)*cumsumT) , cos2p = cos(((2*pi)/12)*cumsumT) ) %&gt;% ## keep key variables and reshape select(SID, Full_Date, Mon:night, linear:cos2p) %&gt;% pivot_longer(cols = c(-SID, -Full_Date) , names_to = &quot;trait&quot; , values_to = &quot;value&quot;) %&gt;% mutate(category = &quot;time&quot; , facet = trait) time_features ## # A tibble: 413,928 x 6 ## SID Full_Date trait value category facet ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 2018-10-22 15:23 Mon 1 time Mon ## 2 01 2018-10-22 15:23 Tue 0 time Tue ## 3 01 2018-10-22 15:23 Wed 0 time Wed ## 4 01 2018-10-22 15:23 Thu 0 time Thu ## 5 01 2018-10-22 15:23 Fri 0 time Fri ## 6 01 2018-10-22 15:23 Sat 0 time Sat ## 7 01 2018-10-22 15:23 Sun 0 time Sun ## 8 01 2018-10-22 15:23 morning 0 time morning ## 9 01 2018-10-22 15:23 midday 1 time midday ## 10 01 2018-10-22 15:23 evening 1 time evening ## # … with 413,918 more rows 2.2.5 Combine Features Now, let’s bring the personality, affect/situation/DIAMONDS, and timing features together. all_features &lt;- bfi_long %&gt;% full_join(features) %&gt;% full_join(ipcs_times %&gt;% select(SID, Full_Date)) %&gt;% full_join(time_features) %&gt;% arrange(SID, category, trait, facet, Full_Date) all_features ## # A tibble: 871,011 x 6 ## SID Full_Date trait facet value category ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 01 2018-10-22 15:23 afraid afraid 2 Affect ## 2 01 2018-10-22 23:23 afraid afraid 3 Affect ## 3 01 2018-10-22 23:25 afraid afraid 3 Affect ## 4 01 2018-10-23 17:50 afraid afraid 4 Affect ## 5 01 2018-10-23 19:39 afraid afraid 2 Affect ## 6 01 2018-10-24 0:00 afraid afraid 2 Affect ## 7 01 2018-10-24 11:44 afraid afraid 3 Affect ## 8 01 2018-10-24 15:37 afraid afraid 2 Affect ## 9 01 2018-10-24 20:46 afraid afraid 3 Affect ## 10 01 2018-10-25 21:07 afraid afraid 4 Affect ## # … with 871,001 more rows 2.3 Setup for Idiographic Machine Learning Models The last step is the most important. We need to: Separate the data for each outcome, participant, and feature set combination. In addition, the outcomes need to be lagged such that same time point features will be predicting “future” behavior. Moreover, the data must be split into training (first 75%) and test sets (last 25%). As we do this, we will also remove participants who have no variance in the outcome in either training or test sets as we can’t (statistically) predict things without variance (even if no variance suggests a good prediction!). The feature sets are as follows: Psychological: Big Five (BFI-2) Psychological: Affect Psychological: Big Five + Affect Situations: Binary Situations: DIAMONDS Situations: Binary + DIAMONDS Full: Big Five + Affect + Binary + DIAMONDS Each of these will be tested with and without the timing features for a total number of 14 feature sets. For now, I’m not going to run the chunk below because it takes a long time. All the resulting files can be found in the online materials: 04-data/02-raw-data: data before being split into training and test 04-data/03-train-data: training data for each participant x outcome x feature set combination (14) 04-data/04-test-data: test data for each participant x outcome x feature set combination (14) save_fun &lt;- function(d, group, set, outcome, SID, time){ print(paste(SID, outcome, group, set, time)) d_split &lt;- initial_time_split(d, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) if(any(table(d_train$o_value) &lt; 2) | sd(d_train$o_value) == 0) { return(NA) # no variance == can&#39;t use that participant } else { d_train &lt;- d_train %&gt;% mutate_at(vars(one_of(dummy_vars)), train_fun) %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- F # this is indexing if there were any other issues or concerns to be aware of } if(length(unique(d_test$o_value[!is.na(d_test$o_value)])) == 1){ d_test &lt;- d_test %&gt;% mutate_at(vars(one_of(dummy_vars)), test_fun) %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- c(ret, T) } else { d_test &lt;- d_test %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- c(ret, F) } d &lt;- d_train %&gt;% full_join(d_test) %&gt;% arrange(Full_Date) d_split &lt;- initial_time_split(d, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) save(d, file = sprintf(&quot;%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) save(d_train, file = sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) save(d_test, d_split, file = sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) # return(T) if(any(ret == T)) ret &lt;- T else ret &lt;- F return(ret) # } } factor_fun &lt;- function(x){if(is.numeric(x)){diff(range(x, na.rm = T)) %in% 1:2} else{F}} dummy_vars &lt;- c(&quot;o_value&quot;, &quot;argument&quot;, &quot;interacted&quot;, &quot;lostSmthng&quot; , &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot;, &quot;excSWk&quot;, &quot;AnxSWk&quot; , &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_dummy &lt;- c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;) data_fun &lt;- function(group, set, outcome, time){ if(set != &quot;all&quot;) groups &lt;- set if(set == &quot;all&quot;) { if(group == &quot;psychological&quot;) groups &lt;- c(&quot;BFI-2&quot;, &quot;Affect&quot;) else if(group == &quot;situations&quot;) groups &lt;- c(&quot;S8-I&quot;, &quot;sit&quot;) else groups &lt;- c(&quot;BFI-2&quot;, &quot;Affect&quot;, &quot;S8-I&quot;, &quot;sit&quot;) } if(time == &quot;time&quot;) groups &lt;- c(groups, &quot;time&quot;) out &lt;- all_features %&gt;% filter(trait == outcome) %&gt;% select(SID, Full_Date, value) %&gt;% group_by(SID) %&gt;% mutate(o_value = lead(value)) %&gt;% ungroup() %&gt;% select(-value) d &lt;- all_features %&gt;% filter(category %in% groups) %&gt;% mutate(name = ifelse(category == &quot;BFI-2&quot;, paste(trait, facet, sep = &quot;_&quot;), trait)) %&gt;% select(SID, Full_Date, name, value) %&gt;% distinct() %&gt;% pivot_wider(names_from = &quot;name&quot; , values_from = &quot;value&quot;) %&gt;% full_join(out) %&gt;% filter(complete.cases(.)) d %&gt;% group_by(SID) %&gt;% filter(n() &gt;= 40) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = pmap(list(data, group, set, outcome, SID, time), possibly(save_fun, NA_real_))) %&gt;% unnest(data) } nested_data &lt;- tribble( ~group, ~set, &quot;psychological&quot;, &quot;BFI-2&quot; , &quot;psychological&quot;, &quot;Affect&quot;, &quot;psychological&quot;, &quot;all&quot; , &quot;situations&quot; , &quot;sit&quot; , &quot;situations&quot; , &quot;S8-I&quot; , &quot;situations&quot; , &quot;all&quot; , &quot;full&quot; , &quot;all&quot; , ) %&gt;% full_join(crossing(group = c(&quot;psychological&quot;, &quot;situations&quot;, &quot;full&quot;) , time = c(&quot;no time&quot;, &quot;time&quot;) , outcome = c(&quot;prcrst&quot;, &quot;lonely&quot;))) %&gt;% mutate(data = pmap(list(group, set, outcome, time), possibly(data_fun, NA_real_))) 2.4 Demographics load(url(sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path))) dem &lt;- baseline %&gt;% select(SID:race) %&gt;% mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB), StartDate = as.Date(ymd_hms(StartDate)), race = factor(race, 0:3, c(&quot;White&quot;, &quot;Black&quot;, &quot;Asian&quot;, &quot;Other&quot;))) %&gt;% select(-YOB) prelim_dem &lt;- all_features %&gt;% filter(category %in% c(&quot;Affect&quot;, &quot;BFI-2&quot;, &quot;sit&quot;, &quot;SI-8&quot;)) %&gt;% group_by(SID, Full_Date, trait, facet, category) %&gt;% summarize(value = mean(value, na.rm = T)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;category&quot;, &quot;trait&quot;, &quot;facet&quot;) , values_from = value) %&gt;% filter(complete.cases(.)) prelim_dem %&gt;% group_by(SID) %&gt;% tally() %&gt;% ungroup() %&gt;% left_join(dem) %&gt;% summarize(N = length(unique(SID)), n = sprintf(&quot;%.2f (%.2f; %i-%i&quot;, mean(n), sd(n), min(n), max(n)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;, na.rm = T), sum(gender == &quot;Female&quot;, na.rm = T)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 9 ## N n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 199 42.23 (24.01; 1-158 144 (70.59%) 19.52 (1.24) 65 (31.86%) 30 (14.71%) 60 (29.41%) 28 (13.73%) NA (NA - NA) final_dem &lt;- prelim_dem %&gt;% group_by(SID) %&gt;% filter(n() &gt;= 40) %&gt;% tally() %&gt;% ungroup() %&gt;% left_join(dem) unique(ldply(str_split(list.files(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models&quot;, res_path)), pattern = &quot;_&quot;), function(x) x[1]))$V1 ## NULL final_dem %&gt;% filter(SID %in% unique(ldply(str_split(list.files(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models&quot;, local_path)), pattern = &quot;_&quot;), function(x) x[1]))$V1) %&gt;% summarize(N = length(unique(SID)), n = sprintf(&quot;%.2f (%.2f; %i-%i&quot;, mean(n), sd(n), min(n), max(n)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;, na.rm = T), sum(gender == &quot;Female&quot;, na.rm = T)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 9 ## N n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 99 57.41 (16.33; 40-109 75 (72.82%) 19.49 (1.31) 32 (31.07%) 14 (13.59%) 33 (32.04%) 16 (15.53%) NA (NA - NA) rm(list = ls()[!ls() %in% c(&quot;codebook&quot;, &quot;ipcs_codebook&quot;, &quot;res_path&quot;, &quot;local_path&quot;, &quot;sheets&quot;, &quot;outcomes&quot;, &quot;ftrs&quot;)]) "],["elastic-net.html", "Chapter 3 Elastic Net 3.1 Functions 3.2 Run Models", " Chapter 3 Elastic Net Elastic Net Regression (ENR) proceeds from the observation that typical OLS-based regression minimizes bias but may have great variance. Using L1 (Ridge) and L2 (LASSO) approaches, which apply penalties to model estimates, ENR attempts to balance the trade-off between bias and variance by choosing the best penalties that minimize an information criterion or prediction error. Together, these both shrink coefficients and help with feature selection by forcing some of the coefficients to be zero. Because there are a large number of values the regularization parameter λ can take on, the typical solution is to use a cross-validation to test a number of possible \\(\\lambda\\) penalty values and choose the one with the one that matches a criterion like minimizing prediction error. In the present study, we used the tidymodels package in R to estimate the ENR models by calling the logistic_reg(), setting the engine as “glmnet”, and the mode as “classification”. The parameters tuned via rolling origin forecast validation were penalty and mixture, which were each set to 10 values. Next, we used the select_best() function with the method set to “accuracy” to allow the algorithm to automatically pick the best combination of penalty and mixture that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of penalty and mixture and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the collect_metrics() function. 3.1 Functions dummy_vars &lt;- c(&quot;o_value&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;, &quot;argument&quot; , &quot;interacted&quot;, &quot;lostSmthng&quot;, &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot; , &quot;excSWk&quot;, &quot;AnxSWk&quot;, &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_vars &lt;- c(&quot;sin2p&quot;, &quot;sin1p&quot;, &quot;cos2p&quot;, &quot;cos1p&quot;) c_fun &lt;- function(m){ # final model characteristics lambda &lt;- min(m$fit$lambda) coefs &lt;- stats::coef(m$fit, s = lambda) coefs &lt;- coefs[, 1L, drop = TRUE] coefs &lt;- coefs[setdiff(x = names(coefs), y = &quot;(Intercept)&quot;)] return(coefs) } elnet_fun &lt;- function(sid, outcome, group, set, time){ # load the data load(sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) d_train &lt;- d_train %&gt;% arrange(Full_Date) %&gt;% select(-Full_Date) #%&gt;% # mutate_at(vars(-o_value), ~as.numeric(as.character(.))) init &lt;- ceiling(nrow(d_train)/3) d_train_cv &lt;- rolling_origin( d_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) no_zv_fun &lt;- function(x){ if(is.numeric(x)) sd(x, na.rm = T) != 0 else length(table(x)) &gt; 1 } # set up the cross-valiation folds # set.seed(234) # d_train_cv &lt;- vfold_cv(d_train, v = 10) # set up the data and formula mod_recipe &lt;- recipe( o_value ~ . , data = d_train ) %&gt;% step_zv(all_predictors(), -all_outcomes()) %&gt;% step_nzv(all_predictors(), unique_cut = 35) %&gt;% step_dummy(one_of(dummy_vars), -all_outcomes()) %&gt;% step_normalize(-one_of(dummy_vars), -one_of(time_vars)) #%&gt;% # estimate the means and standard deviations # prep(training = d_train, retain = TRUE) # set up the model specifications tune_spec &lt;- logistic_reg( penalty = tune() , mixture = tune() ) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;classification&quot;) # set up the ranges for the tuning functions elnet_grid &lt;- grid_regular(penalty() , mixture() , levels = 10) # set up the workflow: combine modeling spec with modeling recipe set.seed(345) elnet_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_recipe(mod_recipe) # combine the workflow, and grid to a final tuning model elnet_res &lt;- elnet_wf %&gt;% tune_grid( resamples = d_train_cv , grid = elnet_grid , control = control_resamples(save_pred = T) ) save(elnet_res, file = sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set)) # plot the metrics across tuning parameters p &lt;- elnet_res %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = mixture)) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_gradient(low = &quot;gray90&quot;, high = &quot;red&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/01-glmnet/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 8) # load(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set, time)) # select the best model based on AUC best_elnet &lt;- elnet_res %&gt;% # select_best(&quot;roc_auc&quot;) select_best(&quot;accuracy&quot;) # set up the workflow for the best model final_wf &lt;- elnet_wf %&gt;% finalize_workflow(best_elnet) # run the final best model on the training data and save final_elnet &lt;- final_wf %&gt;% fit(data = d_train) final_m &lt;- final_elnet %&gt;% pull_workflow_fit() final_coefs &lt;- c_fun(final_m) best_elnet &lt;- best_elnet %&gt;% mutate(nvars = length(final_coefs[final_coefs != 0])) save(final_coefs, best_elnet, file = sprintf(&quot;%s/05-results/01-glmnet/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load the test data load(sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # d_split$data$o_value &lt;- factor(d_split$data$o_value) # d_split$data&lt;-d_split$data %&gt;% # mutate_at(vars(-Full_Date, -o_value), ~as.numeric(as.character(.))) # run the final fit workflow of the training and test data together final_fit &lt;- final_wf %&gt;% last_fit(d_split) save(final_elnet, final_fit , file = sprintf(&quot;%s/05-results/01-glmnet/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # final metrics (accuracy and roc) final_metrics &lt;- final_fit %&gt;% collect_metrics(summarize = T) save(final_metrics , file = sprintf(&quot;%s/05-results/01-glmnet/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # variable importance final_var_imp &lt;- final_elnet %&gt;% pull_workflow_fit() %&gt;% vi() %&gt;% slice_max(Importance, n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/01-glmnet/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # roc plot p_roc &lt;- final_fit %&gt;% collect_predictions() %&gt;% roc_curve(.pred_0, truth = o_value) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/01-glmnet/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = c(&quot;final_var_imp&quot;, &quot;final_metrics&quot;, &quot;final_wf&quot;, &quot;final_elnet&quot;, &quot;final_fit&quot; , &quot;best_elnet&quot;, &quot;elnet_res&quot;, &quot;elnet_wf&quot;, &quot;elnet_grid&quot;, &quot;tune_spec&quot;, &quot;mod_recipe&quot; , &quot;p&quot;, &quot;p_roc&quot;, &quot;d_split&quot;, &quot;d_test&quot;, &quot;d_train&quot;, &quot;d_train_cv&quot;)) gc() return(T) } 3.2 Run Models plan(multisession(workers = 12L)) elnet_mods &lt;- tibble( file = sprintf(&quot;%s/04-data/03-train-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;)) %&gt;% mutate(mod = future_pmap( list(SID, outcome, group, set, time) , safely(elnet_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;dummy_vars&quot;, &quot;c_fun&quot;, &quot;time_vars&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;glmnet&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["biscwit.html", "Chapter 4 BISCWIT 4.1 Functions 4.2 Run Models", " Chapter 4 BISCWIT The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT) is a correlation-based machine learning technique. The technique, which we modified to be compatible with rolling origin validation rather than k-fold cross validation as implemented in the best.scales() function in the psych package in R, proceeds as follows. First, pairwise correlations between predictors and outcome(s) are calculated using rolling origin forecast validation where the number of best items is varied. Second, the number of items in the final model is determined by finding the average correlation across the rolling origin training sets with the validation sets for each number of items and choosing the one with the highest average correlation. Third, the final training model is constructed by running the procedure on the full training set with the number of items determined by the validation procedure. Fourth, weighted sum scores of the features for both the training and test sets are extracted. Finally, accuracy and AUC are calculated. Accuracy was estimated by (1) scaling the scores in the test set by the mean and standard deviation of scores in the final training model, (2) multiplying the scaled scores by the correlation between the training scores and the training outcome and the standard deviation of the training outcome and adding the mean of the training data outcome to this. This resulted in predicted outcomes for the test set. As in other classification models, values of .5 and above were considered to predict a “1” while values less than .5 were considered to predict a “0.” Accuracy was determined by comparing the predicted value with the actual test value and averaging the number correct. AUC was calculated using the predicted values and the test set values for the outcome. In the present study, we will use the bestScales() function from the psych package to create the models. Weighted scores were calculated by extracting the correlations from the best scales object and using it in the scoreWtd() function to create the correlation weighted scores. AUC was calculated using the vi() function in the vip package. Variable importance was determined by the items with the highest correlation with the outcome in the final training model. 4.1 Functions biscwit_call &lt;- function(x, nitem){ ## format the ro training set x_train &lt;- training(x) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() ## call the best scales function with nitem set from outer loop bs &lt;- bestScales( x = x_train , criteria = &quot;o_value&quot; , n.item = nitem , n.iter = 1 ) ## get the multiple R&#39;s (weights for biscwit v biscuit) # the second line sets the items not found by biscuit to be &quot;best&quot; # to be 0 mR &lt;- bs$R mR[!names(mR) %in% str_remove(bs$best.keys$o_value, &quot;-&quot;)] &lt;- 0 mR &lt;- as.matrix(mR); colnames(mR) &lt;- &quot;o_value&quot; ## format the test set x_test &lt;- testing(x) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% # select(one_of(names(mR))) %&gt;% unclass() %&gt;% data.frame() ## score the test set using the correlations as weights scores &lt;- scoreWtd(mR, x_test) ## calculate the correlation between the weighted score and the test y r &lt;- cor(scores, as.numeric(as.character(testing(x)$o_value))) ## return the biscuit object and the test correlations (criterion) return(list(bs = bs, r = r)) } biscwit_fun &lt;- function(sid, outcome, group, set, time){ # load the data load(sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## format the training data x_train &lt;- d_train %&gt;% arrange(Full_Date) %&gt;% select(-Full_Date) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() ## create the rolling_origin training and validation sets init &lt;- ceiling(nrow(d_train)/3) d_train_cv &lt;- rolling_origin( x_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) ## run biscwit on the ro ## run for nitem = 3 to the number of x training columns in increments of 3 tune_res &lt;- d_train_cv %&gt;% mutate(nitem = map(splits, ~seq(3, ncol(training(.)) - 1, 3))) %&gt;% unnest(nitem) %&gt;% mutate(cv = map2(splits, nitem, possibly(biscwit_call, NA_real_))) %&gt;% ## run the biscuit procedure # mutate(cv = map2(splits, nitem, biscwit_call)) %&gt;% ## run the biscuit procedure filter(!is.na(cv)) %&gt;% mutate(error = map_dbl(cv, ~(.)$r)) ## correlation is the training &quot;error&quot; (higher = lower error) save(tune_res, file = sprintf(&quot;%s/05-results/02-biscwit/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # plot the metrics across tuning parameters p &lt;- tune_res %&gt;% group_by(nitem) %&gt;% summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %&gt;% arrange(desc(abs(merror))) %&gt;% ggplot(aes(x = nitem, y = merror)) + scale_alpha_continuous(range=c(.4,1))+ geom_point(aes(alpha = abs(merror)), color = &quot;red&quot;) + labs(x = &quot;n items&quot;, y = &quot;Correlation&quot;, &quot;Correlation Strength (absolute value)&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/02-biscwit/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) # find the best model by averaging correlations across ro sets # and choosing the one with the highest average correlation best_biscwit &lt;- tune_res %&gt;% group_by(nitem) %&gt;% summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %&gt;% arrange(desc(abs(merror))) %&gt;% slice_head(n = 1) ## rerun biscuit on the full training set with the best nitem final_biscwit &lt;- bestScales( x = x_train , criteria = &quot;o_value&quot; , n.item = best_biscwit$nitem , n.iter = 1 ) save(final_biscwit , file = sprintf(&quot;%s/05-results/02-biscwit/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # get the correlations for weighting # the second line sets the items not found by biscuit to be &quot;best&quot; # to be 0 mR &lt;- final_biscwit$R; mR &lt;- mR[!names(mR) == &quot;o_value&quot;] mR[!names(mR) %in% str_remove(final_biscwit$best.keys$o_value, &quot;-&quot;)] &lt;- 0 mR &lt;- as.matrix(mR); colnames(mR) &lt;- &quot;o_value&quot; final_m &lt;- final_biscwit final_coefs &lt;- mR best_biscwit &lt;- best_biscwit %&gt;% mutate(nvars = length(mR[mR != 0])) save(final_coefs, best_biscwit, file = sprintf(&quot;%s/05-results/02-biscwit/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load the test data load(sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## format the test set x_test &lt;- d_test %&gt;% arrange(Full_Date) %&gt;% select(o_value, everything(), -Full_Date) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() # adapted from Elleman, McDougald, Condon, &amp; Revelle (2020) # Step 1. Score the train X. scores_best &lt;- scoreWtd(mR, x_train) # Step 2. Score the test X. scores_best_test &lt;- scoreWtd(mR, x_test) # Step 3. Get the diagnostic info from the train y&#39;s scoring. cor(scores_best, x_train$o_value, use = &quot;pairwise&quot;) ## mean and sd of training scores for observations 1 to t. mean_biscuit &lt;- mean(scores_best, na.rm = T) sd_biscuit &lt;- sd( scores_best, na.rm = T) # performance in training set multiple_R &lt;- cor(scores_best, x_train$o_value, use = &quot;pairwise&quot;) # descriptives of the outcome variable in the training set mean_data &lt;- mean(x_train$o_value, na.rm = T) sd_data &lt;- sd( x_train$o_value, na.rm = T) # Step 4. Apply the diagnostic info to the test y&#39;s scoring. # Standardardize scores in the test set using the training set # subtract average training score from each score in the test set and divide by SD scores_best.z_test &lt;- c((scores_best_test - mean_biscuit) / sd_biscuit) # scores_best.z_test_vector &lt;- c(scores_best.z_test) # reverse transform using mean and SD of outcome in the training set # and the correlation between scores and outcome in the training set value_test &lt;- ( c(multiple_R) * scores_best.z_test * sd_data ) + mean_data # Step 5. You have your predicted test values. Look at them to make sure they make sense!! temp &lt;- data.frame(crit_actual = x_test$o_value, crit_predicted = value_test) # Get the accuracy. Since the reverse standardization should make these raw, # we&#39;ll just essentially round these values to 0 or 1 (whichever is closer) # to determine accuracy temp_diagnostics &lt;- as_tibble(temp) %&gt;% mutate( accuracy = ifelse((crit_predicted &gt;= .5 &amp; crit_actual == 1) | (crit_predicted &lt; .5 &amp; crit_actual == 0), 1, 0) ) # Best model results, testing data ## Getting and saving final metrics ## First get average accuracy in the test set value_accuracy &lt;- mean(temp_diagnostics$accuracy, na.rm = T) ## Next get AUC&#39;s. Building in a failsafe in case in the case of no variance ## in the test set, which would result in an error and break the code ## formatted to match output from tidymodels collect_metrics() for consistency if(sd(temp$crit_actual) != 0){ roc &lt;- roc_auc(temp %&gt;% mutate(crit_actual = as.factor(crit_actual)) , truth = crit_actual , crit_predicted) } else { roc &lt;- tibble( .metric = &quot;roc_auc&quot; , .estimator = &quot;binary&quot; , .estimate = NA_real_ ) } ## reformat accuracy and join with AUC final_metrics &lt;- roc %&gt;% bind_rows(tibble( .metric = &quot;accuracy&quot; , .estimator = &quot;binary&quot; , .estimate = value_accuracy)) save(final_metrics , file = sprintf(&quot;%s/05-results/02-biscwit/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## variable importance ## reformatted to match the output of the vi() function in the vip package final_var_imp &lt;- data.frame(Importance = final_biscwit$R) %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% mutate(Sign = ifelse(sign(Importance) == 1, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% as_tibble() %&gt;% arrange(desc(abs(Importance))) %&gt;% slice_max(abs(Importance), n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/02-biscwit/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## roc curve of test data p_roc &lt;- roc_curve( temp %&gt;% mutate(crit_actual = as.factor(crit_actual)) , truth = crit_actual , crit_predicted ) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/02-biscwit/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = ls()) gc() return(T) } 4.2 Run Models done &lt;- tibble(file = list.files(sprintf(&quot;%s/05-results/02-biscwit/06-final-model-performance&quot;, res_path)), done = &quot;done&quot;) plan(multisession(workers = 12L)) res &lt;- tibble( file = sprintf(&quot;%s/04-data/02-model-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , mod = future_pmap( list(SID, outcome, group, set, time) , possibly(biscwit_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;biscwit_call&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;psych&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["random-forest.html", "Chapter 5 Random Forest 5.1 Set Up Data", " Chapter 5 Random Forest Random forest models are a variant of decision tree classification algorithms that additionally draw on bagging (i.e. ensemble methods; bootstrapping with aggregation) methods. The name itself hints at how it different from classic decision trees; we have a forest instead of a tree. In so doing, random forest models draw upon a large number of decision trees of varying depth that are then aggregated. The random part comes from two sources. First, each tree in the forest in trained on a data set drawn with replacement from the training dataset (i.e. bootstrapped) as part of the bagging procedure. Observations left out of each model are termed out-of-bag observations (and collectively, the out-of-bag dataset). These are used to evaluate the performance of the tree. Second, the features used in each tree are also randomly drawn from the full of features. Each of these trees, based on their data generates a prediction given new data. The final prediction is based off the ensemble of trees – that is, the decision made by a majority of the trees (i.e. aggregation). Importantly, because of the random feature selection part of the procedure, random forest can also provide estimates of variable importance, indicating which features are critical to making a less error-prone classification. Because random forest using bagging (i.e. bootstrapping with aggregation), we will have to perform a series of steps that make bootstrapping appropriate with time series data: differencing, Box-Cox transformations, and time-delay embedding. Essentially, differencing and Box-Cox transformations stabilize the mean and variance, respectively, to make the time series stationary (Priestley, 1988), and time-delay embedding quite literally embeds the sequence of the time series into predictor variables, which in effect preserves the order of the times series (Von Oertzen &amp; Boker, 2010). We can easily back transform forecasts to their original scale. In the present study, we used the tidymodels package in R to estimate the random forest models by calling the rand_forest(), setting the engine as “ranger”, with importance = “permutation” in order to extract variable importance, and the mode as “classification”. The parameters tuned via rolling origin forecast validation were mtry (i.e. the number of predictors that will be randomly sampled at each split when creating tree models) and min_n (i.e. the minimum number of data points in a node that is required for the node to be split further), which were each set to 10 values. Next, we used the select_best() function with the method set to “accuracy” to allow the algorithm to automatically pick the best combination of mtry and min_n that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of mtry and min_n and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the collect_metrics() function. 5.1 Set Up Data 5.1.1 Differencing and Box Cox dummy_vars &lt;- c(&quot;o_value&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;, &quot;argument&quot; , &quot;interacted&quot;, &quot;lostSmthng&quot;, &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot; , &quot;excSWk&quot;, &quot;AnxSWk&quot;, &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_vars &lt;- c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot; , &quot;sin2p&quot;, &quot;sin1p&quot;, &quot;cos2p&quot;, &quot;cos1p&quot; , &quot;cub&quot;, &quot;linear&quot;, &quot;quad&quot;) rf_fun &lt;- function(sid, outcome, group, set, time){ load(sprintf(&quot;%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # differencing and box-cox d &lt;- d %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), log) %&gt;% mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), ~. - lag(.)) # time delay embedding d_mbd &lt;- map(d %&gt;% select(-Full_Date, -one_of(time_vars)), ~embed(., 2)) %&gt;% ldply(.) %&gt;% group_by(.id) %&gt;% mutate(beep = 1:n()) %&gt;% ungroup() %&gt;% pivot_wider(names_from = &quot;.id&quot; , names_glue = &quot;{.id}_{.value}&quot; , values_from = c(&quot;1&quot;, &quot;2&quot;)) %&gt;% bind_cols(d[-1,] %&gt;% select(Full_Date)) %&gt;% select(-beep) d_mbd &lt;- d_mbd %&gt;% full_join(d %&gt;% select(Full_Date, one_of(time_vars))) %&gt;% mutate_at(vars(contains(dummy_vars)), factor) %&gt;% # mutate(o_value_1 = factor(o_value_1)) %&gt;% drop_na() # training and test sets d_split &lt;- initial_time_split(d_mbd, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) d_train &lt;- d_train %&gt;% arrange(lubridate::ymd_hm(Full_Date)) %&gt;% select(-Full_Date) ## create the rolling_origin training and validation sets init &lt;- ceiling(nrow(d_train)/3) # set up the cross-valiation folds d_train_cv &lt;- rolling_origin( d_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) # set up the data and formula mod_recipe &lt;- recipe( o_value_1 ~ . , data = d_train ) %&gt;% step_zv(all_numeric(), contains(dummy_vars), contains(time_vars)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_nzv(all_predictors(), unique_cut = 35) #%&gt;% # estimate the means and standard deviations # prep(training = d_train, retain = TRUE) # set up the model specifications tune_spec &lt;- rand_forest( mtry = tune() , trees = 1000 , min_n = tune() ) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;permutation&quot;) %&gt;% set_mode(&quot;classification&quot;) # set up the workflow: combine modeling spec with modeling recipe set.seed(345) rf_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_recipe(mod_recipe) # set up the ranges for the tuning functions set.seed(345) tune_res &lt;- tune_grid( rf_wf , resamples = d_train_cv , grid = 20 ) save(tune_res, file = sprintf(&quot;%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load(sprintf(&quot;%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set, time)) # plot the metrics across tuning parameters p &lt;- tune_res %&gt;% collect_metrics() %&gt;% ggplot(aes(mtry, mean, color = min_n)) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_gradient(low = &quot;gray90&quot;, high = &quot;red&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/03-rf/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 8) # select the best model based on AUC best_rf &lt;- tune_res %&gt;% # select_best(&quot;roc_auc&quot;) select_best(&quot;accuracy&quot;) # set up the workflow for the best model final_wf &lt;- rf_wf %&gt;% finalize_workflow(best_rf) # run the final best model on the training data and save final_rf &lt;- final_wf %&gt;% fit(data = d_train) final_m &lt;- final_rf %&gt;% pull_workflow_fit() final_coefs &lt;- final_m$fit$variable.importance best_rf &lt;- best_rf %&gt;% mutate(nvars = length(final_coefs[final_coefs != 0])) save(final_coefs, best_rf, file = sprintf(&quot;%s/05-results/03-rf/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # run the final fit workflow of the training and test data together final_fit &lt;- final_wf %&gt;% last_fit(d_split) save(final_rf, final_fit , file = sprintf(&quot;%s/05-results/03-rf/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # final metrics (accuracy and roc) final_metrics &lt;- final_fit %&gt;% collect_metrics(summarize = T) save(final_metrics , file = sprintf(&quot;%s/05-results/03-rf/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # variable importance final_var_imp &lt;- final_rf %&gt;% pull_workflow_fit() %&gt;% vi() %&gt;% slice_max(Importance, n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/03-rf/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # roc plot p_roc &lt;- final_fit %&gt;% collect_predictions() %&gt;% roc_curve(.pred_0, truth = o_value_1) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/03-rf/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = c(&quot;final_var_imp&quot;, &quot;final_metrics&quot;, &quot;final_wf&quot;, &quot;final_rf&quot;, &quot;final_fit&quot; , &quot;best_rf&quot;, &quot;tune_res&quot;, &quot;rf_wf&quot;, &quot;tune_spec&quot;, &quot;mod_recipe&quot; , &quot;p&quot;, &quot;p_roc&quot;, &quot;d_split&quot;, &quot;d_test&quot;, &quot;d_train&quot;, &quot;d_train_cv&quot;)) gc() return(T) } 5.1.2 Run Models plan(multisession(workers = 12L)) rf_res &lt;- tibble( file = sprintf(&quot;%s/04-data/02-model-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;)) %&gt;% mutate( mod = future_pmap( list(SID, outcome, group, set, time) , safely(rf_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;dummy_vars&quot;, &quot;time_vars&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;glmnet&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["summarizing-models.html", "Chapter 6 Summarizing Models 6.1 Question 1: Can we predict procrastination and loneliness? 6.2 Question 2: Are there individual differences in the idiographic range of prediction across people? 6.3 Question 3: Do Psychological, Situational, or Full Feature Sets Perform Best? 6.4 Question 4: Which features are most associated with Procrastination and Loneliness? 6.5 Question 5: Do people vary in the which features are most important?", " Chapter 6 Summarizing Models Now that all the models have been run, the next step is to take various metrics and results from the models and format them into tables and figures that are more understable than thousands of model objects. 6.1 Question 1: Can we predict procrastination and loneliness? 6.1.1 Performance Metrics To begin, we’ll pull the performance metrics – classification accuracy and area under the receiver operating curve (AUC) – to determine and display: Overall Model Performance Participant Specific Model Performance (e.g., did certain feature sets perform differently) Participants best models (in terms of accuracy and AUC) along with summaries of such accuracy and AUC, the feature set, etc. The first thing we need to do is load in the final model performance metrics – that is, the accuracy and AUC of the model chosen via rolling-origin validation on the test / holdout set. loadRData &lt;- function(fileName, type, model){ #loads an RData file, and returns it path &lt;- sprintf(&quot;%s/05-results/%s/06-final-model-performance/%s&quot;, local_path, model, fileName) load(path) get(ls()[grepl(type, ls())]) } sum_res &lt;- tibble( model = c(&quot;01-glmnet&quot;, &quot;02-biscwit&quot;, &quot;03-rf&quot;) ) %&gt;% mutate(file = map(model, ~sprintf(&quot;%s/05-results/%s/06-final-model-performance&quot;, local_path, .) %&gt;% list.files())) %&gt;% unnest(file) %&gt;% mutate(data = map2(file, model, ~loadRData(.x, &quot;final_metrics&quot;, .y))) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , model = str_remove_all(model, &quot;[0-9 -]&quot;)) save(sum_res, file = sprintf(&quot;%s/05-results/final-model-performance.RData&quot;, local_path)) Which looks something like this: load(url(sprintf(&quot;%s/05-results/final-model-performance.RData&quot;, res_path))) sum_res %&gt;% unnest(data) ## # A tibble: 10,970 x 10 ## model SID outcome group set time .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 glmnet 01 prcrst full all no time accuracy binary 0.909 Preprocessor1_Model1 ## 2 glmnet 01 prcrst full all no time roc_auc binary 0.4 Preprocessor1_Model1 ## 3 glmnet 01 prcrst psychological Affect no time accuracy binary 0.909 Preprocessor1_Model1 ## 4 glmnet 01 prcrst psychological Affect no time roc_auc binary 0.8 Preprocessor1_Model1 ## 5 glmnet 01 prcrst psychological all no time accuracy binary 0.909 Preprocessor1_Model1 ## 6 glmnet 01 prcrst psychological all no time roc_auc binary 0.4 Preprocessor1_Model1 ## 7 glmnet 01 prcrst psychological BFI-2 no time accuracy binary 0.917 Preprocessor1_Model1 ## 8 glmnet 01 prcrst psychological BFI-2 no time roc_auc binary 0.5 Preprocessor1_Model1 ## 9 glmnet 01 prcrst situations all no time accuracy binary 0.818 Preprocessor1_Model1 ## 10 glmnet 01 prcrst situations all no time roc_auc binary 0.7 Preprocessor1_Model1 ## # … with 10,960 more rows loadRData &lt;- function(fileName, type, model){ #loads an RData file, and returns it path &lt;- sprintf(&quot;%s/05-results/%s/07-final-model-param/%s&quot;, local_path, model, fileName) load(path) get(ls()[grepl(type, ls())]) } param_res &lt;- tibble( model = c(&quot;01-glmnet&quot;, &quot;02-biscwit&quot;, &quot;03-rf&quot;) ) %&gt;% mutate(file = map(model, ~sprintf(&quot;%s/05-results/%s/07-final-model-param&quot;, local_path, .) %&gt;% list.files())) %&gt;% unnest(file) %&gt;% mutate(params = map2(file, model, ~loadRData(.x, &quot;best&quot;, .y)) , coefs = map2(file, model, ~loadRData(.x, &quot;coef&quot;, .y))) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , model = str_remove_all(model, &quot;[0-9 -]&quot;)) save(param_res, file = sprintf(&quot;%s/05-results/final-model-param.RData&quot;, local_path)) Which looks like this: load(url(sprintf(&quot;%s/05-results/final-model-param.RData&quot;, res_path))) param_res ## # A tibble: 5,485 x 8 ## model SID outcome group set time params coefs ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 glmnet 01 prcrst full all no time &lt;tibble [1 × 4]&gt; &lt;dbl [49]&gt; ## 2 glmnet 01 prcrst psychological Affect no time &lt;tibble [1 × 4]&gt; &lt;dbl [10]&gt; ## 3 glmnet 01 prcrst psychological all no time &lt;tibble [1 × 4]&gt; &lt;dbl [25]&gt; ## 4 glmnet 01 prcrst psychological BFI-2 no time &lt;tibble [1 × 4]&gt; &lt;dbl [15]&gt; ## 5 glmnet 01 prcrst situations all no time &lt;tibble [1 × 4]&gt; &lt;dbl [24]&gt; ## 6 glmnet 01 prcrst situations S8-I no time &lt;tibble [1 × 4]&gt; &lt;dbl [8]&gt; ## 7 glmnet 01 prcrst situations sit no time &lt;tibble [1 × 4]&gt; &lt;dbl [16]&gt; ## 8 glmnet 02 prcrst full all no time &lt;tibble [1 × 4]&gt; &lt;dbl [48]&gt; ## 9 glmnet 02 prcrst full all time &lt;tibble [1 × 4]&gt; &lt;dbl [64]&gt; ## 10 glmnet 02 prcrst psychological Affect no time &lt;tibble [1 × 4]&gt; &lt;dbl [10]&gt; ## # … with 5,475 more rows 6.1.2 Classification Accuracy and AUC for all Models Now that we’ve loaded in the results, the first thing that we’ll do is create tables on the performance of each model for all the tested feature sets. The goal here is less to make any specific argument with the results and more to just document them in a nice table format that is easier to read. perf_tab_fun &lt;- function(d, outcome, group, set, time){ # format groups, time, and outcomes to nice names g &lt;- str_to_title(group); s &lt;- str_to_title(set) tm &lt;- if(time == &quot;time&quot;) &quot;With Time&quot; else &quot;Without Time&quot; o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) # create the caption cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Performance Metrics of the %s (%s) Feature Set %s Predicting %s&quot;, g, s, tm, o) # call kable to create the html table tab &lt;- d %&gt;% kable(. , &quot;html&quot; , col.names = c(&quot;ID&quot;, rep(c(&quot;Accuracy&quot;, &quot;AUC&quot;), times = 3)) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 6)) , digits = 2 , caption = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) # save the table to files save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/01-participant-metrics/%s_%s_%s_%s.html&quot;, local_path, outcome, group, set, time)) # return the table object return(tab) } sum_res_tab &lt;- sum_res %&gt;% unnest(data) %&gt;% select(-.estimator, -.config) %&gt;% pivot_wider(names_from = c(&quot;model&quot;, &quot;.metric&quot;) , values_from = &quot;.estimate&quot;) %&gt;% group_by(outcome, group, set, time) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, outcome, group, set, time), perf_tab_fun)) sum_res_tab ## # A tibble: 28 x 6 ## outcome group set time data tab ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 prcrst full all no time &lt;tibble [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 2 prcrst psychological Affect no time &lt;tibble [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 3 prcrst psychological all no time &lt;tibble [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 4 prcrst psychological BFI-2 no time &lt;tibble [98 × 7]&gt; &lt;kablExtr [1]&gt; ## 5 prcrst situations all no time &lt;tibble [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 6 prcrst situations S8-I no time &lt;tibble [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 7 prcrst situations sit no time &lt;tibble [98 × 7]&gt; &lt;kablExtr [1]&gt; ## 8 prcrst full all time &lt;tibble [75 × 7]&gt; &lt;kablExtr [1]&gt; ## 9 prcrst psychological Affect time &lt;tibble [81 × 7]&gt; &lt;kablExtr [1]&gt; ## 10 prcrst psychological all time &lt;tibble [81 × 7]&gt; &lt;kablExtr [1]&gt; ## # … with 18 more rows Now I’ll print the tables in different tabs below. Here, I’m only showing the set with combined features for each category for parsimony. The full results are in the online materials under 05-results/04-tables/01-participant-metrics. 6.1.2.1 Procrastination As is clear in each of these tables, overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy. tmp &lt;- sum_res_tab %&gt;% filter(set == &quot;all&quot; &amp; outcome == &quot;prcrst&quot;) %&gt;% mutate(group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) for(i in 1:nrow(tmp)){ cat(&#39; \\n\\n##### &#39;, tmp$group[i], &#39;\\n\\n &#39;, sep =&quot;&quot;) tmp$tab[[i]] %&gt;% scroll_box(height = &quot;500px&quot;) %&gt;% print() } ## ## ## ##### Full, No Time ## ## ## ## ##### Psychological, No Time ## ## ## ## ##### Situations, No Time ## ## ## ## ##### Full, Time ## ## ## ## ##### Psychological, Time ## ## ## ## ##### Situations, Time ## ## 6.1.2.2 Loneliness As with procrastination, the loneliness tables indicate that overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy. tmp &lt;- sum_res_tab %&gt;% filter(set == &quot;all&quot; &amp; outcome == &quot;lonely&quot;) %&gt;% mutate(group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) for(i in 1:nrow(tmp)){ cat(&#39; \\n\\n##### &#39;, tmp$group[i], &#39;\\n\\n &#39;, sep =&quot;&quot;) tmp$tab[[i]] %&gt;% scroll_box(height = &quot;750px&quot;) %&gt;% print() } 6.1.2.2.1 Full, No Time 6.1.2.2.2 Full, Time 6.1.2.2.3 Psychological, No Time 6.1.2.2.4 Psychological, Time 6.1.2.2.5 Situations, No Time 6.1.2.2.6 Situations, Time 6.1.3 Best Models Next, to get a more concise indication of how these models are performing, we will choose the best model in terms of accuracy and AUC for each participant, outcome, and model combination. best_mods &lt;- sum_res %&gt;% unnest(data) %&gt;% filter(set == &quot;all&quot;) %&gt;% group_by(SID, outcome, .metric, model) %&gt;% filter(!is.na(.estimate) &amp; .estimate != 1) %&gt;% arrange(desc(.estimate), model, group) %&gt;% slice_head(n = 1) %&gt;% ungroup(); best_mods ## # A tibble: 817 x 10 ## model SID outcome group set time .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 biscwit 01 prcrst full all no time accuracy binary 0.909 &lt;NA&gt; ## 2 glmnet 01 prcrst full all no time accuracy binary 0.909 Preprocessor1_Model1 ## 3 rf 01 prcrst psychological all no time accuracy binary 0.909 Preprocessor1_Model1 ## 4 biscwit 01 prcrst situations all no time roc_auc binary 0.85 &lt;NA&gt; ## 5 glmnet 01 prcrst situations all no time roc_auc binary 0.7 Preprocessor1_Model1 ## 6 rf 01 prcrst full all no time roc_auc binary 0.556 Preprocessor1_Model1 ## 7 biscwit 02 prcrst full all time accuracy binary 0.769 &lt;NA&gt; ## 8 glmnet 02 prcrst full all time accuracy binary 0.846 Preprocessor1_Model1 ## 9 rf 02 prcrst full all no time accuracy binary 0.833 Preprocessor1_Model1 ## 10 biscwit 02 prcrst situations all no time roc_auc binary 0.705 &lt;NA&gt; ## # … with 807 more rows 6.1.3.1 Participant Summaries (Table) Now that we have participants best models, the first thing we’ll do is create a summary of just how well participants’ best models actually performed. These Supplementary Tables will be split by outcome (procrastination, loneliness) and metric (accuracy, AUC) with each row giving details on which feature set was chosen for each method and what the accuracy or AUC for that method was. px_bm_fun &lt;- function(d, outcome, metric){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) m &lt;- if(metric == &quot;accuracy&quot;) &quot;Accuracy&quot; else &quot;AUC&quot; cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Feature Set and %s for Predicting %s for Each Participant&#39;s Best Model&lt;/em&gt;&quot;, m, o) tab &lt;- d %&gt;% select(SID, contains(&quot;glmnet&quot;), contains(&quot;biscwit&quot;), contains(&quot;rf&quot;)) %&gt;% kable(. , &quot;html&quot; , digits = 2 , col.names = c(&quot;ID&quot;, rep(c(&quot;Feature Set&quot;, m), times = 3)) , align = c(&quot;r&quot;, rep(c(&quot;l&quot;, &quot;c&quot;), times = 3)) , cap = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/02-participant-best-models/%s_%s.html&quot;, local_path, outcome, metric)) return(tab) } px_best_mods &lt;- best_mods %&gt;% select(-.estimator, -.config, -set) %&gt;% mutate_at(vars(group, time), str_to_title) %&gt;% unite(group, group, time, sep = &quot;, &quot;) %&gt;% pivot_wider(names_from = &quot;model&quot; , values_from = c(&quot;group&quot;, &quot;.estimate&quot;) , names_glue = &quot;{model}_{.value}&quot;) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, outcome, .metric), px_bm_fun)); px_best_mods ## # A tibble: 4 x 4 ## outcome .metric data tab ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 prcrst accuracy &lt;tibble [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 2 prcrst roc_auc &lt;tibble [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 3 lonely accuracy &lt;tibble [51 × 7]&gt; &lt;kablExtr [1]&gt; ## 4 lonely roc_auc &lt;tibble [51 × 7]&gt; &lt;kablExtr [1]&gt; Now, let’s print each of these tables and see what they demonstrate. 6.1.3.1.1 Procrastination, Accuracy Across people, accuracy was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly. px_best_mods$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and Accuracy for Predicting Procrastinating for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set Accuracy Feature Set Accuracy Feature Set Accuracy 01 Full, No Time 0.91 Full, No Time 0.91 Psychological, No Time 0.91 02 Full, Time 0.85 Full, Time 0.77 Full, No Time 0.83 05 Full, No Time 0.92 Full, No Time 0.92 Psychological, No Time 0.92 09 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.90 10 Psychological, No Time 0.82 Psychological, No Time 0.78 103 Full, No Time 0.62 Situations, No Time 0.69 Full, No Time 0.64 105 Full, No Time 0.92 Situations, Time 0.92 Full, No Time 0.92 106 Full, Time 0.88 Full, Time 0.81 Full, Time 0.87 107 Full, No Time 0.93 Psychological, No Time 0.93 Situations, No Time 0.93 108 Full, No Time 0.70 Psychological, No Time 0.70 Situations, No Time 0.58 110 Full, No Time 0.92 Full, Time 0.92 Situations, No Time 0.92 111 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 112 Psychological, Time 0.77 Full, Time 0.69 Psychological, Time 0.83 113 Full, No Time 0.90 118 Psychological, Time 0.80 Psychological, Time 0.80 Psychological, No Time 0.62 119 Psychological, Time 0.64 Full, No Time 0.64 Full, No Time 0.57 123 Situations, No Time 0.87 Psychological, Time 0.67 Full, No Time 0.79 124 Psychological, No Time 0.64 Full, No Time 0.64 Psychological, No Time 0.60 126 Psychological, Time 0.58 Psychological, Time 0.58 Full, Time 0.73 129 Psychological, No Time 0.50 Situations, No Time 0.67 Situations, No Time 0.83 133 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.90 135 Full, No Time 0.93 Full, Time 0.93 Situations, No Time 0.93 146 Full, Time 0.67 Full, Time 0.67 Full, Time 0.67 148 Full, No Time 0.58 Situations, Time 0.67 Situations, No Time 0.64 149 Full, No Time 0.58 Full, No Time 0.58 Situations, No Time 0.63 150 Situations, No Time 0.88 Full, No Time 0.72 Situations, No Time 0.83 152 Full, No Time 0.88 Full, No Time 0.88 Situations, No Time 0.88 155 Situations, Time 0.67 Psychological, Time 0.67 Full, No Time 0.59 156 Full, No Time 0.48 Psychological, Time 0.52 Situations, No Time 0.55 157 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.95 158 Situations, No Time 0.73 Full, No Time 0.60 Situations, No Time 0.93 159 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 160 Full, No Time 0.89 Full, No Time 0.89 Full, No Time 0.89 162 Full, No Time 0.93 Psychological, No Time 0.93 Situations, Time 0.92 164 Full, No Time 0.96 Situations, No Time 0.96 165 Full, No Time 0.64 Full, Time 0.73 Situations, No Time 0.70 166 Full, No Time 0.67 Full, No Time 0.67 Situations, No Time 0.82 167 Situations, Time 0.73 Full, No Time 0.64 Full, No Time 0.78 168 Full, No Time 0.62 Full, Time 0.62 Full, No Time 0.85 169 Psychological, No Time 0.94 Psychological, No Time 0.94 Psychological, No Time 0.93 17 Psychological, No Time 0.64 Psychological, Time 0.62 Situations, Time 0.77 170 Psychological, Time 0.94 Full, No Time 0.85 Full, No Time 0.83 171 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 174 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 185 Full, No Time 0.95 Psychological, Time 0.95 Full, No Time 0.95 186 Psychological, No Time 0.80 Full, No Time 0.80 Situations, No Time 0.71 188 Full, No Time 0.92 Full, No Time 0.83 Full, No Time 0.92 189 Psychological, Time 0.83 Situations, No Time 0.83 Situations, No Time 0.82 190 Psychological, No Time 0.93 Psychological, No Time 0.93 Psychological, Time 0.93 192 Full, No Time 0.92 Full, No Time 0.92 Full, No Time 0.92 195 Full, No Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 196 Full, No Time 0.83 Psychological, No Time 0.75 Situations, No Time 0.83 20 Full, No Time 0.79 Psychological, No Time 0.79 Situations, No Time 0.79 206 Full, No Time 0.89 Full, No Time 0.89 Situations, No Time 0.88 207 Full, No Time 0.86 Full, No Time 0.86 Full, No Time 0.85 21 Situations, No Time 0.60 Situations, No Time 0.67 Full, Time 0.57 211 Psychological, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 212 Full, Time 0.95 Psychological, No Time 0.95 Situations, No Time 0.95 214 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.93 216 Full, No Time 0.78 Full, No Time 0.78 Full, No Time 0.83 22 Full, No Time 0.71 Full, No Time 0.79 Full, No Time 0.82 220 Situations, No Time 0.92 Psychological, Time 0.73 Full, No Time 0.91 25 Full, No Time 0.75 Psychological, No Time 0.67 Situations, No Time 0.75 26 Full, No Time 0.81 Full, Time 0.79 Full, Time 0.85 27 Full, No Time 0.83 Full, No Time 0.75 Full, Time 0.89 30 Full, Time 0.92 Full, Time 0.92 Full, Time 0.92 31 Full, No Time 0.93 Full, No Time 0.93 Psychological, No Time 0.92 32 Full, No Time 0.86 Situations, Time 0.86 Situations, No Time 0.86 33 Full, No Time 0.91 Full, Time 0.91 Situations, No Time 0.91 35 Full, No Time 0.75 Full, No Time 0.67 Full, No Time 0.82 38 Psychological, No Time 0.73 Situations, No Time 0.67 Situations, No Time 0.64 40 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 43 Full, No Time 0.91 Situations, No Time 0.91 Full, No Time 0.90 49 Psychological, No Time 0.64 Psychological, No Time 0.64 Psychological, No Time 0.50 51 Psychological, No Time 0.92 Situations, No Time 0.91 Psychological, No Time 0.91 52 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.91 53 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 61 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.91 63 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 66 Full, No Time 0.91 Situations, No Time 0.91 67 Full, No Time 0.64 Full, No Time 0.64 Situations, No Time 0.69 71 Situations, No Time 0.82 Psychological, No Time 0.91 Full, No Time 0.70 73 Full, No Time 0.93 Psychological, No Time 0.93 Full, No Time 0.92 74 Full, No Time 0.93 Situations, No Time 0.92 77 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 78 Full, No Time 0.91 Full, No Time 0.90 84 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 85 Full, No Time 0.85 86 Full, Time 0.92 Full, No Time 0.93 Full, No Time 0.93 6.1.3.1.2 Procrastination, AUC Like accuracy, AUC in predicting future procrastination was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5. px_best_mods$tab[[2]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and AUC for Predicting Procrastinating for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set AUC Feature Set AUC Feature Set AUC 01 Situations, No Time 0.70 Situations, No Time 0.85 Full, No Time 0.56 02 Psychological, No Time 0.50 Situations, No Time 0.70 Full, Time 0.50 05 Full, No Time 0.91 Situations, No Time 0.95 Full, No Time 0.50 09 Psychological, No Time 0.20 Psychological, Time 0.70 Psychological, No Time 0.11 10 Psychological, No Time 0.56 Psychological, Time 0.57 103 Psychological, No Time 0.78 Full, No Time 0.57 Situations, No Time 0.72 105 Full, No Time 0.92 Full, No Time 0.42 Situations, No Time 0.82 106 Full, No Time 0.71 Situations, No Time 0.50 Psychological, Time 0.69 107 Situations, No Time 0.86 Full, No Time 0.86 Psychological, No Time 0.83 108 Full, No Time 0.67 Situations, Time 0.59 Psychological, Time 0.65 110 Full, No Time 0.82 Full, No Time 0.64 Situations, Time 0.50 111 Full, Time 0.67 Situations, Time 0.89 Psychological, No Time 0.88 112 Situations, Time 0.70 Psychological, Time 0.87 Psychological, Time 0.85 113 Situations, No Time 0.56 118 Psychological, Time 0.71 Psychological, No Time 0.29 Psychological, No Time 0.47 119 Psychological, No Time 0.69 Situations, Time 0.60 Psychological, Time 0.50 123 Situations, No Time 0.64 Psychological, No Time 0.69 Situations, Time 0.70 124 Psychological, No Time 0.50 Psychological, Time 0.79 Situations, No Time 0.58 126 Situations, Time 0.46 Full, No Time 0.77 Full, No Time 0.75 129 Situations, No Time 0.56 Psychological, Time 0.75 Situations, No Time 0.91 133 Full, No Time 0.33 Psychological, No Time 0.89 Full, No Time 0.89 135 Full, No Time 0.77 Situations, Time 0.77 Psychological, No Time 0.83 146 Psychological, No Time 0.54 Situations, Time 0.76 Situations, No Time 0.54 148 Situations, Time 0.49 Psychological, No Time 0.77 Situations, No Time 0.71 149 Psychological, No Time 0.44 Psychological, Time 0.75 Situations, No Time 0.75 150 Psychological, No Time 0.72 Situations, No Time 0.54 Situations, No Time 0.89 152 Situations, No Time 0.76 Psychological, Time 0.83 Situations, No Time 0.86 155 Situations, Time 0.71 Situations, Time 0.58 Situations, No Time 0.79 156 Situations, No Time 0.50 Situations, No Time 0.68 Situations, No Time 0.79 157 Situations, No Time 0.45 Psychological, No Time 0.79 Situations, Time 0.78 158 Full, Time 0.64 Situations, No Time 0.69 Situations, Time 0.85 159 Full, Time 0.50 Situations, No Time 0.69 Situations, No Time 0.67 160 Situations, No Time 0.94 Psychological, Time 0.84 Full, No Time 0.94 162 Situations, No Time 0.92 Full, Time 0.75 Full, Time 0.89 164 Full, No Time 0.91 Full, No Time 0.95 165 Situations, No Time 0.64 Full, No Time 0.71 Full, Time 0.95 166 Full, No Time 0.81 Psychological, No Time 0.53 Situations, Time 0.79 167 Situations, Time 0.77 Situations, Time 0.47 Full, No Time 0.67 168 Full, No Time 0.50 Full, No Time 0.70 Full, No Time 0.90 169 Psychological, No Time 0.80 Psychological, Time 0.40 Psychological, No Time 0.93 17 Psychological, No Time 0.88 Situations, No Time 0.58 Full, Time 0.78 170 Situations, No Time 0.91 Psychological, No Time 0.35 Situations, No Time 0.86 171 Full, Time 0.91 Full, Time 0.91 Situations, No Time 0.62 174 Situations, No Time 0.98 Psychological, Time 0.88 Situations, No Time 0.96 185 Situations, No Time 0.84 Situations, Time 0.89 Situations, No Time 0.53 186 Full, Time 0.93 Situations, No Time 0.16 Situations, Time 0.92 188 Situations, Time 0.55 Psychological, Time 0.91 Situations, No Time 0.55 189 Psychological, No Time 0.35 Full, Time 0.85 Situations, No Time 0.89 190 Psychological, Time 0.93 Psychological, No Time 0.79 Psychological, Time 0.86 192 Situations, Time 0.75 Psychological, No Time 0.83 Situations, No Time 0.67 195 Full, No Time 0.62 Full, No Time 0.75 Psychological, No Time 0.38 196 Full, No Time 0.50 Situations, Time 0.70 Full, No Time 0.67 20 Psychological, No Time 0.67 Situations, No Time 0.85 Situations, No Time 0.88 206 Psychological, No Time 0.41 Situations, Time 0.81 Situations, No Time 0.50 207 Situations, No Time 0.79 Full, Time 0.86 Full, No Time 0.91 21 Situations, Time 0.59 Psychological, No Time 0.56 Psychological, Time 0.52 211 Psychological, No Time 0.81 Psychological, No Time 0.88 Situations, No Time 0.56 212 Full, No Time 0.80 Psychological, No Time 0.55 Full, Time 0.78 214 Full, Time 0.87 Psychological, Time 0.87 Situations, No Time 0.93 216 Situations, No Time 0.68 Full, No Time 0.75 Situations, Time 0.49 22 Full, No Time 0.50 Psychological, Time 0.51 Full, No Time 0.50 220 Situations, No Time 0.50 Full, Time 0.90 Full, No Time 0.50 25 Full, No Time 0.78 Full, No Time 0.59 Full, No Time 0.50 26 Psychological, Time 0.67 Psychological, Time 0.70 Full, No Time 0.81 27 Situations, No Time 0.89 Psychological, Time 0.71 Situations, No Time 0.86 30 Psychological, Time 0.83 Full, No Time 0.79 Full, No Time 0.95 31 Full, No Time 0.92 Situations, No Time 0.81 Situations, Time 0.67 32 Situations, No Time 0.79 Psychological, No Time 0.54 Psychological, No Time 0.55 33 Full, No Time 0.80 Full, No Time 0.85 Psychological, Time 0.88 35 Psychological, No Time 0.67 Situations, No Time 0.43 Situations, No Time 0.61 38 Situations, No Time 0.44 Full, Time 0.73 Psychological, Time 0.55 40 Psychological, Time 0.70 Full, No Time 0.90 Situations, Time 0.60 43 Psychological, No Time 0.70 Situations, No Time 0.70 Situations, No Time 0.89 49 Psychological, No Time 0.87 Psychological, No Time 0.10 Psychological, No Time 0.50 51 Full, No Time 0.70 Full, Time 0.80 Full, Time 0.56 52 Full, No Time 0.90 Situations, Time 0.80 Full, Time 0.62 53 Full, No Time 0.79 Situations, No Time 0.64 Psychological, Time 0.50 61 Psychological, Time 0.80 Psychological, Time 0.70 Psychological, No Time 0.70 63 Situations, Time 0.80 Full, No Time 0.90 Psychological, No Time 0.80 66 Psychological, No Time 0.90 Full, Time 0.71 67 Psychological, No Time 0.44 Psychological, Time 0.98 Psychological, No Time 0.50 71 Full, Time 0.92 Full, Time 0.96 Situations, Time 0.71 73 Psychological, No Time 0.31 Full, Time 0.92 Psychological, No Time 0.75 74 Full, No Time 0.85 Situations, Time 0.92 77 Psychological, No Time 0.22 Full, No Time 0.78 Full, No Time 0.71 78 Situations, No Time 0.50 84 Psychological, Time 0.42 Full, No Time 0.88 Situations, No Time 0.83 85 Situations, No Time 0.61 86 Psychological, Time 0.92 Full, No Time 0.77 Full, No Time 0.85 6.1.3.1.3 Loneliness, Accuracy Across people, accuracy in predicting future loneliness was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly. There are some exceptions to this, but the magnitude of these differences remains relatively small (magnitude of about .1 at most). px_best_mods$tab[[3]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and Accuracy for Predicting Lonely for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set Accuracy Feature Set Accuracy Feature Set Accuracy 08 Full, No Time 0.83 Full, No Time 0.92 Full, No Time 0.82 10 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.89 103 Full, No Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 105 Psychological, No Time 0.80 Psychological, No Time 0.80 Situations, No Time 0.92 106 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 108 Full, Time 0.80 Psychological, No Time 0.80 Full, No Time 0.81 112 Full, No Time 0.69 Full, No Time 0.69 Situations, No Time 0.69 118 Psychological, Time 0.80 Psychological, Time 0.70 Psychological, Time 0.62 119 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.93 123 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.93 129 Psychological, No Time 0.67 Situations, No Time 0.67 Situations, Time 0.75 133 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.90 135 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 146 Psychological, Time 0.94 Psychological, No Time 0.95 Psychological, No Time 0.95 149 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.95 150 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 152 Psychological, Time 0.96 Full, No Time 0.92 Situations, No Time 0.92 154 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 155 Situations, Time 0.61 Psychological, No Time 0.78 Full, Time 0.71 157 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 158 Full, Time 0.93 Full, No Time 0.93 Full, No Time 0.93 160 Full, No Time 0.83 Full, No Time 0.83 Full, No Time 0.83 164 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 167 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.90 168 Psychological, No Time 0.69 Psychological, Time 0.77 Full, Time 0.92 170 Psychological, No Time 0.95 Psychological, No Time 0.95 Psychological, No Time 0.94 18 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.94 185 Full, No Time 0.65 Full, No Time 0.70 Full, No Time 0.65 186 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 196 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 20 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 201 Situations, No Time 0.82 Situations, No Time 0.82 Situations, No Time 0.91 206 Full, No Time 0.89 Full, No Time 0.89 Situations, No Time 0.88 207 Full, No Time 0.86 Psychological, No Time 0.86 Full, No Time 0.85 211 Psychological, No Time 0.81 Full, No Time 0.81 Full, No Time 0.83 212 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.95 214 Full, No Time 0.75 Full, No Time 0.75 Full, No Time 0.75 220 Full, No Time 0.92 Full, No Time 0.92 Full, No Time 0.91 25 Full, No Time 0.75 Psychological, No Time 0.75 Full, No Time 0.70 27 Full, No Time 0.83 Psychological, Time 0.82 Situations, No Time 0.83 32 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 35 Full, No Time 0.83 Psychological, No Time 0.75 Full, No Time 0.82 36 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.91 43 Situations, No Time 0.91 Situations, No Time 0.91 Situations, No Time 0.90 51 Situations, No Time 0.82 Psychological, Time 0.75 Psychological, No Time 0.73 53 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 63 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 67 Situations, No Time 0.93 71 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.90 73 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.92 84 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 6.1.3.1.4 Loneliness, AUC Like accuracy, AUC in predicting future loneliness was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5. px_best_mods$tab[[4]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and AUC for Predicting Lonely for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set AUC Feature Set AUC Feature Set AUC 08 Full, Time 0.80 Situations, Time 0.30 Full, Time 0.78 10 Psychological, Time 0.80 Psychological, No Time 0.70 Psychological, No Time 0.88 103 Situations, Time 0.93 Psychological, No Time 0.87 Psychological, Time 0.92 105 Psychological, Time 0.72 Full, No Time 0.53 Situations, No Time 0.90 106 Psychological, Time 0.73 Full, No Time 0.97 Psychological, No Time 0.93 108 Psychological, No Time 0.50 Situations, No Time 0.86 Situations, Time 0.32 112 Psychological, Time 0.64 Situations, No Time 0.54 Situations, Time 0.75 118 Psychological, Time 0.44 Psychological, No Time 0.54 Psychological, Time 0.73 119 Situations, No Time 0.77 Psychological, Time 0.77 Full, Time 0.92 123 Full, No Time 0.93 Situations, No Time 0.50 Full, No Time 0.92 129 Full, No Time 0.86 Situations, No Time 0.29 Situations, No Time 0.97 133 Situations, No Time 0.33 Situations, No Time 0.94 Situations, No Time 0.89 135 Psychological, Time 0.92 Full, No Time 0.88 Situations, No Time 0.62 146 Psychological, No Time 0.89 Situations, Time 0.75 Situations, Time 0.50 149 Psychological, No Time 0.72 Situations, Time 0.94 Full, Time 0.86 150 Situations, No Time 0.83 Psychological, No Time 0.75 Full, No Time 0.95 152 Full, No Time 0.91 Situations, Time 0.34 Full, No Time 0.86 154 Situations, Time 0.93 Psychological, No Time 0.94 Full, No Time 0.86 155 Situations, No Time 0.48 Situations, No Time 0.64 Psychological, Time 0.86 157 Full, No Time 0.37 Full, Time 0.89 Full, No Time 0.94 158 Psychological, Time 0.93 Situations, No Time 0.89 Full, Time 0.92 160 Situations, No Time 0.78 Full, Time 0.42 Full, Time 0.79 164 Psychological, Time 0.86 Situations, Time 0.91 Full, No Time 0.70 167 Situations, Time 0.90 Full, No Time 0.60 Psychological, No Time 0.50 168 Situations, No Time 0.73 Psychological, No Time 0.48 Psychological, Time 0.97 170 Psychological, No Time 0.94 Psychological, No Time 0.22 Psychological, No Time 0.81 18 Situations, Time 0.72 Psychological, No Time 0.94 Situations, No Time 0.76 185 Psychological, No Time 0.52 Psychological, Time 0.68 Full, No Time 0.64 186 Full, Time 0.86 Situations, Time 0.93 Psychological, No Time 0.92 196 Psychological, No Time 0.82 Situations, No Time 0.41 Full, No Time 0.80 20 Full, No Time 0.08 Full, No Time 0.92 Situations, No Time 0.46 201 Psychological, No Time 0.83 Situations, No Time 0.69 Full, No Time 0.88 206 Psychological, No Time 0.69 Situations, No Time 0.72 Psychological, No Time 0.64 207 Situations, Time 0.95 Psychological, No Time 0.67 Full, No Time 0.86 211 Situations, Time 0.43 Full, Time 0.82 Psychological, Time 0.46 212 Situations, No Time 0.72 Psychological, Time 0.89 Full, No Time 0.78 214 Situations, No Time 0.58 Psychological, Time 0.48 Situations, Time 0.64 220 Full, No Time 0.64 Situations, Time 0.80 Psychological, Time 0.70 25 Situations, No Time 0.74 Full, No Time 0.59 Situations, No Time 0.74 27 Full, Time 0.83 Situations, No Time 0.62 Full, No Time 0.50 32 Full, Time 0.77 Psychological, No Time 0.92 Psychological, No Time 0.80 35 Psychological, No Time 0.75 Situations, No Time 0.85 Full, No Time 0.78 36 Situations, Time 0.60 Situations, No Time 0.91 Situations, No Time 0.70 43 Full, No Time 0.83 Psychological, No Time 0.67 Situations, No Time 0.88 51 Full, Time 0.83 Situations, Time 0.42 Situations, No Time 0.57 53 Situations, No Time 0.93 Full, No Time 0.75 Situations, No Time 0.93 63 Situations, Time 0.80 Full, No Time 0.90 Psychological, No Time 0.90 67 Situations, Time 0.54 71 Psychological, Time 0.60 Full, No Time 0.90 Situations, No Time 0.56 73 Psychological, No Time 0.92 Situations, No Time 0.92 Full, Time 0.83 84 Psychological, No Time 0.75 Situations, No Time 0.67 Psychological, No Time 0.80 6.1.3.2 Classification Accuracy and AUC 6.1.3.2.1 Table Similar to how we created tables for each outcome, feature set, and metric in the first section, we will next create a single, similar table for participants best models, summarizing the mean, standard deviation, median, and range for each outcome, method, and metric. In the manuscript, this will be summarized in a figure, but I’m still creating the table for ease of access. bm_tab &lt;- best_mods %&gt;% group_by(model, outcome, .metric) %&gt;% summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median), model = factor(model, levels = c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;)), .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) ) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = &quot;outcome&quot; , values_from = c(mean, median, range, n) , names_glue = &quot;{outcome}_{.value}&quot;) %&gt;% arrange(model, .metric) %&gt;% select(.metric, contains(&quot;lonely&quot;), contains(&quot;prcrst&quot;)) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Metric&quot;, rep(c(&quot;&lt;em&gt;M&lt;/em&gt; (&lt;em&gt;SD&lt;/em&gt;)&quot;, &quot;Median&quot;, &quot;Range&quot;, &quot;&lt;em&gt;N&lt;/em&gt;&quot;), times = 2)) , align = c(&quot;r&quot;, rep(&quot;c&quot;,8)) , cap = &quot;&lt;strong&gt;Table X&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Model Performance Across of the Best Performing Model for Each Participant&lt;/em&gt;&quot; ) %&gt;% kable_styling(full_width = F) %&gt;% kableExtra::group_rows(&quot;Elastic Net&quot;, 1, 2) %&gt;% kableExtra::group_rows(&quot;BISCWIT&quot;, 3, 4) %&gt;% kableExtra::group_rows(&quot;Random Forest&quot;, 5, 6) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Loneliness&quot; = 4, &quot;Procrastination&quot; = 4)) %&gt;% footnote(&quot;Accuracy = Classification accuracy; AUC = Area under the receiver operating characteristic (ROC) curve.&quot;) save_kable(bm_tab, file = sprintf(&quot;%s/05-results/04-tables/01-best-summary.html&quot;, local_path)) bm_tab Table 6.2: Table XDescriptive Statistics of Model Performance Across of the Best Performing Model for Each Participant Loneliness Procrastination Metric M (SD) Median Range N M (SD) Median Range N Elastic Net Accuracy 0.87 (0.09) 0.91 0.61-0.96 50 0.82 (0.13) 0.88 0.48-0.96 82 AUC 0.74 (0.19) 0.77 0.08-0.95 50 0.69 (0.19) 0.70 0.20-0.98 82 BISCWIT Accuracy 0.87 (0.08) 0.92 0.67-0.96 51 0.82 (0.12) 0.88 0.52-0.96 89 AUC 0.71 (0.21) 0.75 0.22-0.97 51 0.71 (0.18) 0.75 0.10-0.98 89 Random Forest Accuracy 0.87 (0.09) 0.91 0.62-0.96 50 0.83 (0.12) 0.89 0.50-0.96 87 AUC 0.77 (0.16) 0.80 0.32-0.97 50 0.71 (0.17) 0.72 0.11-0.96 86 Note: Accuracy = Classification accuracy; AUC = Area under the receiver operating characteristic (ROC) curve. There are a few key takeaways from this table. First, accuracy across all models and outcomes was quite high, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination. 6.1.3.2.2 Figure (Figure 1) Now, we’ll create distributions of the performance (accuracy, AUC) of participants’ best models and plot those along with the descriptive statistics that were created for the Supplementary Table in the previous section. This figure will become Figure 1 in the manuscript. p_dist_fun &lt;- function(d, outcome) { o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) d %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;) , c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)) , .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) , group = factor(str_to_title(group))) %&gt;% ggplot(aes(y = model, x = .estimate)) + scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + geom_density_ridges(aes(fill = model), alpha = .5) + stat_pointinterval() + geom_vline(aes(xintercept = .5), linetype = &quot;dashed&quot;) + labs(x = NULL, y = NULL, title = o) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(legend.position = &quot;none&quot; , axis.text = element_text(face = &quot;bold&quot;) , axis.title = element_text(face = &quot;bold&quot;) , strip.background = element_blank() , strip.text.y = element_blank() , plot.margin = margin(.1,.1,1,.1, unit = &quot;cm&quot;) , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) , plot.title = element_text(face = &quot;bold&quot;, size = rel(1.2), hjust = .5)) } bm_dist &lt;- best_mods %&gt;% group_by(outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = map2(data, outcome, p_dist_fun)) tab_fun &lt;- function(d){ tab &lt;- d %&gt;% select(-model) %&gt;% setNames(c(&quot;M (SD)&quot;, &quot;Median&quot;, &quot;N&quot;, &quot;Range&quot;)) %&gt;% tableGrob(rows = NULL , theme = ttheme_minimal(base_family = &quot;Times&quot;)) tab &lt;- gtable_add_grob(tab, grobs = segmentsGrob( # line across the bottom x0 = unit(0,&quot;npc&quot;), y0 = unit(0,&quot;npc&quot;), x1 = unit(1,&quot;npc&quot;), y1 = unit(0,&quot;npc&quot;), gp = gpar(lwd = 2.0)), t = 1, b = 1, l = 1, ncol(tab)) tab$grobs[1:4] &lt;- lapply(tab$grobs[1:4], function(x) {x$grobs[[1]]$gp$fontface = &quot;bold&quot;; return(x)}) return(tab) } bm_tbl &lt;- best_mods %&gt;% group_by(model, outcome, .metric) %&gt;% summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median), model = factor(model, levels = c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), labels = c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) ) %&gt;% select(-sd, -min, -max) %&gt;% arrange(outcome, .metric, model) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = map(data, tab_fun)) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) ttl &lt;- ggdraw() + draw_label( &quot;Procrastination&quot;, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) bm_dist$p[[1]] &lt;- bm_dist$p[[1]] + labs(title = NULL) bm_tab1 &lt;- plot_grid(bm_tbl$tab[[3]], bm_tbl$tab[[4]], nrow = 2, rel_heights = c(.4, .4)) bm_prcrst &lt;- plot_grid(bm_dist$p[[1]], bm_tab1, ncol = 2) bm_prcrst &lt;- plot_grid(ttl, bm_prcrst, nrow = 2, rel_heights = c(.05,.95)) bm_dist$p[[2]] &lt;- bm_dist$p[[2]] + labs(title = NULL) + theme(axis.text.y = element_blank()) ttl &lt;- ggdraw() + draw_label( &quot;Loneliness&quot;, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) bm_tab2 &lt;- plot_grid(bm_tbl$tab[[1]], bm_tbl$tab[[2]], nrow = 2) bm_lonely &lt;- plot_grid(bm_dist$p[[2]], bm_tab2, ncol = 2, rel_widths = c(.4, .6)) bm_lonely &lt;- plot_grid(ttl, bm_lonely, nrow = 2, rel_heights = c(.05,.95)) bm_plot &lt;- plot_grid(bm_prcrst, bm_lonely, ncol = 2, rel_widths = c(.55, .45)); bm_plot Figure 6.1: Histograms of classification accuracy and Area Under the Receiver Operator Curve (AUC) for participants’ best models. ggsave(bm_plot, file = sprintf(&quot;%s/05-results/05-figures/fig-1-best-models.pdf&quot;, local_path) , width = 12, height = 5) ggsave(bm_plot, file = sprintf(&quot;%s/05-results/05-figures/fig-1-best-models.png&quot;, local_path) , width = 12, height = 5) Figure 1 presents histograms and descriptive statistics of accuracy and AUC across the full sample for each outcome and model. As is clear in the figure, predictive accuracy was high overall, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination. 6.1.4 Tuning Parameters (Table) Next, I’m going to create tables that include the tuning parameters, features, and accuracy for each participants best model for each machine learning method and outcome as well as which feature set was used in their best model. px_tun_par_tab_fun &lt;- function(d, model, outcome){ if(model == &quot;glmnet&quot;){ cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;Penalty&quot;, &quot;Mixture&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 4)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, penalty, mixture, nvars, .estimate) %&gt;% mutate(penalty = ifelse(penalty &lt; .01, sprintf(&quot;%.1e&quot;, penalty), sprintf(&quot;%.2f&quot;, penalty)) , .estimate = sprintf(&quot;%.2f&quot;, .estimate) , mixture = ifelse(mixture == 0, &quot;0&quot;, sprintf(&quot;%.2f&quot;, mixture))) } else if(model == &quot;rf&quot;){ cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;# Features Sampled&quot;, &quot;Min N for Split&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 4)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, mtry, min_n, nvars, .estimate) %&gt;% mutate(.estimate = sprintf(&quot;%.2f&quot;, .estimate)) } else { cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;# Items&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 3)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, nitem, nvars, .estimate) %&gt;% mutate(.estimate = sprintf(&quot;%.2f&quot;, .estimate)) } o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) m &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;rf&quot;, &quot;biscwit&quot;), c(&quot;Elastic Net&quot;, &quot;Random Forest&quot;, &quot;BISCWIT&quot;)) cap &lt;- sprintf(&quot;&lt;strong&gt;Table X&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Tuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants&#39; Best Model of %s Using %s&quot;, o, m) tab &lt;- tab %&gt;% kable(. , &quot;html&quot; , escape = &quot;F&quot; , col.names = cn , align = al , cap = cap ) %&gt;% kable_styling(full_width = F) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/03-px-tuning-params/%s_%s.html&quot;, local_path, outcome, model)) return(tab) } tuning_param &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config)) %&gt;% select(-coefs) %&gt;% group_by(outcome, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, ~(.) %&gt;% unnest(params) %&gt;% filter(.metric == &quot;accuracy&quot;)), tab = pmap(list(data, model, outcome), px_tun_par_tab_fun)) 6.1.4.1 Elastic Net Rather than splitting these by outcome and model, I’m going to do a broad discussion across outcomes. From the tables, a few things become clear – penalties tended to be quite low (near 0) or quite high (near 1). Indeed, of the 10 tested values, only 3 appeared: 0.0000000001, 0.08, and 1.00. For mixture, the most frequent value was 0, but there was was a also more variability than for penalty, with almost all of the 10 possible values being represented. The number of features tended to vary quite widely and does not appear to be a function of stronger penalties or mixture values. The tuning parameters also appear to have little effect on model accuracy. 6.1.4.1.1 Procrastination (tuning_param %&gt;% filter(model == &quot;glmnet&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using Elastic Net ID Group Penalty Mixture # Features Accuracy 01 Full, No Time 1.0e-10 0 49 0.91 02 Full, Time 1.0e-10 0 64 0.85 05 Full, No Time 1.0e-10 0 46 0.92 09 Psychological, No Time 1.0e-10 0 25 0.91 103 Full, No Time 1.00 0.22 43 0.62 105 Full, No Time 0.08 0.78 19 0.92 106 Full, Time 1.00 0.11 57 0.88 107 Full, No Time 0.08 0.11 42 0.93 108 Full, No Time 0.08 0.11 43 0.70 110 Full, No Time 1.0e-10 0 45 0.92 111 Full, No Time 1.0e-10 0.11 44 0.90 112 Psychological, Time 1.0e-10 0 41 0.77 118 Psychological, Time 1.0e-10 1.00 18 0.80 119 Psychological, Time 1.0e-10 0 41 0.64 123 Situations, No Time 0.08 0.22 22 0.87 124 Psychological, No Time 1.00 0.33 24 0.64 126 Psychological, Time 1.0e-10 0 42 0.58 129 Psychological, No Time 1.0e-10 0.89 17 0.50 133 Full, No Time 1.0e-10 0 49 0.90 135 Full, No Time 1.0e-10 0 45 0.93 146 Full, Time 1.0e-10 0 62 0.67 148 Full, No Time 1.0e-10 0 47 0.58 149 Full, No Time 0.08 0.33 41 0.58 150 Situations, No Time 0.08 0.44 17 0.88 152 Full, No Time 1.00 0 45 0.88 155 Situations, Time 1.0e-10 0.11 36 0.67 156 Full, No Time 1.00 0 46 0.48 157 Full, No Time 1.0e-10 0 44 0.90 158 Situations, No Time 0.08 1.00 19 0.73 159 Full, No Time 1.0e-10 0 47 0.90 160 Full, No Time 1.0e-10 0 46 0.89 162 Full, No Time 1.0e-10 0 46 0.93 165 Full, No Time 1.0e-10 0 47 0.64 166 Full, No Time 1.0e-10 0 42 0.67 167 Situations, Time 1.0e-10 0.11 30 0.73 168 Full, No Time 1.00 0.11 45 0.62 169 Psychological, No Time 0.08 0 25 0.94 17 Psychological, No Time 1.0e-10 0 25 0.64 170 Psychological, Time 1.0e-10 0 41 0.94 171 Full, No Time 1.0e-10 0 28 0.96 174 Full, No Time 1.00 0 30 0.96 185 Full, No Time 1.00 0 45 0.95 186 Psychological, No Time 1.0e-10 0 24 0.80 188 Full, No Time 1.0e-10 0 49 0.92 189 Psychological, Time 1.0e-10 0 41 0.83 190 Psychological, No Time 1.00 0.11 25 0.93 192 Full, No Time 1.0e-10 0 48 0.92 195 Full, No Time 1.0e-10 0 47 0.94 196 Full, No Time 1.00 0.22 37 0.83 20 Full, No Time 1.0e-10 0 46 0.79 206 Full, No Time 1.0e-10 0 49 0.89 207 Full, No Time 1.0e-10 0 47 0.86 21 Situations, No Time 1.00 0 22 0.60 211 Psychological, No Time 1.00 0 25 0.96 212 Full, Time 1.00 0.11 59 0.95 214 Full, Time 1.0e-10 0 63 0.94 216 Full, No Time 1.0e-10 0 34 0.78 22 Full, No Time 1.00 0.22 40 0.71 220 Situations, No Time 1.00 0.22 23 0.92 25 Full, No Time 1.0e-10 0.78 28 0.75 26 Full, No Time 1.0e-10 0 48 0.81 27 Full, No Time 6.0e-03 0.89 28 0.83 30 Full, Time 1.0e-10 0 63 0.92 31 Full, No Time 1.0e-10 0 44 0.93 32 Full, No Time 1.0e-10 0 49 0.86 33 Full, No Time 1.0e-10 0 45 0.91 35 Full, No Time 1.0e-10 0 44 0.75 38 Psychological, No Time 0.08 0.11 25 0.73 40 Full, No Time 1.0e-10 0 46 0.91 43 Full, No Time 1.0e-10 0 47 0.91 49 Psychological, No Time 0.08 0 24 0.64 51 Psychological, No Time 1.00 0.22 23 0.92 52 Full, No Time 1.0e-10 0 47 0.91 53 Full, No Time 1.0e-10 0 47 0.93 61 Psychological, No Time 1.0e-10 0 25 0.91 63 Full, No Time 1.0e-10 0 49 0.91 67 Full, No Time 1.0e-10 0 48 0.64 71 Situations, No Time 1.0e-10 0 21 0.82 73 Full, No Time 1.00 0.11 43 0.93 77 Full, No Time 1.0e-10 0 48 0.90 84 Full, No Time 1.0e-10 0 47 0.92 86 Full, Time 1.0e-10 0 63 0.92 6.1.4.1.2 Loneliness (tuning_param %&gt;% filter(model == &quot;glmnet&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using Elastic Net ID Group Penalty Mixture # Features Accuracy 08 Full, No Time 1.0e-10 0 48 0.83 10 Psychological, No Time 1.0e-10 0 23 0.91 103 Full, No Time 1.0e-10 0 47 0.94 105 Psychological, No Time 1.00 0 25 0.80 106 Full, Time 1.0e-10 0 62 0.94 108 Full, Time 1.0e-10 0 60 0.80 112 Full, No Time 1.0e-10 0 46 0.69 118 Psychological, Time 1.0e-10 0 41 0.80 119 Full, No Time 1.0e-10 0 48 0.93 123 Full, No Time 1.0e-10 0 46 0.93 129 Psychological, No Time 1.0e-10 0 25 0.67 133 Full, No Time 1.0e-10 0 49 0.90 135 Full, No Time 1.0e-10 0 45 0.93 146 Psychological, Time 1.0e-10 0 41 0.94 149 Full, No Time 1.0e-10 0 46 0.95 150 Full, No Time 1.00 0 42 0.96 152 Psychological, Time 1.0e-10 0 42 0.96 154 Full, Time 1.0e-10 0 64 0.94 155 Situations, Time 1.00 0.11 36 0.61 157 Full, No Time 1.0e-10 0 44 0.90 158 Full, Time 1.0e-10 0 60 0.93 160 Full, No Time 1.0e-10 0 46 0.83 164 Full, No Time 1.0e-10 0 42 0.96 167 Full, No Time 1.0e-10 0 47 0.91 168 Psychological, No Time 1.0e-10 0 25 0.69 170 Psychological, No Time 1.00 0 25 0.95 18 Full, No Time 0.08 0 47 0.95 185 Full, No Time 1.00 0 45 0.65 186 Full, No Time 1.0e-10 0 45 0.93 196 Full, No Time 0.08 0.67 27 0.92 20 Full, No Time 1.0e-10 0 46 0.93 201 Situations, No Time 1.0e-10 0 23 0.82 206 Full, No Time 1.00 0.11 48 0.89 207 Full, No Time 1.0e-10 0 47 0.86 211 Psychological, No Time 0.08 0 25 0.81 212 Full, No Time 1.0e-10 0 48 0.95 214 Full, No Time 1.0e-10 0 46 0.75 220 Full, No Time 1.0e-10 0 48 0.92 25 Full, No Time 0.08 1.00 19 0.75 27 Full, No Time 1.0e-10 0 49 0.83 32 Full, No Time 1.0e-10 0 49 0.93 35 Full, No Time 1.0e-10 0 44 0.83 36 Full, No Time 1.0e-10 0 48 0.92 43 Situations, No Time 1.0e-10 0.44 18 0.91 51 Situations, No Time 1.0e-10 0 22 0.82 53 Full, No Time 1.0e-10 0 47 0.93 63 Full, No Time 1.0e-10 0 49 0.91 71 Full, No Time 1.0e-10 0 46 0.91 73 Full, No Time 1.0e-10 0 49 0.93 84 Full, No Time 1.0e-10 0 47 0.92 6.1.4.2 BISCWIT 6.1.4.2.1 Procrastination The only turning parameter for BISCWIT was the number of items selected through rolling origin validation. As is clear in the table, relative to ENR, BISCWIT tended to select fewer features (e.g., Participant 01 had the full feature set with 22 features for BISCWIT but 49 features for ENR). Divergences in feature numbers are due to ties. As with ENR, the number of features did not appear to be related to the accuracy of the model and the was a wide range in which feature set produced the best model. (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using BISCWIT ID Group # Items # Features Accuracy 01 Full, No Time 21 22 0.91 02 Full, Time 3 5 0.77 05 Full, No Time 6 7 0.92 09 Psychological, No Time 3 4 0.91 10 Psychological, No Time 6 7 0.82 103 Situations, No Time 18 17 0.69 105 Situations, Time 3 4 0.92 106 Full, Time 6 7 0.81 107 Psychological, No Time 9 10 0.93 108 Psychological, No Time 21 13 0.70 110 Full, Time 6 7 0.92 111 Full, No Time 24 25 0.90 112 Full, Time 51 38 0.69 113 Full, No Time 3 4 0.90 118 Psychological, Time 3 4 0.80 119 Full, No Time 36 26 0.64 123 Psychological, Time 12 13 0.67 124 Full, No Time 24 25 0.64 126 Psychological, Time 30 26 0.58 129 Situations, No Time 18 19 0.67 133 Full, No Time 3 4 0.90 135 Full, Time 39 37 0.93 146 Full, Time 3 4 0.67 148 Situations, Time 3 4 0.67 149 Full, No Time 3 4 0.58 150 Full, No Time 3 4 0.72 152 Full, No Time 15 11 0.88 155 Psychological, Time 6 7 0.67 156 Psychological, Time 27 20 0.52 157 Full, No Time 3 4 0.90 158 Full, No Time 9 10 0.60 159 Full, No Time 6 7 0.90 160 Full, No Time 18 19 0.89 162 Psychological, No Time 3 4 0.93 164 Full, No Time 3 4 0.96 165 Full, Time 24 25 0.73 166 Full, No Time 36 25 0.67 167 Full, No Time 9 10 0.64 168 Full, Time 36 33 0.62 169 Psychological, No Time 9 10 0.94 17 Psychological, Time 9 10 0.62 170 Full, No Time 21 22 0.85 171 Full, No Time 3 4 0.96 174 Full, No Time 3 4 0.96 185 Psychological, Time 24 21 0.95 186 Full, No Time 15 16 0.80 188 Full, No Time 36 31 0.83 189 Situations, No Time 6 8 0.83 190 Psychological, No Time 12 13 0.93 192 Full, No Time 6 7 0.92 195 Full, No Time 18 19 0.94 196 Psychological, No Time 15 13 0.75 20 Psychological, No Time 3 4 0.79 206 Full, No Time 3 4 0.89 207 Full, No Time 3 4 0.86 21 Situations, No Time 9 10 0.67 211 Full, No Time 3 4 0.96 212 Psychological, No Time 18 13 0.95 214 Full, No Time 9 10 0.94 216 Full, No Time 6 7 0.78 22 Full, No Time 3 4 0.79 220 Psychological, Time 3 4 0.73 25 Psychological, No Time 6 7 0.67 26 Full, Time 36 34 0.79 27 Full, No Time 9 10 0.75 30 Full, Time 3 4 0.92 31 Full, No Time 9 10 0.93 32 Situations, Time 3 4 0.86 33 Full, Time 3 4 0.91 35 Full, No Time 15 16 0.67 38 Situations, No Time 3 4 0.67 40 Full, No Time 3 4 0.91 43 Situations, No Time 15 14 0.91 49 Psychological, No Time 3 4 0.64 51 Situations, No Time 3 4 0.91 52 Full, No Time 3 6 0.91 53 Full, No Time 15 16 0.93 61 Psychological, No Time 18 17 0.91 63 Full, No Time 15 16 0.91 66 Full, No Time 24 25 0.91 67 Full, No Time 15 16 0.64 71 Psychological, No Time 3 4 0.91 73 Psychological, No Time 6 7 0.93 74 Full, No Time 9 10 0.93 77 Full, No Time 6 7 0.90 78 Full, No Time 9 11 0.91 84 Full, No Time 3 4 0.92 86 Full, No Time 12 13 0.93 6.1.4.2.2 Loneliness (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using BISCWIT ID Group # Items # Features Accuracy 08 Full, No Time 3 5 0.92 10 Psychological, No Time 3 4 0.91 103 Full, No Time 33 31 0.94 105 Psychological, No Time 3 4 0.80 106 Full, No Time 3 4 0.94 108 Psychological, No Time 3 4 0.80 112 Full, No Time 6 7 0.69 118 Psychological, Time 15 16 0.70 119 Full, No Time 15 16 0.93 123 Full, No Time 9 10 0.93 129 Situations, No Time 9 12 0.67 133 Full, No Time 36 34 0.90 135 Full, No Time 3 5 0.93 146 Psychological, No Time 3 4 0.95 149 Full, No Time 33 24 0.95 150 Full, No Time 3 4 0.96 152 Full, No Time 3 4 0.92 154 Full, No Time 36 35 0.94 155 Psychological, No Time 6 7 0.78 157 Full, No Time 3 4 0.90 158 Full, No Time 18 19 0.93 160 Full, No Time 3 4 0.83 164 Full, No Time 3 4 0.96 167 Full, No Time 3 4 0.91 168 Psychological, Time 9 10 0.77 170 Psychological, No Time 3 4 0.95 18 Full, No Time 12 13 0.95 185 Full, No Time 3 4 0.70 186 Full, No Time 24 21 0.93 196 Full, No Time 18 19 0.92 20 Full, No Time 3 4 0.93 201 Situations, No Time 3 4 0.82 206 Full, No Time 3 4 0.89 207 Psychological, No Time 3 4 0.86 211 Full, No Time 3 4 0.81 212 Full, No Time 6 7 0.95 214 Full, No Time 6 7 0.75 220 Full, No Time 6 7 0.92 25 Psychological, No Time 15 14 0.75 27 Psychological, Time 33 33 0.82 32 Full, No Time 3 4 0.93 35 Psychological, No Time 9 10 0.75 36 Full, No Time 3 4 0.92 43 Situations, No Time 12 13 0.91 51 Psychological, Time 24 25 0.75 53 Full, No Time 3 4 0.93 63 Full, No Time 15 16 0.91 71 Full, No Time 3 4 0.91 73 Full, No Time 6 7 0.93 84 Full, No Time 3 4 0.92 6.1.4.3 Random Forest Random forest used two tuning parameters, the number of features sampled from the feature set to train the model and the minimum sample size in each group needed for a binary split. The number of features sampled in each small tree tended to be smaller than the final number of features selected but varied widely across people. The minimum N for a split also varied quite widely. However, 10 was the most frequent number, which logically makes sense given the sample sizes in the present study. Because we used time delay embedding to preserve the “order” of the time series, the final number of features here tended to be larger than other methods (the number of possible features was doubled using an embedding dimension of 1). Each of these appeared to unrelated to accuracy. ##### Procrastination (tuning_param %&gt;% filter(model == &quot;rf&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using Random Forest ID Group # Features Sampled Min N for Split # Features Accuracy 01 Psychological, No Time 17 10 29 0.91 02 Full, No Time 32 10 80 0.83 05 Psychological, No Time 17 10 46 0.92 09 Psychological, No Time 17 10 0 0.90 10 Psychological, No Time 16 10 27 0.78 103 Full, No Time 32 10 80 0.64 105 Full, No Time 32 10 74 0.92 106 Full, Time 2 40 105 0.87 107 Situations, No Time 6 35 29 0.93 108 Situations, No Time 1 40 35 0.58 110 Situations, No Time 15 10 39 0.92 111 Situations, No Time 1 40 0 0.90 112 Psychological, Time 23 10 67 0.83 118 Psychological, No Time 17 10 50 0.62 119 Full, No Time 33 10 74 0.57 123 Full, No Time 32 10 82 0.79 124 Psychological, No Time 38 33 0 0.60 126 Full, Time 39 10 97 0.73 129 Situations, No Time 8 3 45 0.83 133 Full, No Time 32 10 0 0.90 135 Situations, No Time 16 10 40 0.93 146 Full, Time 36 10 28 0.67 148 Situations, No Time 33 21 38 0.64 149 Situations, No Time 1 40 42 0.63 150 Situations, No Time 6 3 33 0.83 152 Situations, No Time 1 40 40 0.88 155 Full, No Time 31 10 87 0.59 156 Situations, No Time 32 28 39 0.55 157 Full, No Time 31 10 76 0.95 158 Situations, No Time 7 3 36 0.93 159 Situations, No Time 15 10 26 0.90 160 Full, No Time 2 40 87 0.89 162 Situations, Time 44 21 16 0.92 164 Situations, No Time 12 10 24 0.96 165 Situations, No Time 15 10 41 0.70 166 Situations, No Time 10 9 39 0.82 167 Full, No Time 88 5 82 0.78 168 Full, No Time 63 28 61 0.85 169 Psychological, No Time 17 10 41 0.93 17 Situations, Time 15 9 66 0.77 170 Full, No Time 32 10 49 0.83 171 Situations, No Time 9 10 17 0.96 174 Situations, No Time 9 10 23 0.96 185 Full, No Time 30 10 78 0.95 186 Situations, No Time 13 10 26 0.71 188 Full, No Time 33 10 63 0.92 189 Situations, No Time 15 10 38 0.82 190 Psychological, Time 23 10 62 0.93 192 Full, No Time 32 10 28 0.92 195 Situations, No Time 15 10 23 0.94 196 Situations, No Time 1 40 0 0.83 20 Situations, No Time 8 3 45 0.79 206 Situations, No Time 17 10 42 0.88 207 Full, No Time 31 10 57 0.85 21 Full, Time 2 40 107 0.57 211 Situations, No Time 10 10 20 0.96 212 Situations, No Time 8 3 47 0.95 214 Situations, No Time 7 35 35 0.93 216 Full, No Time 24 10 56 0.83 22 Full, No Time 2 40 0 0.82 220 Full, No Time 2 40 0 0.91 25 Situations, No Time 1 40 0 0.75 26 Full, Time 2 40 0 0.85 27 Full, Time 37 10 97 0.89 30 Full, Time 37 10 32 0.92 31 Psychological, No Time 15 10 23 0.92 32 Situations, No Time 1 40 0 0.86 33 Situations, No Time 14 10 16 0.91 35 Full, No Time 30 10 65 0.82 38 Situations, No Time 15 10 45 0.64 40 Full, No Time 31 10 49 0.91 43 Full, No Time 32 10 72 0.90 49 Psychological, No Time 1 40 0 0.50 51 Psychological, No Time 8 35 0 0.91 52 Situations, No Time 14 10 2 0.91 53 Situations, No Time 15 10 31 0.93 61 Psychological, No Time 17 10 0 0.91 63 Full, No Time 33 10 38 0.91 66 Situations, No Time 16 10 25 0.91 67 Situations, No Time 15 10 32 0.69 71 Full, No Time 14 35 0 0.70 73 Full, No Time 2 40 0 0.92 74 Situations, No Time 15 10 20 0.92 77 Situations, No Time 15 10 30 0.90 78 Full, No Time 32 10 32 0.90 84 Situations, No Time 15 10 24 0.92 86 Full, No Time 31 10 57 0.93 6.1.4.3.1 Loneliness (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using BISCWIT ID Group # Items # Features Accuracy 08 Full, No Time 3 5 0.92 10 Psychological, No Time 3 4 0.91 103 Full, No Time 33 31 0.94 105 Psychological, No Time 3 4 0.80 106 Full, No Time 3 4 0.94 108 Psychological, No Time 3 4 0.80 112 Full, No Time 6 7 0.69 118 Psychological, Time 15 16 0.70 119 Full, No Time 15 16 0.93 123 Full, No Time 9 10 0.93 129 Situations, No Time 9 12 0.67 133 Full, No Time 36 34 0.90 135 Full, No Time 3 5 0.93 146 Psychological, No Time 3 4 0.95 149 Full, No Time 33 24 0.95 150 Full, No Time 3 4 0.96 152 Full, No Time 3 4 0.92 154 Full, No Time 36 35 0.94 155 Psychological, No Time 6 7 0.78 157 Full, No Time 3 4 0.90 158 Full, No Time 18 19 0.93 160 Full, No Time 3 4 0.83 164 Full, No Time 3 4 0.96 167 Full, No Time 3 4 0.91 168 Psychological, Time 9 10 0.77 170 Psychological, No Time 3 4 0.95 18 Full, No Time 12 13 0.95 185 Full, No Time 3 4 0.70 186 Full, No Time 24 21 0.93 196 Full, No Time 18 19 0.92 20 Full, No Time 3 4 0.93 201 Situations, No Time 3 4 0.82 206 Full, No Time 3 4 0.89 207 Psychological, No Time 3 4 0.86 211 Full, No Time 3 4 0.81 212 Full, No Time 6 7 0.95 214 Full, No Time 6 7 0.75 220 Full, No Time 6 7 0.92 25 Psychological, No Time 15 14 0.75 27 Psychological, Time 33 33 0.82 32 Full, No Time 3 4 0.93 35 Psychological, No Time 9 10 0.75 36 Full, No Time 3 4 0.92 43 Situations, No Time 12 13 0.91 51 Psychological, Time 24 25 0.75 53 Full, No Time 3 4 0.93 63 Full, No Time 15 16 0.91 71 Full, No Time 3 4 0.91 73 Full, No Time 6 7 0.93 84 Full, No Time 3 4 0.92 param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config)) %&gt;% pivot_wider(names_from = &quot;.metric&quot;, values_from = &quot;.estimate&quot;) %&gt;% select(-coefs, -set) %&gt;% unnest(params) %&gt;% select(-.config, -merror, -time) %&gt;% pivot_longer(cols = c(-(model:group)) , names_to = &quot;param&quot; , values_to = &quot;value&quot; , values_drop_na = T) %&gt;% group_by(model, outcome, group, param) %&gt;% summarize_at(vars(value), lst(mean, sd, min, max)) %&gt;% ungroup() %&gt;% mutate(mean = sprintf(&quot;%.2f (%.2f)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max)) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = c(&quot;outcome&quot;, &quot;group&quot;) , values_from = c(&quot;mean&quot;, &quot;range&quot;) , names_glue = &quot;{outcome}_{group}_{.value}&quot;) %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)) , param = factor(param, c(&quot;accuracy&quot;, &quot;roc_auc&quot;, &quot;penalty&quot;, &quot;mixture&quot;, &quot;nitem&quot;, &quot;min_n&quot;, &quot;mtry&quot;, &quot;nvars&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;, &quot;Penalty&quot;, &quot;Mixture&quot;, &quot;# Items&quot;, &quot;Min N Split&quot;, &quot;# Predictors Samples&quot;, &quot;# Features Selected&quot;))) %&gt;% arrange(model, param) ## # A tibble: 14 x 14 ## model param lonely_full_mean lonely_psychologic… lonely_situations… prcrst_full_mean prcrst_psychologi… prcrst_situation… lonely_full_ran… ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Elastic… Accuracy 0.89 (0.08) 0.84 (0.11) 0.79 (0.13) 0.84 (0.12) 0.78 (0.15) 0.78 (0.11) 0.65-0.96 ## 2 Elastic… AUC 0.73 (0.25) 0.75 (0.15) 0.73 (0.18) 0.74 (0.17) 0.62 (0.21) 0.70 (0.16) 0.08-0.93 ## 3 Elastic… Penalty 0.10 (0.29) 0.19 (0.39) 0.24 (0.42) 0.18 (0.38) 0.32 (0.46) 0.39 (0.48) 0.00-1.00 ## 4 Elastic… Mixture 0.05 (0.18) 0.08 (0.25) 0.05 (0.13) 0.08 (0.19) 0.08 (0.22) 0.13 (0.29) 0.00-1.00 ## 5 Elastic… # Feature… 47.82 (8.01) 30.19 (8.64) 28.18 (8.52) 46.74 (9.10) 29.28 (8.10) 24.53 (8.48) 19.00-65.00 ## 6 BISCWIT Accuracy 0.90 (0.07) 0.81 (0.08) 0.80 (0.12) 0.83 (0.11) 0.79 (0.14) 0.78 (0.12) 0.69-0.96 ## 7 BISCWIT AUC 0.77 (0.18) 0.70 (0.21) 0.69 (0.22) 0.79 (0.12) 0.69 (0.20) 0.67 (0.17) 0.42-0.97 ## 8 BISCWIT # Items 10.69 (11.03) 8.88 (7.52) 6.50 (4.90) 12.08 (11.58) 10.47 (8.34) 8.68 (7.13) 3.00-36.00 ## 9 BISCWIT # Feature… 11.18 (9.98) 9.44 (7.08) 7.42 (4.21) 12.28 (9.85) 10.35 (6.72) 8.78 (5.75) 4.00-35.00 ## 10 Random … Accuracy 0.84 (0.09) 0.83 (0.14) 0.91 (0.06) 0.83 (0.11) 0.82 (0.15) 0.84 (0.12) 0.65-0.93 ## 11 Random … AUC 0.82 (0.11) 0.79 (0.16) 0.70 (0.19) 0.73 (0.17) 0.64 (0.20) 0.74 (0.14) 0.50-0.95 ## 12 Random … Min N Spl… 16.91 (13.34) 19.18 (14.82) 13.66 (10.65) 16.84 (12.30) 17.21 (12.35) 17.16 (13.58) 3.00-40.00 ## 13 Random … # Predict… 25.73 (15.98) 12.18 (7.60) 14.71 (7.45) 28.02 (15.91) 16.52 (9.50) 12.78 (9.60) 2.00-75.00 ## 14 Random … # Feature… 58.82 (28.28) 38.88 (22.69) 28.09 (13.39) 53.12 (34.54) 35.28 (24.01) 32.35 (15.37) 0.00-106.00 ## # … with 5 more variables: lonely_psychological_range &lt;chr&gt;, lonely_situations_range &lt;chr&gt;, prcrst_full_range &lt;chr&gt;, ## # prcrst_psychological_range &lt;chr&gt;, prcrst_situations_range &lt;chr&gt; 6.2 Question 2: Are there individual differences in the idiographic range of prediction across people? Next, rather than grouping performance information by the feature sets, we’ll group the feature sets by participant, demonstrating the mean, standard deviation, median, and range for each person to answer the range of prediction across people. In the manuscript, we include a subset of this as a figure of a sample of 25 participants for each outcome. But below, we’ll create tables for each outcome, where each participant is a row to describe their results. 6.2.1 The Range of Prediction 6.2.1.1 Table px_sum_tab &lt;- function(d, outcome){ # clean up the outcome names o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) # create the caption cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Model Performance for Each Participant for %s&quot;, outcome) # create the span headers for the table h1 &lt;- c(1, rep(2, 6)); names(h1) &lt;- c(&quot; &quot;, rep(c(&quot;Accuracy&quot;, &quot;AUC&quot;), times = 3)) h2 &lt;- c(1, rep(4, 3)); names(h2) &lt;- c(&quot; &quot;, &quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) # call the kable table tab &lt;- d %&gt;% kable(. , &quot;html&quot; , col.names = c(&quot;ID&quot;, rep(c(&quot;M (SD)&quot;, &quot;Range&quot;), times = 6)) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 12)) , caption = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(h1) %&gt;% add_header_above(h2) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/04-participant-sum/%s.html&quot;, local_path, outcome)) return(tab) } # indexing the preferred column order ord &lt;- paste(rep(c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), each = 6) , rep(c(&quot;accuracy&quot;, &quot;roc_auc&quot;), each = 3, times = 3) , rep(c(&quot;mean&quot;, &quot;median&quot;, &quot;range&quot;), times = 6) , sep = &quot;_&quot;) px_tabs &lt;- sum_res %&gt;% unnest(data) %&gt;% group_by(SID, outcome, model, .metric) %&gt;% # summaries for each participant, outcome, model, and metric combinations summarize_at(vars(.estimate), lst(mean, median, sd, min, max), na.rm = T) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median)) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = c(&quot;model&quot;, &quot;.metric&quot;) , values_from = c(&quot;mean&quot;, &quot;median&quot;, &quot;range&quot;) , names_glue = &quot;{model}_{.metric}_{.value}&quot;) %&gt;% select(SID, outcome, ord) %&gt;% select(-contains(&quot;median&quot;)) %&gt;% group_by(outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = map2(data, outcome, px_sum_tab)) From the tables below, a few things become clear. First, for both accuracy and AUC, some individuals are very consistent in how accurately we can predict future procrastination or loneliness, regardless of what feature set is used (albeit low or high accuracy). Others, however, show a wide range, with a few participants even showing the full 0 - 1 range (e.g., Participant 135 for procrastination, 154 for loneliness) that is possible for classification accuracy and AUC. 6.2.1.1.1 Procrastination px_tabs$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;)## procrastination Table 6.4: Table SXDescriptive Statistics of Model Performance for Each Participant for prcrst Elastic Net BISCWIT Random Forest Accuracy AUC Accuracy AUC Accuracy AUC ID M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range 01 0.90 (0.04) 0.82-0.92 0.51 (0.20) 0.20-0.80 0.91 (&lt;.01) 0.91-0.92 0.66 (0.20) 0.45-1.00 0.89 (0.03) 0.83-0.91 0.43 (0.21) 0.18-0.80 02 0.78 (0.10) 0.54-0.85 0.40 (0.17) 0.18-0.82 0.75 (0.07) 0.62-0.85 0.64 (0.11) 0.45-0.86 0.83 (&lt;.01) 0.83-0.85 0.43 (0.12) 0.18-0.50 03 0.86 (0.09) 0.73-0.91 0.57 (0.28) 0.22-0.80 0.69 (0.11) 0.60-0.82 0.42 (0.35) 0.10-0.78 0.83 (0.15) 0.60-0.91 0.46 (0.16) 0.22-0.60 05 0.91 (0.02) 0.85-0.92 0.45 (0.30) 0.09-0.91 0.90 (0.05) 0.83-1.00 0.57 (0.28) 0.00-0.95 0.92 (0.02) 0.91-1.00 0.43 (0.25) 0.00-1.00 09 0.90 (0.03) 0.83-0.92 0.39 (0.32) 0.10-1.00 0.90 (0.03) 0.83-0.92 0.48 (0.20) 0.20-0.82 0.90 (&lt;.01) 0.89-0.91 0.26 (0.20) 0.06-0.67 10 0.78 (0.04) 0.75-0.82 0.39 (0.22) 0.06-0.69 0.75 (0.04) 0.70-0.80 0.54 (0.14) 0.37-0.81 103 0.62 (0.02) 0.56-0.69 0.54 (0.14) 0.33-0.78 0.61 (0.05) 0.50-0.69 0.51 (0.05) 0.42-0.60 0.62 (0.02) 0.57-0.64 0.62 (0.08) 0.49-0.72 105 0.85 (0.06) 0.73-0.92 0.59 (0.21) 0.17-0.92 0.82 (0.06) 0.73-0.92 0.31 (0.15) 0.12-0.69 0.86 (0.05) 0.75-0.92 0.58 (0.20) 0.33-1.00 106 0.80 (0.12) 0.50-0.88 0.61 (0.12) 0.43-0.79 0.74 (0.08) 0.56-0.81 0.41 (0.14) 0.21-0.79 0.79 (0.12) 0.56-0.94 0.62 (0.10) 0.42-0.79 107 0.88 (0.09) 0.67-0.93 0.60 (0.34) 0.00-1.00 0.92 (0.03) 0.87-0.93 0.34 (0.30) 0.00-0.93 0.93 (&lt;.01) 0.92-0.93 0.56 (0.27) 0.00-1.00 108 0.62 (0.04) 0.60-0.70 0.55 (0.12) 0.39-0.85 0.56 (0.09) 0.40-0.75 0.44 (0.16) 0.09-0.70 0.57 (0.01) 0.56-0.60 0.52 (0.14) 0.32-0.83 11 0.74 (0.12) 0.64-0.91 0.53 (0.45) 0.00-1.00 0.71 (0.09) 0.60-0.82 0.34 (0.31) 0.10-0.78 0.74 (0.31) 0.30-1.00 0.42 (0.12) 0.33-0.50 110 0.85 (0.10) 0.58-0.92 0.52 (0.28) 0.18-1.00 0.83 (0.07) 0.75-0.92 0.39 (0.28) 0.00-1.00 0.85 (0.10) 0.58-0.92 0.60 (0.22) 0.36-1.00 111 0.76 (0.16) 0.50-0.90 0.46 (0.31) 0.00-1.00 0.77 (0.10) 0.60-0.90 0.50 (0.32) 0.00-1.00 0.89 (0.03) 0.80-0.90 0.43 (0.20) 0.00-0.88 112 0.64 (0.12) 0.38-0.77 0.48 (0.21) 0.22-0.81 0.67 (0.07) 0.54-0.77 0.50 (0.27) 0.14-0.87 0.58 (0.20) 0.23-0.83 0.60 (0.14) 0.38-0.85 113 0.86 (0.05) 0.80-0.90 0.37 (0.18) 0.11-0.61 118 0.64 (0.08) 0.55-0.80 0.67 (0.16) 0.36-0.83 0.69 (0.06) 0.64-0.80 0.37 (0.14) 0.25-0.68 0.56 (0.16) 0.25-0.82 0.58 (0.21) 0.33-0.89 119 0.57 (0.04) 0.43-0.64 0.53 (0.12) 0.31-0.73 0.54 (0.07) 0.36-0.64 0.48 (0.12) 0.29-0.68 0.57 (&lt;.01) 0.57-0.57 0.46 (0.08) 0.27-0.54 121 0.92 (&lt;.01) 0.92-0.92 0.57 (0.44) 0.00-0.91 0.92 (&lt;.01) 0.92-0.92 0.56 (0.38) 0.09-1.00 0.90 (&lt;.01) 0.90-0.91 0.57 (0.33) 0.11-0.89 123 0.74 (0.11) 0.47-0.87 0.49 (0.15) 0.25-0.82 0.68 (0.11) 0.53-0.93 0.45 (0.16) 0.18-0.72 0.78 (0.03) 0.71-0.80 0.46 (0.24) 0.00-0.72 124 0.49 (0.11) 0.30-0.64 0.40 (0.11) 0.21-0.59 0.42 (0.08) 0.33-0.64 0.61 (0.09) 0.49-0.79 0.48 (0.11) 0.27-0.60 0.44 (0.13) 0.21-0.71 126 0.48 (0.10) 0.25-0.58 0.39 (0.18) 0.09-0.71 0.48 (0.10) 0.33-0.58 0.60 (0.14) 0.29-0.77 0.64 (0.08) 0.42-0.73 0.61 (0.12) 0.46-0.79 129 0.36 (0.09) 0.17-0.50 0.38 (0.14) 0.06-0.56 0.46 (0.18) 0.25-0.75 0.58 (0.22) 0.20-0.88 0.65 (0.15) 0.42-0.92 0.63 (0.23) 0.28-0.94 133 0.90 (&lt;.01) 0.90-0.90 0.56 (0.33) 0.11-1.00 0.91 (0.04) 0.90-1.00 0.45 (0.36) 0.00-0.89 0.90 (&lt;.01) 0.90-0.90 0.58 (0.40) 0.00-1.00 135 0.87 (0.13) 0.43-0.93 0.48 (0.32) 0.00-1.00 0.88 (0.05) 0.79-0.93 0.48 (0.30) 0.08-1.00 0.93 (0.02) 0.92-1.00 0.56 (0.27) 0.08-1.00 146 0.62 (0.05) 0.53-0.68 0.46 (0.13) 0.24-0.68 0.61 (0.05) 0.53-0.67 0.56 (0.13) 0.32-0.76 0.61 (0.06) 0.53-0.68 0.39 (0.16) 0.16-0.63 148 0.56 (0.08) 0.33-0.67 0.45 (0.12) 0.24-0.74 0.54 (0.10) 0.33-0.67 0.55 (0.13) 0.29-0.77 0.53 (0.09) 0.36-0.64 0.47 (0.14) 0.28-0.71 149 0.53 (0.08) 0.44-0.63 0.36 (0.14) 0.09-0.67 0.46 (0.12) 0.28-0.68 0.65 (0.13) 0.36-0.82 0.59 (0.05) 0.53-0.68 0.46 (0.16) 0.25-0.75 15 0.55 (0.07) 0.45-0.64 0.51 (0.12) 0.37-0.67 0.52 (0.14) 0.36-0.64 0.55 (0.16) 0.40-0.77 0.50 (&lt;.01) 0.50-0.50 0.49 (0.09) 0.36-0.56 150 0.70 (0.23) 0.20-0.88 0.56 (0.14) 0.32-0.73 0.67 (0.05) 0.60-0.76 0.48 (0.10) 0.30-0.65 0.78 (0.08) 0.62-0.83 0.74 (0.16) 0.35-0.89 152 0.87 (0.01) 0.83-0.88 0.42 (0.21) 0.16-0.77 0.88 (&lt;.01) 0.88-0.88 0.60 (0.16) 0.30-0.83 0.88 (&lt;.01) 0.87-0.88 0.65 (0.14) 0.37-0.86 155 0.57 (0.04) 0.56-0.67 0.55 (0.07) 0.50-0.71 0.60 (0.05) 0.50-0.67 0.45 (0.11) 0.27-0.58 0.55 (0.06) 0.47-0.67 0.56 (0.14) 0.32-0.79 156 0.48 (0.01) 0.48-0.52 0.48 (0.08) 0.36-0.61 0.48 (0.03) 0.43-0.52 0.53 (0.11) 0.37-0.71 0.49 (0.05) 0.45-0.57 0.58 (0.15) 0.32-0.79 157 0.90 (0.02) 0.85-0.90 0.34 (0.12) 0.11-0.58 0.90 (&lt;.01) 0.90-0.90 0.64 (0.09) 0.45-0.79 0.93 (0.02) 0.90-0.95 0.68 (0.22) 0.28-1.00 158 0.51 (0.16) 0.27-0.73 0.53 (0.14) 0.30-0.73 0.52 (0.10) 0.33-0.67 0.57 (0.14) 0.31-0.75 0.64 (0.15) 0.40-0.93 0.65 (0.25) 0.14-1.00 159 0.86 (0.08) 0.65-0.90 0.43 (0.11) 0.19-0.54 0.89 (0.02) 0.84-0.90 0.54 (0.14) 0.29-0.79 0.89 (&lt;.01) 0.88-0.90 0.43 (0.18) 0.21-0.72 160 0.89 (&lt;.01) 0.89-0.89 0.65 (0.23) 0.22-0.94 0.87 (0.03) 0.83-0.89 0.43 (0.29) 0.06-0.88 0.89 (&lt;.01) 0.88-0.89 0.71 (0.18) 0.28-0.94 162 0.90 (0.05) 0.77-0.93 0.50 (0.33) 0.08-1.00 0.90 (0.05) 0.77-0.93 0.58 (0.38) 0.04-1.00 0.95 (0.04) 0.90-1.00 0.63 (0.34) 0.11-1.00 164 0.96 (&lt;.01) 0.96-0.96 0.44 (0.34) 0.00-0.98 0.95 (&lt;.01) 0.95-0.96 0.67 (0.32) 0.09-1.00 165 0.61 (0.08) 0.36-0.64 0.54 (0.14) 0.29-0.71 0.61 (0.08) 0.45-0.73 0.46 (0.18) 0.25-0.71 0.61 (0.05) 0.56-0.70 0.65 (0.20) 0.50-1.00 166 0.67 (0.05) 0.58-0.75 0.71 (0.10) 0.47-0.81 0.60 (0.07) 0.42-0.67 0.38 (0.12) 0.20-0.59 0.70 (0.11) 0.55-0.83 0.64 (0.13) 0.43-0.80 167 0.58 (0.08) 0.45-0.73 0.64 (0.12) 0.30-0.77 0.56 (0.08) 0.36-0.64 0.40 (0.11) 0.23-0.60 0.61 (0.10) 0.44-0.78 0.51 (0.12) 0.22-0.67 168 0.57 (0.04) 0.54-0.62 0.51 (0.05) 0.43-0.66 0.51 (0.11) 0.31-0.62 0.54 (0.10) 0.40-0.70 0.69 (0.13) 0.50-0.92 0.74 (0.15) 0.50-0.90 169 0.94 (0.02) 0.89-0.95 0.51 (0.35) 0.06-0.94 0.94 (0.02) 0.89-0.95 0.56 (0.36) 0.00-1.00 0.95 (0.02) 0.93-1.00 0.67 (0.26) 0.14-0.93 17 0.40 (0.14) 0.23-0.64 0.58 (0.21) 0.14-0.88 0.47 (0.10) 0.31-0.62 0.50 (0.17) 0.20-0.75 0.46 (0.20) 0.15-0.77 0.57 (0.13) 0.33-0.78 170 0.81 (0.06) 0.75-0.94 0.78 (0.14) 0.50-0.97 0.79 (0.03) 0.75-0.85 0.25 (0.11) 0.13-0.50 0.68 (0.22) 0.17-0.83 0.67 (0.15) 0.39-0.86 171 0.96 (0.01) 0.92-0.96 0.51 (0.33) 0.00-1.00 0.96 (0.01) 0.92-0.96 0.58 (0.30) 0.12-0.91 0.96 (0.02) 0.95-1.00 0.55 (0.28) 0.19-1.00 174 0.96 (&lt;.01) 0.96-0.96 0.53 (0.30) 0.04-0.98 0.96 (0.01) 0.92-0.96 0.46 (0.30) 0.04-0.88 0.96 (0.02) 0.94-1.00 0.34 (0.38) 0.00-0.96 177 0.92 (&lt;.01) 0.92-0.92 0.52 (0.04) 0.50-0.58 0.83 (0.10) 0.69-0.92 0.31 (0.20) 0.17-0.58 0.94 (0.08) 0.83-1.00 0.52 (0.03) 0.50-0.55 185 0.94 (0.03) 0.84-0.95 0.52 (0.30) 0.00-1.00 0.92 (0.03) 0.89-0.95 0.50 (0.32) 0.00-0.89 0.95 (&lt;.01) 0.94-0.95 0.36 (0.27) 0.00-0.89 186 0.72 (0.04) 0.67-0.80 0.83 (0.09) 0.66-0.95 0.75 (0.04) 0.67-0.80 0.15 (0.11) 0.00-0.43 0.70 (0.02) 0.64-0.73 0.75 (0.17) 0.56-1.00 188 0.89 (0.05) 0.75-0.92 0.25 (0.19) 0.00-0.55 0.86 (0.05) 0.75-0.92 0.73 (0.17) 0.41-1.00 0.91 (0.02) 0.83-0.92 0.48 (0.11) 0.36-0.82 189 0.61 (0.21) 0.00-0.83 0.24 (0.14) 0.00-0.55 0.72 (0.09) 0.58-0.83 0.72 (0.21) 0.45-1.00 0.78 (0.05) 0.70-0.83 0.57 (0.16) 0.33-0.89 190 0.93 (&lt;.01) 0.93-0.94 0.44 (0.24) 0.06-0.93 0.89 (0.07) 0.71-0.94 0.62 (0.29) 0.07-1.00 0.91 (0.04) 0.82-0.94 0.47 (0.23) 0.00-0.86 192 0.88 (0.08) 0.69-0.92 0.49 (0.30) 0.08-1.00 0.92 (0.02) 0.85-0.92 0.46 (0.35) 0.00-1.00 0.92 (&lt;.01) 0.92-0.92 0.43 (0.23) 0.00-0.67 195 0.94 (&lt;.01) 0.94-0.94 0.58 (0.32) 0.19-1.00 0.94 (&lt;.01) 0.94-0.94 0.46 (0.31) 0.00-0.75 0.94 (0.03) 0.93-1.00 0.42 (0.40) 0.03-1.00 196 0.82 (0.04) 0.67-0.83 0.41 (0.17) 0.05-0.70 0.69 (0.12) 0.58-1.00 0.51 (0.22) 0.00-0.70 0.83 (0.03) 0.82-0.92 0.55 (0.11) 0.50-0.90 20 0.60 (0.18) 0.36-0.79 0.49 (0.17) 0.12-0.76 0.71 (0.06) 0.64-0.79 0.46 (0.24) 0.23-0.94 0.76 (0.04) 0.71-0.86 0.69 (0.18) 0.42-0.97 206 0.89 (&lt;.01) 0.89-0.89 0.30 (0.14) 0.12-0.56 0.89 (&lt;.01) 0.89-0.89 0.66 (0.16) 0.36-0.81 0.93 (0.06) 0.88-1.00 0.55 (0.22) 0.17-0.80 207 0.85 (&lt;.01) 0.85-0.86 0.38 (0.31) 0.00-0.96 0.85 (&lt;.01) 0.85-0.86 0.56 (0.35) 0.02-0.86 0.85 (&lt;.01) 0.85-0.85 0.80 (0.16) 0.50-1.00 21 0.52 (0.12) 0.20-0.67 0.49 (0.16) 0.20-0.72 0.57 (0.06) 0.47-0.67 0.42 (0.13) 0.22-0.70 0.57 (0.06) 0.47-0.71 0.51 (0.17) 0.33-0.83 211 0.96 (&lt;.01) 0.93-0.96 0.34 (0.26) 0.08-0.96 0.96 (&lt;.01) 0.96-0.96 0.34 (0.32) 0.04-0.88 0.98 (0.02) 0.96-1.00 0.66 (0.24) 0.36-1.00 212 0.93 (0.04) 0.86-0.95 0.53 (0.14) 0.28-0.80 0.92 (0.03) 0.86-0.95 0.35 (0.22) 0.00-0.74 0.95 (&lt;.01) 0.95-0.95 0.44 (0.34) 0.00-0.95 214 0.92 (0.04) 0.81-0.94 0.61 (0.26) 0.13-1.00 0.93 (0.02) 0.88-0.94 0.40 (0.29) 0.00-0.87 0.93 (0.02) 0.91-1.00 0.44 (0.33) 0.00-0.93 216 0.78 (&lt;.01) 0.78-0.78 0.44 (0.14) 0.25-0.68 0.78 (&lt;.01) 0.78-0.78 0.58 (0.13) 0.38-0.83 0.80 (0.03) 0.78-0.83 0.42 (0.12) 0.27-0.69 22 0.65 (0.07) 0.58-0.71 0.47 (0.08) 0.20-0.50 0.66 (0.11) 0.50-0.79 0.40 (0.14) 0.20-0.60 0.70 (0.07) 0.64-0.82 0.51 (0.04) 0.50-0.64 220 0.80 (0.13) 0.58-0.92 0.58 (0.27) 0.18-1.00 0.67 (0.13) 0.50-0.92 0.39 (0.31) 0.00-0.90 0.90 (0.02) 0.82-0.92 0.44 (0.21) 0.00-0.90 25 0.65 (0.12) 0.50-0.75 0.60 (0.13) 0.43-0.78 0.54 (0.11) 0.42-0.67 0.46 (0.15) 0.22-0.61 0.71 (0.06) 0.60-0.75 0.58 (0.14) 0.50-0.85 26 0.80 (0.01) 0.79-0.81 0.55 (0.09) 0.33-0.67 0.77 (0.03) 0.71-0.81 0.60 (0.09) 0.44-0.73 0.81 (0.02) 0.79-0.85 0.62 (0.13) 0.46-0.81 27 0.66 (0.13) 0.42-0.83 0.67 (0.17) 0.34-0.89 0.54 (0.21) 0.17-0.75 0.35 (0.28) 0.00-0.77 0.59 (0.18) 0.30-0.89 0.74 (0.16) 0.39-0.95 30 0.85 (0.10) 0.54-0.92 0.47 (0.28) 0.12-0.92 0.86 (0.05) 0.79-0.92 0.38 (0.28) 0.00-0.79 0.89 (0.04) 0.85-0.92 0.84 (0.27) 0.18-1.00 31 0.90 (0.05) 0.79-0.93 0.49 (0.36) 0.00-1.00 0.93 (&lt;.01) 0.93-0.93 0.53 (0.31) 0.00-1.00 0.92 (&lt;.01) 0.92-0.93 0.35 (0.21) 0.00-0.67 32 0.83 (0.07) 0.64-0.86 0.60 (0.16) 0.33-0.79 0.80 (0.05) 0.71-0.86 0.36 (0.16) 0.17-0.71 0.84 (0.03) 0.77-0.86 0.51 (0.06) 0.41-0.68 33 0.91 (&lt;.01) 0.91-0.91 0.69 (0.30) 0.10-1.00 0.90 (0.02) 0.82-0.91 0.32 (0.24) 0.00-0.85 0.92 (0.04) 0.89-1.00 0.72 (0.32) 0.00-1.00 35 0.73 (0.04) 0.67-0.75 0.54 (0.09) 0.44-0.67 0.68 (0.06) 0.58-0.75 0.42 (0.10) 0.28-0.54 0.80 (0.04) 0.73-0.82 0.56 (0.25) 0.11-0.89 38 0.58 (0.11) 0.36-0.73 0.38 (0.13) 0.15-0.62 0.53 (0.08) 0.45-0.67 0.59 (0.18) 0.15-0.80 0.52 (0.11) 0.30-0.64 0.43 (0.16) 0.08-0.65 40 0.90 (0.02) 0.82-0.91 0.51 (0.31) 0.00-0.90 0.90 (0.02) 0.82-0.91 0.55 (0.34) 0.05-1.00 0.91 (&lt;.01) 0.91-0.91 0.39 (0.31) 0.00-0.90 43 0.86 (0.10) 0.64-0.91 0.74 (0.21) 0.40-1.00 0.83 (0.08) 0.73-0.91 0.32 (0.19) 0.10-0.70 0.87 (0.08) 0.70-0.90 0.42 (0.33) 0.00-0.89 44 0.48 (0.11) 0.36-0.64 0.42 (0.15) 0.20-0.53 0.55 (0.07) 0.45-0.64 0.46 (0.15) 0.33-0.60 0.43 (0.03) 0.40-0.45 0.53 (0.07) 0.50-0.63 49 0.60 (0.07) 0.50-0.67 0.76 (0.10) 0.63-0.87 0.61 (0.05) 0.55-0.67 0.23 (0.11) 0.10-0.39 0.57 (0.08) 0.50-0.67 0.59 (0.12) 0.50-0.78 51 0.92 (0.02) 0.91-1.00 0.48 (0.25) 0.09-1.00 0.75 (0.12) 0.55-0.92 0.70 (0.29) 0.10-1.00 0.89 (0.09) 0.58-1.00 0.55 (0.16) 0.36-1.00 52 0.91 (&lt;.01) 0.91-0.91 0.56 (0.33) 0.00-1.00 0.91 (&lt;.01) 0.91-0.91 0.52 (0.37) 0.00-1.00 0.92 (0.04) 0.89-1.00 0.53 (0.30) 0.10-1.00 53 0.88 (0.11) 0.58-0.93 0.55 (0.29) 0.09-1.00 0.92 (0.04) 0.83-1.00 0.47 (0.35) 0.00-1.00 0.93 (0.02) 0.91-1.00 0.76 (0.27) 0.20-1.00 56 0.90 (&lt;.01) 0.90-0.91 0.33 (0.46) 0.00-1.00 0.90 (&lt;.01) 0.90-0.91 0.68 (0.46) 0.00-1.00 0.90 (&lt;.01) 0.90-0.91 0.40 (0.45) 0.00-1.00 61 0.91 (&lt;.01) 0.91-0.92 0.66 (0.37) 0.10-1.00 0.91 (&lt;.01) 0.91-0.92 0.55 (0.41) 0.00-1.00 0.91 (&lt;.01) 0.91-0.92 0.60 (0.31) 0.09-0.90 63 0.91 (&lt;.01) 0.91-0.91 0.46 (0.34) 0.00-1.00 0.90 (0.03) 0.82-0.91 0.58 (0.25) 0.00-0.90 0.90 (&lt;.01) 0.90-0.91 0.54 (0.24) 0.11-1.00 66 0.91 (&lt;.01) 0.91-0.91 0.49 (0.37) 0.00-1.00 0.89 (0.02) 0.88-0.91 0.42 (0.40) 0.00-1.00 67 0.62 (0.08) 0.36-0.64 0.23 (0.22) 0.00-0.60 0.64 (0.02) 0.57-0.64 0.73 (0.26) 0.42-1.00 0.65 (0.04) 0.62-0.69 0.39 (0.10) 0.22-0.50 71 0.73 (0.04) 0.64-0.82 0.76 (0.21) 0.42-1.00 0.78 (0.16) 0.55-1.00 0.41 (0.40) 0.00-1.00 0.70 (&lt;.01) 0.70-0.73 0.50 (0.21) 0.14-0.81 73 0.93 (&lt;.01) 0.92-0.93 0.22 (0.20) 0.00-0.62 0.92 (0.03) 0.85-0.93 0.70 (0.27) 0.25-1.00 0.92 (0.02) 0.85-0.92 0.56 (0.13) 0.50-0.92 74 0.93 (&lt;.01) 0.93-0.93 0.70 (0.33) 0.00-1.00 0.92 (&lt;.01) 0.92-0.92 0.45 (0.33) 0.04-0.92 77 0.86 (0.11) 0.60-0.90 0.46 (0.40) 0.00-1.00 0.90 (0.06) 0.80-1.00 0.52 (0.37) 0.00-0.89 0.88 (0.04) 0.80-0.90 0.51 (0.24) 0.11-0.71 78 0.91 (&lt;.01) 0.91-0.91 0.74 (0.36) 0.10-1.00 0.90 (&lt;.01) 0.90-0.90 0.85 (0.26) 0.44-1.00 81 0.39 (&lt;.01) 0.38-0.40 0.57 (0.19) 0.37-0.78 0.39 (&lt;.01) 0.38-0.40 0.45 (0.19) 0.26-0.62 0.42 (0.03) 0.38-0.46 0.45 (0.27) 0.20-0.80 84 0.92 (&lt;.01) 0.92-0.92 0.39 (0.24) 0.00-0.96 0.92 (&lt;.01) 0.92-0.92 0.63 (0.19) 0.12-0.88 0.92 (&lt;.01) 0.91-0.92 0.46 (0.33) 0.00-0.88 85 0.85 (&lt;.01) 0.85-0.85 0.47 (0.13) 0.27-0.66 86 0.81 (0.10) 0.69-0.92 0.32 (0.35) 0.00-0.92 0.92 (0.02) 0.85-0.93 0.44 (0.38) 0.00-1.00 0.93 (&lt;.01) 0.92-0.93 0.50 (0.29) 0.08-1.00 88 0.45 (0.24) 0.20-0.70 0.42 (0.29) 0.11-0.67 0.70 (0.12) 0.60-0.80 0.58 (0.29) 0.33-0.89 0.48 (0.30) 0.10-0.80 0.18 (0.14) 0.00-0.33 6.2.1.1.2 Loneliness px_tabs$tab[[2]] %&gt;% scroll_box(height = &quot;750px&quot;)## loneliness Table 6.4: Table SXDescriptive Statistics of Model Performance for Each Participant for lonely Elastic Net BISCWIT Random Forest Accuracy AUC Accuracy AUC Accuracy AUC ID M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range 03 0.90 (&lt;.01) 0.90-0.91 0.21 (0.10) 0.10-0.33 0.90 (&lt;.01) 0.90-0.91 0.69 (0.15) 0.56-0.90 0.90 (&lt;.01) 0.90-0.91 0.46 (0.05) 0.40-0.50 08 0.49 (0.22) 0.17-0.83 0.62 (0.24) 0.15-1.00 0.79 (0.11) 0.58-0.92 0.25 (0.14) 0.05-0.45 0.48 (0.31) 0.17-0.82 0.52 (0.14) 0.22-0.78 10 0.88 (0.10) 0.64-0.92 0.61 (0.26) 0.09-0.91 0.91 (&lt;.01) 0.91-0.92 0.43 (0.24) 0.18-0.82 0.90 (&lt;.01) 0.89-0.92 0.60 (0.32) 0.11-0.89 103 0.92 (0.03) 0.88-0.94 0.63 (0.27) 0.13-0.93 0.92 (0.04) 0.81-0.94 0.42 (0.24) 0.07-0.87 0.93 (&lt;.01) 0.93-0.94 0.59 (0.22) 0.27-0.92 105 0.74 (0.09) 0.47-0.82 0.59 (0.14) 0.33-0.83 0.79 (0.07) 0.62-0.88 0.42 (0.13) 0.23-0.75 0.83 (0.06) 0.73-0.92 0.73 (0.14) 0.50-0.95 106 0.93 (0.02) 0.88-0.94 0.54 (0.25) 0.20-1.00 0.92 (0.03) 0.88-0.94 0.51 (0.25) 0.07-0.97 0.93 (&lt;.01) 0.93-0.94 0.69 (0.29) 0.00-0.93 108 0.77 (0.10) 0.45-0.80 0.32 (0.14) 0.10-0.50 0.74 (0.07) 0.55-0.80 0.62 (0.11) 0.45-0.86 0.80 (0.01) 0.79-0.81 0.32 (0.10) 0.23-0.55 11 0.88 (0.04) 0.82-0.91 0.59 (0.29) 0.30-0.89 0.90 (&lt;.01) 0.90-0.91 0.75 (0.43) 0.11-1.00 0.95 (0.06) 0.89-1.00 0.71 (0.06) 0.67-0.75 112 0.68 (0.04) 0.54-0.69 0.46 (0.19) 0.17-0.72 0.69 (&lt;.01) 0.69-0.69 0.38 (0.24) 0.11-0.88 0.68 (0.01) 0.67-0.69 0.66 (0.12) 0.47-0.83 118 0.59 (0.21) 0.30-0.82 0.37 (0.16) 0.11-0.61 0.66 (0.08) 0.55-0.82 0.47 (0.19) 0.22-0.75 0.66 (0.11) 0.50-0.82 0.38 (0.24) 0.00-0.73 119 0.93 (&lt;.01) 0.93-0.93 0.46 (0.25) 0.08-0.85 0.93 (&lt;.01) 0.93-0.93 0.59 (0.26) 0.15-1.00 0.93 (&lt;.01) 0.93-0.93 0.49 (0.17) 0.23-0.92 121 0.90 (0.04) 0.83-0.92 0.32 (0.40) 0.09-0.91 0.88 (0.05) 0.83-0.92 0.78 (0.18) 0.64-1.00 0.93 (0.05) 0.90-1.00 0.24 (0.23) 0.10-0.50 123 0.91 (0.06) 0.73-1.00 0.76 (0.20) 0.43-1.00 0.93 (&lt;.01) 0.93-0.93 0.24 (0.25) 0.00-0.64 0.93 (&lt;.01) 0.93-0.93 0.71 (0.31) 0.15-1.00 129 0.60 (0.06) 0.50-0.75 0.76 (0.11) 0.49-0.94 0.60 (0.04) 0.58-0.67 0.27 (0.17) 0.06-0.66 0.59 (0.09) 0.50-0.83 0.81 (0.12) 0.60-0.97 133 0.87 (0.08) 0.70-0.90 0.52 (0.42) 0.00-1.00 0.90 (&lt;.01) 0.90-0.90 0.42 (0.33) 0.00-0.94 0.90 (&lt;.01) 0.90-0.90 0.56 (0.38) 0.11-1.00 135 0.88 (0.11) 0.64-0.93 0.55 (0.25) 0.15-0.92 0.93 (&lt;.01) 0.93-0.93 0.60 (0.24) 0.15-0.92 0.96 (0.04) 0.93-1.00 0.53 (0.15) 0.27-0.77 146 0.93 (0.02) 0.86-0.95 0.62 (0.27) 0.07-0.90 0.93 (0.01) 0.91-0.95 0.36 (0.19) 0.05-0.75 0.93 (0.01) 0.90-0.95 0.45 (0.30) 0.00-1.00 149 0.94 (0.01) 0.89-0.95 0.54 (0.19) 0.12-0.89 0.95 (&lt;.01) 0.94-0.95 0.61 (0.25) 0.22-0.94 0.94 (&lt;.01) 0.93-0.95 0.45 (0.28) 0.06-0.86 15 0.59 (0.09) 0.55-0.73 0.38 (0.21) 0.17-0.56 0.50 (0.12) 0.36-0.64 0.60 (0.14) 0.44-0.78 0.60 (0.22) 0.30-0.80 0.42 (0.19) 0.25-0.69 150 0.95 (0.02) 0.88-0.96 0.48 (0.27) 0.08-0.83 0.95 (0.01) 0.92-0.96 0.42 (0.27) 0.04-0.88 0.96 (&lt;.01) 0.95-0.96 0.86 (0.16) 0.48-1.00 152 0.92 (0.01) 0.92-0.96 0.70 (0.15) 0.48-0.91 0.92 (&lt;.01) 0.92-0.92 0.24 (0.15) 0.05-0.59 0.92 (&lt;.01) 0.91-0.92 0.77 (0.15) 0.45-0.96 154 0.87 (0.10) 0.71-1.00 0.63 (0.36) 0.00-1.00 0.93 (0.02) 0.88-0.94 0.46 (0.37) 0.03-1.00 0.94 (&lt;.01) 0.93-0.94 0.74 (0.22) 0.17-1.00 155 0.53 (0.11) 0.28-0.67 0.47 (0.09) 0.32-0.64 0.58 (0.12) 0.28-0.78 0.47 (0.09) 0.30-0.64 0.61 (0.06) 0.47-0.71 0.64 (0.13) 0.43-0.86 157 0.90 (0.01) 0.86-0.90 0.28 (0.18) 0.06-0.74 0.90 (&lt;.01) 0.90-0.90 0.77 (0.21) 0.21-1.00 0.90 (&lt;.01) 0.89-0.90 0.82 (0.19) 0.44-1.00 158 0.87 (0.11) 0.67-1.00 0.56 (0.39) 0.00-1.00 0.90 (0.06) 0.80-1.00 0.45 (0.39) 0.00-1.00 0.93 (&lt;.01) 0.93-0.93 0.72 (0.27) 0.15-1.00 160 0.83 (&lt;.01) 0.83-0.83 0.39 (0.26) 0.02-0.79 0.83 (&lt;.01) 0.83-0.83 0.54 (0.33) 0.08-1.00 0.83 (&lt;.01) 0.82-0.83 0.65 (0.15) 0.36-0.83 164 0.96 (&lt;.01) 0.96-0.96 0.53 (0.33) 0.00-1.00 0.96 (&lt;.01) 0.96-0.96 0.63 (0.30) 0.09-1.00 0.95 (&lt;.01) 0.95-0.96 0.44 (0.27) 0.05-1.00 167 0.91 (0.04) 0.82-1.00 0.67 (0.31) 0.20-1.00 0.90 (0.04) 0.82-1.00 0.39 (0.30) 0.00-0.90 0.93 (0.05) 0.89-1.00 0.56 (0.40) 0.00-1.00 168 0.52 (0.16) 0.31-0.77 0.59 (0.15) 0.35-0.81 0.65 (0.12) 0.46-0.85 0.31 (0.10) 0.17-0.47 0.66 (0.17) 0.38-0.92 0.86 (0.09) 0.75-1.00 170 0.93 (0.01) 0.92-0.95 0.80 (0.28) 0.22-1.00 0.94 (0.04) 0.88-1.00 0.21 (0.28) 0.00-0.80 0.92 (0.02) 0.88-0.94 0.64 (0.19) 0.38-0.94 18 0.95 (&lt;.01) 0.95-0.95 0.57 (0.29) 0.00-1.00 0.95 (&lt;.01) 0.95-0.95 0.52 (0.31) 0.00-0.94 0.94 (0.03) 0.88-1.00 0.46 (0.32) 0.06-1.00 185 0.64 (0.02) 0.58-0.65 0.37 (0.13) 0.17-0.52 0.61 (0.07) 0.47-0.70 0.57 (0.12) 0.41-0.77 0.63 (0.02) 0.61-0.65 0.51 (0.12) 0.29-0.73 186 0.92 (0.03) 0.87-0.93 0.46 (0.34) 0.00-1.00 0.93 (0.02) 0.87-0.93 0.43 (0.34) 0.00-1.00 0.93 (&lt;.01) 0.92-0.93 0.44 (0.37) 0.00-1.00 196 0.90 (0.05) 0.75-0.92 0.67 (0.25) 0.27-1.00 0.86 (0.09) 0.75-1.00 0.24 (0.26) 0.00-0.82 0.88 (0.05) 0.75-0.92 0.65 (0.16) 0.50-0.91 20 0.93 (&lt;.01) 0.93-0.93 0.15 (0.15) 0.00-0.46 0.93 (&lt;.01) 0.93-0.93 0.91 (0.12) 0.58-1.00 0.91 (0.13) 0.62-1.00 0.43 (0.09) 0.31-0.54 201 0.66 (0.15) 0.36-0.82 0.51 (0.18) 0.28-0.83 0.69 (0.15) 0.45-0.82 0.52 (0.23) 0.11-0.81 0.76 (0.17) 0.60-1.00 0.73 (0.28) 0.28-1.00 206 0.89 (&lt;.01) 0.89-0.89 0.54 (0.07) 0.44-0.69 0.88 (0.02) 0.83-0.89 0.44 (0.15) 0.28-0.72 0.85 (0.04) 0.81-0.88 0.43 (0.11) 0.27-0.64 207 0.76 (0.13) 0.46-0.86 0.71 (0.18) 0.38-0.95 0.85 (0.02) 0.79-0.86 0.42 (0.17) 0.18-0.71 0.85 (0.04) 0.77-0.92 0.68 (0.23) 0.27-1.00 211 0.79 (0.03) 0.69-0.81 0.38 (0.05) 0.27-0.46 0.80 (0.02) 0.77-0.81 0.65 (0.09) 0.50-0.82 0.77 (0.13) 0.35-0.83 0.39 (0.07) 0.28-0.57 212 0.95 (&lt;.01) 0.95-0.95 0.40 (0.26) 0.05-0.74 0.95 (&lt;.01) 0.95-0.95 0.58 (0.25) 0.00-0.95 0.96 (0.02) 0.95-1.00 0.54 (0.28) 0.08-0.94 214 0.75 (&lt;.01) 0.75-0.75 0.50 (0.10) 0.33-0.62 0.74 (0.02) 0.69-0.75 0.43 (0.05) 0.31-0.48 0.75 (0.02) 0.73-0.80 0.67 (0.14) 0.48-0.93 220 0.91 (0.03) 0.82-0.92 0.34 (0.29) 0.00-0.90 0.91 (&lt;.01) 0.91-0.92 0.61 (0.28) 0.09-1.00 0.91 (&lt;.01) 0.91-0.92 0.45 (0.21) 0.14-0.70 25 0.73 (0.06) 0.67-0.83 0.53 (0.19) 0.26-0.78 0.71 (0.08) 0.58-0.83 0.47 (0.15) 0.15-0.59 0.69 (0.06) 0.58-0.75 0.64 (0.09) 0.50-0.74 27 0.83 (0.03) 0.75-0.91 0.64 (0.15) 0.50-0.94 0.73 (0.08) 0.64-0.91 0.35 (0.21) 0.06-0.62 0.81 (0.02) 0.78-0.83 0.49 (0.03) 0.40-0.50 32 0.70 (0.29) 0.07-0.93 0.33 (0.31) 0.00-0.92 0.92 (0.03) 0.86-0.93 0.57 (0.30) 0.23-1.00 0.87 (0.07) 0.77-0.93 0.62 (0.27) 0.00-1.00 35 0.80 (0.11) 0.58-0.92 0.43 (0.33) 0.12-1.00 0.74 (0.11) 0.58-0.92 0.57 (0.30) 0.05-0.93 0.79 (0.05) 0.73-0.83 0.67 (0.15) 0.50-0.83 36 0.86 (0.10) 0.64-1.00 0.59 (0.30) 0.18-1.00 0.92 (0.02) 0.91-1.00 0.43 (0.35) 0.00-1.00 0.90 (0.04) 0.82-1.00 0.43 (0.34) 0.00-1.00 43 0.81 (0.06) 0.73-0.91 0.62 (0.21) 0.33-0.83 0.83 (0.06) 0.73-0.91 0.42 (0.31) 0.06-0.94 0.81 (0.04) 0.80-0.90 0.66 (0.15) 0.50-0.88 51 0.66 (0.12) 0.33-0.82 0.59 (0.18) 0.27-0.83 0.72 (0.12) 0.45-0.92 0.33 (0.18) 0.07-0.70 0.73 (0.06) 0.60-0.83 0.54 (0.09) 0.48-0.78 53 0.92 (0.03) 0.83-0.93 0.53 (0.29) 0.08-1.00 0.92 (&lt;.01) 0.92-0.93 0.46 (0.28) 0.00-0.77 0.93 (0.02) 0.91-1.00 0.53 (0.34) 0.00-0.93 58 0.92 (&lt;.01) 0.92-0.92 0.37 (0.23) 0.17-0.64 0.91 (&lt;.01) 0.91-0.92 0.50 (0.35) 0.14-0.91 63 0.90 (0.03) 0.82-0.91 0.51 (0.25) 0.20-0.90 0.90 (0.02) 0.82-0.91 0.53 (0.29) 0.10-0.90 0.93 (0.04) 0.90-1.00 0.73 (0.20) 0.40-1.00 67 0.86 (0.02) 0.86-0.93 0.37 (0.18) 0.15-0.62 68 0.89 (0.05) 0.82-0.91 0.42 (0.44) 0.00-0.90 0.91 (&lt;.01) 0.91-0.91 0.59 (0.34) 0.10-0.90 0.90 (&lt;.01) 0.90-0.91 0.46 (0.42) 0.00-1.00 71 0.91 (&lt;.01) 0.91-0.91 0.59 (0.39) 0.00-1.00 0.90 (0.05) 0.73-0.91 0.47 (0.37) 0.00-1.00 0.89 (0.04) 0.80-0.91 0.57 (0.25) 0.28-1.00 73 0.93 (&lt;.01) 0.92-0.93 0.56 (0.31) 0.15-1.00 0.93 (&lt;.01) 0.92-0.93 0.52 (0.32) 0.00-0.92 0.92 (&lt;.01) 0.92-0.92 0.85 (0.16) 0.50-1.00 84 0.92 (&lt;.01) 0.92-0.92 0.60 (0.19) 0.38-1.00 0.92 (&lt;.01) 0.92-0.92 0.29 (0.21) 0.08-0.71 0.92 (&lt;.01) 0.91-0.92 0.51 (0.34) 0.00-0.90 88 0.90 (&lt;.01) 0.90-0.90 0.42 (0.50) 0.00-1.00 0.90 (&lt;.01) 0.90-0.90 0.50 (0.28) 0.11-0.78 0.89 (0.01) 0.88-0.90 0.72 (0.26) 0.44-1.00 6.2.1.2 Figure (Figure 2) Now, we’ll create the figure that samples 25 participants ranges. We’ll create separate figures for each outcome (Procrastination, Loneliness) and metric (accuracy, AUC), which will be in Supplemental Materials (05-results/05/figures/01-px-sum-dist). Then, we’ll create a combined version for accuracy and both outcomes that will become Figure 2 in the manuscript. px_sum_plot &lt;- function(d, metric, outcome, model){ m &lt;- if(metric == &quot;accuracy&quot;) &quot;Accuracy&quot; else &quot;AUC&quot; mod &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;), warn_missing = F) # set.seed(6) d %&gt;% # filter(SID %in% sample(SID, 25)) %&gt;% mutate(SID = forcats::fct_reorder(SID, .estimate, median)) %&gt;% ggplot(aes(x = SID, y = .estimate)) + scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + stat_pointinterval() + labs(x = NULL, y = m, title = mod) + coord_flip() + # facet_grid(. ~ , scales = &quot;free&quot;, space = &quot;free&quot;) + theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;, size = rel(1.2), hjust = .5) , axis.text = element_text(face = &quot;bold&quot;, color = &quot;black&quot;) , axis.title = element_text(face = &quot;bold&quot;, size = rel(1.1)) , axis.line = element_blank() , panel.background = element_rect(color = &quot;black&quot;, size = 1) # , plot.margin = margin(1,.1,.1,.1, unit = &quot;cm&quot;) ) } combine_px_plots &lt;- function(d, outcome, metric){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) ttl &lt;- ggdraw() + draw_label( o, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) p1 &lt;- d$p[[1]] + labs(y = &quot;&quot;); p2 &lt;- d$p[[2]]; p3 &lt;- d$p[[3]] + labs(y = &quot;&quot;) p &lt;- cowplot::plot_grid(p1, p2, p3, nrow = 1, axis = &quot;b&quot;) p &lt;- plot_grid(ttl, p, nrow = 2, rel_heights = c(.05,.95)) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/01-px-sum-dist/%s_%s.pdf&quot; , local_path, outcome, metric) , width = 8 , height = 5) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/01-px-sum-dist/png/%s_%s.png&quot; , local_path, outcome, metric) , width = 8 , height = 5) return(p) } set.seed(8) px_plots_sum &lt;- sum_res %&gt;% unnest(data) %&gt;% group_by(outcome) %&gt;% filter(SID %in% sample(unique(SID), 25)) %&gt;% group_by(outcome, .metric, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;)) , p = pmap(list(data, .metric, outcome, model), px_sum_plot)) %&gt;% arrange(outcome, model, .metric) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = pmap(list(data, outcome, .metric), combine_px_plots)) p &lt;- cowplot::plot_grid(px_plots_sum$p[[3]], px_plots_sum$p[[1]] , nrow = 2) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-2-accuracy.pdf&quot; , local_path) , width = 8, height = 10) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-2-accuracy.png&quot; , local_path) , width = 8, height = 10) Figure 2 in the manuscript is the combination of the classification accuracy graphs below. Each of the figures below present the median, 66%, and 95% range of classification accuracy for a random sample of 25 participants, ordered by the median accuracy (AUC is available in the online materials and webapp [“Model Performance Distributions”]). As is clear in the figures, accuracy varies both across people and within them. In other words, although there are between-person differences in the degree of accuracy, there are also within-person differences, depending on which features are used. 6.2.1.2.1 Procrastination, Accuracy px_plots_sum$p[[3]] 6.2.1.2.2 Procrastination, AUC px_plots_sum$p[[4]] 6.2.1.2.3 Loneliness, Accuracy px_plots_sum$p[[1]] 6.2.1.2.4 Loneliness, AUC px_plots_sum$p[[2]] 6.3 Question 3: Do Psychological, Situational, or Full Feature Sets Perform Best? To answer the question of whether psychological, situational, or full feature sets (with or without time) perform best, we’ll pull the performance metric data we’ve been working with and combine it with information about specific coefficinets. 6.3.1 Psychological Features, Situations, or Time? 6.3.1.1 Table ord &lt;- paste(rep(c(&quot;lonely&quot;, &quot;prcrst&quot;), each = 6) , rep(c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), each = 2, times = 2) , rep(c(&quot;n&quot;, &quot;perc&quot;), times = 6) , sep = &quot;_&quot;) fps_tab &lt;- best_mods %&gt;% group_by(model, outcome, group, time, .metric) %&gt;% tally() %&gt;% group_by(model, outcome, .metric) %&gt;% mutate(perc = n/(sum(n, na.rm = T))*100 , perc = sprintf(&quot;%.1f%%&quot;,perc)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;outcome&quot;, &quot;model&quot;) , values_from = c(&quot;n&quot;, &quot;perc&quot;) , names_glue = &quot;{outcome}_{model}_{.value}&quot; , names_sort = T) %&gt;% mutate_all(~ifelse(is.na(.), &quot;0&quot;, .)) %&gt;% arrange(.metric, group, time) %&gt;% mutate(group = factor(str_to_title(group)) , time = factor(time, c(&quot;no time&quot;, &quot;time&quot;), c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% select(group, time, ord) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Set&quot;, &quot;Time&quot;, rep(c(&quot;#&quot;, &quot;%&quot;), times = 6)) , align = c(&quot;r&quot;, &quot;r&quot;, rep(&quot;c&quot;, 12)) , cap = &quot;&lt;strong&gt;Table 1&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Frequencies of the Full, Psychological, and Situation Feature Sets with or Without Time Being the Best Model for a Participant&lt;/em&gt;&quot; ) %&gt;% kable_styling(full_width = F) %&gt;% collapse_rows(1) %&gt;% kableExtra::group_rows(&quot;Accuracy&quot;, 1, 6) %&gt;% kableExtra::group_rows(&quot;AUC&quot;, 7, 12) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;Loneliness&quot; = 6, &quot;Procrastination&quot; = 6)) save_kable(fps_tab, file = sprintf(&quot;%s/05-results/04-tables/02-feature-perf-tab.html&quot;, local_path)) fps_tab Table 6.5: Table 1Frequencies of the Full, Psychological, and Situation Feature Sets with or Without Time Being the Best Model for a Participant Loneliness Procrastination Elastic Net BISCWIT Random Forest Elastic Net BISCWIT Random Forest Set Time # % # % # % # % # % # % Accuracy Full No 33 66.0% 34 66.7% 16 32.0% 49 59.8% 45 50.6% 27 31.0% Full Yes 4 8.0% 0 0 2 4.0% 7 8.5% 11 12.4% 7 8.0% Psychological No 6 12.0% 9 17.6% 4 8.0% 12 14.6% 15 16.9% 11 12.6% Psychological Yes 3 6.0% 4 7.8% 1 2.0% 6 7.3% 8 9.0% 2 2.3% Situations No 3 6.0% 4 7.8% 26 52.0% 6 7.3% 7 7.9% 38 43.7% Situations Yes 1 2.0% 0 0 1 2.0% 2 2.4% 3 3.4% 2 2.3% AUC Full No 7 14.0% 9 17.6% 13 26.0% 19 23.2% 19 21.3% 17 19.8% Full Yes 5 10.0% 3 5.9% 6 12.0% 7 8.5% 9 10.1% 8 9.3% Psychological No 11 22.0% 11 21.6% 9 18.0% 20 24.4% 16 18.0% 13 15.1% Psychological Yes 9 18.0% 4 7.8% 6 12.0% 8 9.8% 16 18.0% 10 11.6% Situations No 10 20.0% 15 29.4% 12 24.0% 19 23.2% 18 20.2% 27 31.4% Situations Yes 8 16.0% 9 17.6% 4 8.0% 9 11.0% 11 12.4% 11 12.8% Table 1 presents the number of and percentage of participants whose best model was for each feature set. As is clear, feature sets without time performed better than those with time. Second, relative to AUC, using accuracy as the selection metric was more likely to indicate that the full feature set performed best. Third, with some slight differences, relative proportions were similar across the three methods. Finally, for accuracy but not AUC, only RF indicated that situation feature models performed better than psychological feature models. We next examined the breakdown of selected features for each participant. 6.3.1.2 Figure (Figure 3) Next, to demonstrate the relative performance of feature sets and coefficients within those feature sets, we’ll create a series of sequence plots that show the proportion of features from each category for each participant. This will become Figure 3. seq_plot_fun &lt;- function(d, outcome, model){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name) m &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;) , c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) , warn_missing = F) ord &lt;- (d %&gt;% select(-n) %&gt;% pivot_wider(names_from = &quot;category&quot;, values_from = &quot;perc&quot;) %&gt;% arrange(desc(psychological)))$SID p &lt;- d %&gt;% mutate(SID = factor(SID, levels = ord), category = factor(category,rev(unique(category)) , str_to_title(rev(unique(category))))) %&gt;% ggplot(aes(x = SID , y = perc , rev = T) ) + scale_fill_manual(values = c(&quot;lightgoldenrod1&quot;, &quot;seagreen3&quot;, &quot;deepskyblue4&quot;)) + geom_bar(aes(fill = category) , stat = &quot;identity&quot; ) + labs(x = &quot;Participant ID&quot; , y = &quot;Percentage&quot; , fill = &quot;Feature Category&quot; , title = m) + coord_flip() + theme_classic() + theme(legend.position = &quot;bottom&quot; , axis.text.y = element_blank() , axis.ticks.y = element_blank() , axis.text.x = element_text(face = &quot;bold&quot;, size = rel(1.2), color = &quot;black&quot;) , axis.title = element_text(face = &quot;bold&quot;, size = rel(1.2)) , legend.text = element_text(face = &quot;bold&quot;) , legend.title = element_text(face = &quot;bold&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) ) return(p) } ## first get counts and percentages and create each plot for each outcome and model seq_plot &lt;- param_res %&gt;% right_join(best_mods %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(model:time)) %&gt;% select(-params) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;)))) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(coef != 0) %&gt;% left_join(ftrs %&gt;% select(category = group, Variable = old_name, new_name)) %&gt;% filter(!is.na(category)) %&gt;% group_by(SID, outcome, model, category) %&gt;% tally() %&gt;% group_by(SID, outcome, model) %&gt;% mutate(perc = n/sum(n)*100) %&gt;% group_by(outcome, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = pmap(list(data, outcome, model), seq_plot_fun)) # seq_plot &lt;- seq_plot %&gt;% # unnest(data) %&gt;% # group_by(outcome, SID, category) %&gt;% # summarize(perc = mean(perc, na.rm = T)) %&gt;% # ungroup() %&gt;% # mutate(model = &quot;combined&quot;) %&gt;% # group_by(outcome, model) %&gt;% # nest() %&gt;% # ungroup() %&gt;% # full_join(seq_plot) %&gt;% # mutate(p = pmap(list(data, outcome), seq_plot_fun)) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) legend &lt;- get_legend(seq_plot$p[[1]]) seq_plot &lt;- seq_plot %&gt;% mutate(p = map(p, ~(.) + theme(legend.position = &quot;none&quot;))) p1 &lt;- plot_grid( (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] + labs(y = NULL) , (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;biscwit&quot;))$p[[1]]+ labs(x = NULL, y = NULL) , (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;rf&quot;))$p[[1]]+ labs(x = NULL, y = NULL) , nrow = 1 , axis = &quot;b&quot; , align = &quot;hv&quot; ) ttl &lt;- ggdraw() + draw_label(&quot;Procrastination&quot;, fontface = title_theme$face) p1 &lt;- plot_grid(ttl, p1, nrow = 2, rel_heights = c(.05, .95)) p2 &lt;- plot_grid( (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] + labs(y = NULL, title = NULL) , (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;biscwit&quot;))$p[[1]]+ labs(x = NULL, title = NULL) , (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;rf&quot;))$p[[1]]+ labs(x = NULL, y = NULL, title = NULL) , nrow = 1 , axis = &quot;b&quot; , align = &quot;hv&quot; ) ttl &lt;- ggdraw() + draw_label(&quot;Loneliness&quot;, fontface = title_theme$face) p2 &lt;- plot_grid(ttl, p2, nrow = 2, rel_heights = c(.05, .95)) p &lt;- plot_grid(p1, p2, nrow = 2, rel_heights = c(.6, .4)) p &lt;- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p Figure 6.2: Figure 3. Sequence plots of the percentage of features from the Psychological, Situational, and Time Features Sets for each participant for each outcome. ggsave(p , file = sprintf(&quot;%s/05-results/05-figures/fig-3-seq-plot.pdf&quot;, local_path) , width = 8, height = 8) ggsave(p , file = sprintf(&quot;%s/05-results/05-figures/fig-3-seq-plot.png&quot;, local_path) , width = 8, height = 8) As is clear in Figure 3, which shows proportions of features for all participants’ best models for each method, there were individual differences in the proportion of psychological, situational, and time features. Some participants’ best models included exclusively psychological or situational features, with most showing a varying mixture of both. In addition, as should not be surprising given that Table 1 indicated that random forest was more likely to select the situation feature set as a participant’s best model, Figure 3 demonstrates the impact this has on the relative proportion of each type of feature for each outcome. 6.4 Question 4: Which features are most associated with Procrastination and Loneliness? 6.4.1 Feature Frequency: Psychological Features, Situations, or Time? To better understand which features were driving differences in which feature set produced the best model for each person, we next examined the variable importance metrics for each participant’s best models. To do so, we extracted the top five features and calculated the proportion of the sample that had each feature in their top five. Then, we created a figure that visually depicts those proportional frequencies of each feature for each outcome and model. 6.4.1.1 Figure (Figure 4) var_freq &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config) %&gt;% filter(.metric == &quot;accuracy&quot;) ) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;))) ) %&gt;% select(-params) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(coef != 0) %&gt;% # select(-.) %&gt;% group_by(model, SID, outcome) %&gt;% arrange(desc(abs(coef))) %&gt;% slice_max(abs(coef), n = 5) %&gt;% group_by(model, outcome) %&gt;% mutate(N = length(unique(SID))) %&gt;% group_by(model, outcome, Variable, N) %&gt;% tally() %&gt;% ungroup() %&gt;% mutate(n = n/N*100) %&gt;% right_join(crossing(model = c(&quot;biscwit&quot;, &quot;glmnet&quot;, &quot;rf&quot;), outcome = c(&quot;lonely&quot;, &quot;prcrst&quot;), ftrs %&gt;% select(group, Variable = old_name, new_name) %&gt;% mutate(group = str_to_title(group)) %&gt;% group_by(group) %&gt;% mutate(ni = 1:n(), ni = ifelse(ni &lt; 10, paste0(&quot;0&quot;, ni), ni), Variable2 = paste0(substr(group, 1, 1), ni) ) %&gt;% ungroup()) ) %&gt;% filter(!is.na(group)) %&gt;% distinct() p1 &lt;- var_freq %&gt;% filter(outcome == &quot;prcrst&quot;) %&gt;% mutate(model2 = as.numeric(mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), seq(5,1,-2))), # model2 = ifelse(outcome == &quot;prcrst&quot;, model2 + 1, model2), model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), new_name = factor(new_name, ftrs$new_name), outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %&gt;% arrange(model) %&gt;% ggplot(aes(x = Variable2 , y = model2 , group = factor(Variable2) , fill = model # , color = model # , shape = group , rev=F )) + geom_line(size = .2) + #keep this here, otherwise there is an error xlab(&quot;&quot;) + ylab(&quot;&quot;) + # Generate the grid lines geom_hline(yintercept = 1:7 , colour = &quot;grey80&quot; , size = .2) + geom_vline(xintercept = 1:67 , colour = &quot;grey80&quot; , size = .2) + # Points and lines geom_line(colour=&quot;grey80&quot;, size = .2) + geom_point(aes(size = n, alpha = n) , color = &quot;black&quot; , shape = 21) + # Fill the middle space with a white blank rectangle geom_rect(xmin=-Inf ,xmax=Inf ,ymin=-Inf ,ymax=0 ,fill=&quot;white&quot; , color=NA) + scale_y_continuous(limits=c(-5,5.5) , expand=c(0,0) , breaks=seq(1,5,2) , labels = NULL) + scale_size_continuous(range = c(.5,8) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 28) # 5 features , breaks = c(5, 10, 15, 20, 25)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_alpha_continuous(range = c(.3, 1) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 28) # 5 features , breaks = c(5, 10, 15, 20, 25)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Polar coordinates coord_polar() + # facet_wrap(~outcome, nrow = 2) + # The angle for the symptoms and remove the default grid lines theme_classic()+ theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = &quot;bold&quot;) , panel.border = element_blank() , axis.line = element_blank() , axis.ticks = element_blank() , panel.grid = element_blank() , panel.background = element_blank() , legend.position=&quot;bottom&quot; # , legend.position = &quot;none&quot; # legend.direction = &quot;vertical&quot;, , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = &quot;cm&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) , strip.background = element_blank() , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) ) + labs(size = &quot;% Participants&quot; # , fill = &quot;Model&quot; , alpha = &quot;% Participants&quot; , title = &quot;Procrastination&quot;) + guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , fill = &quot;none&quot;#guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5)) legend &lt;- cowplot::get_legend(p1) p1 &lt;- p1 + theme(legend.position = &quot;none&quot;) p1 &lt;- plot_grid(p1, legend, nrow = 2, rel_heights = c(.9, .1)) mx &lt;- max((var_freq %&gt;%filter(outcome == &quot;lonely&quot;))$n) p2 &lt;- var_freq %&gt;% filter(outcome == &quot;lonely&quot;) %&gt;% mutate(model2 = as.numeric(mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), seq(5,1,-2))), # model2 = ifelse(outcome == &quot;prcrst&quot;, model2 + 1, model2), model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), new_name = factor(new_name, ftrs$new_name), outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %&gt;% arrange(model) %&gt;% ggplot(aes(x = Variable2 , y = model2 , group = factor(Variable2) , fill = model # , color = model # , shape = group , rev=F )) + geom_line(size = .2) + #keep this here, otherwise there is an error xlab(&quot;&quot;) + ylab(&quot;&quot;) + # Generate the grid lines geom_hline(yintercept = 1:7 , colour = &quot;grey80&quot; , size = .2) + geom_vline(xintercept = 1:67 , colour = &quot;grey80&quot; , size = .2) + # Points and lines geom_line(colour=&quot;grey80&quot;, size = .2) + geom_point(aes(size = n, alpha = n) , color = &quot;black&quot; , shape = 21) + # Fill the middle space with a white blank rectangle geom_rect(xmin=-Inf ,xmax=Inf ,ymin=-Inf ,ymax=0 ,fill=&quot;white&quot; , color=NA) + scale_y_continuous(limits=c(-8,5.5) , expand=c(0,0) , breaks=seq(1,5,2) , labels = NULL) + scale_size_continuous(range = c(.5,8) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 35) # 5 features , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features , labels = c(&quot;5&quot;, &quot;12.5&quot;, &quot;20&quot;, &quot;27.5&quot;, &quot;35&quot;)) + # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_alpha_continuous(range = c(.3, 1) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 35) # 5 features , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features , labels = c(&quot;5&quot;, &quot;12.5&quot;, &quot;20&quot;, &quot;27.5&quot;, &quot;35&quot;)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Polar coordinates coord_polar() + # facet_wrap(~outcome, nrow = 2) + # The angle for the symptoms and remove the default grid lines theme_classic()+ theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = &quot;bold&quot;) , panel.border = element_blank() , axis.line = element_blank() , axis.ticks = element_blank() , panel.grid = element_blank() , panel.background = element_blank() , legend.position=&quot;bottom&quot; # legend.direction = &quot;vertical&quot;, , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = &quot;cm&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) , strip.background = element_blank() , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) ) + labs(size = &quot;% Participants&quot; , fill = &quot;Model&quot; , alpha = &quot;% Participants&quot; , title = &quot;Loneliness&quot;) + # guides(size = &quot;none&quot; # , fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0) # , alpha = &quot;none&quot; # , shape = &quot;none&quot;) guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, order = 1, label.hjust = 0.1) , fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0) , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1, label.hjust = 0.1) , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1, label.hjust = 0.1)) legend &lt;- cowplot::get_legend(p2) p2 &lt;- p2 + theme(legend.position = &quot;none&quot;) # guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1) # , fill = &quot;none&quot; # , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1) # , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1)) p3 &lt;- var_freq %&gt;% select(new_name, Variable2) %&gt;% distinct() %&gt;% mutate(new_name = factor(new_name, ftrs$new_name), names = paste0(Variable2, &quot;: &quot;, new_name)) %&gt;% arrange(Variable2) %&gt;% mutate(names = factor(names, .$names)) %&gt;% ggplot(aes(x = 1, y = 1:67)) + geom_text(aes(label = rev(names)), hjust = 0, size = 3) + scale_x_continuous(limits = c(.9999, 1.1))+ theme_classic() + theme(axis.line = element_blank() , axis.text = element_blank() , axis.ticks = element_blank() , axis.title = element_blank()) # p3 &lt;- plot_grid(p3, legend, nrow = 2, rel_heights = c(.95, .05)) + # theme(plot.margin = margin(.1,.5,.5,-1, unit = &quot;cm&quot;)) p &lt;- plot_grid(p1, p2, nrow = 2, rel_heights = c(.53, .47)) p &lt;- plot_grid(p, p3, nrow = 1, rel_widths = c(.65, .35)) p &lt;- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-4-combined_top5.pdf&quot;, local_path) , height = 12 , width = 9) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-4-combined_top5.png&quot;, local_path) , height = 12 , width = 9) The resulting Figure 4 has several takeaways. First, across models, timing features were less frequent, with the exception linear, quadratic, and cubic trends (T12-T14) across the ESM period. Second, for ENR and BISCWIT, psychological features were slightly more frequent than situation features. Third, one consequence of the higher frequency of situation feature RF models being selected than for the other two models, top five situation features were both more frequent as well as more variable (more different sized circles) for the RF models than for ENR or BISCWIT (more similarly sized circles). Finally, and perhaps most crucially, this figure makes clear that person and situation characteristics were both key in predicting each outcome, with neither “dominating” the feature space. It’s noteworthy that this figure depicts relative frequencies of each feature but says nothing about whether certain features were more or less likely to co-occur for each person. This is a question we will return to in Question 5. 6.5 Question 5: Do people vary in the which features are most important? Next we want to address not just general frequencies of important coefficients but also patterns of coefficients at the participant level. To do this, we’ll (1) make tables of all coefficients for each participant’s best model for all outcomes and models, (2) make a figure that displays this graphically, and (3) examine whether there are patterns of coefficients across people. 6.5.1 Participant Coefficients 6.5.1.1 Table First, let’s create tables for each participant and outcome combination of the coefficients from their models. As we’ve previously selected the feature set with the best performance, features from other sets will automatically be set to 0. In addition, as each of the models we used have feature selection procedures, features not chosen by the model will also be 0. px_coef_tab_fun &lt;- function(d, SID, outcome){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) mchar &lt;- d %&gt;% select(model, group, accuracy) %&gt;% distinct() %&gt;% mutate(model = mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), tmp = sprintf(&quot;%s: best model was %s with accuracy %.2f&quot;, model, group, accuracy)) note &lt;- paste(mchar$tmp, collapse = &quot;; &quot;); note &lt;- paste0(note, &quot;.&quot;) d2 &lt;- d %&gt;% select(-group, -accuracy) %&gt;% pivot_wider(names_from = &quot;model&quot; , values_from = &quot;coef&quot; , values_fn = mean) %&gt;% mutate_at(vars(-Variable), ~ifelse(abs(.) &gt; .01, sprintf(&quot;%.2f&quot;, .), ifelse(. == 0, &quot;0&quot;, ifelse(. &gt; -.01 &amp; . &lt; 0, &quot;&gt; -.01&quot;, &quot;&lt; .01&quot;)))) %&gt;% full_join(ftrs %&gt;% select(group, Variable = old_name, new_name)) %&gt;% filter(!is.na(group)) %&gt;% select(-Variable) %&gt;% mutate(new_name = factor(new_name, ftrs$new_name) , group = str_to_title(group)) %&gt;% mutate_at(vars(glmnet, biscwit, rf), ~ifelse(is.na(.), 0, .)) %&gt;% arrange(new_name) rs &lt;- d2 %&gt;% group_by(group) %&gt;% tally() %&gt;% mutate(end = cumsum(n), start = lag(end) + 1, start = ifelse(is.na(start), 1, start)) tab &lt;- d2 %&gt;% select(new_name, glmnet, biscwit, rf) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Variable&quot;, &quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 3)) , cap = sprintf(&quot;%s Model Coefficients for Participant %s&quot;, o, SID)) %&gt;% kable_styling(full_width = F) %&gt;% add_footnote(note, label = NULL) for (i in 1:nrow(rs)){ tab &lt;- tab %&gt;% kableExtra::group_rows(rs$group[i], rs$start[i], rs$end[i]) } save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/05-participant-coef/%s_%s.html&quot; , local_path, SID, outcome)) return(tab) } px_coef &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config) %&gt;% filter(.metric == &quot;accuracy&quot;) ) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;)))) %&gt;% select(-params) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;), group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) %&gt;% select(-set, -.metric, -time) %&gt;% rename(accuracy = .estimate) %&gt;% group_by(SID, outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, SID, outcome), possibly(px_coef_tab_fun, NA_real_))); px_coef ## # A tibble: 140 x 4 ## SID outcome data tab ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 01 prcrst &lt;tibble [149 × 5]&gt; &lt;kablExtr [1]&gt; ## 2 02 prcrst &lt;tibble [225 × 5]&gt; &lt;kablExtr [1]&gt; ## 3 05 prcrst &lt;tibble [143 × 5]&gt; &lt;kablExtr [1]&gt; ## 4 08 lonely &lt;tibble [187 × 5]&gt; &lt;kablExtr [1]&gt; ## 5 09 prcrst &lt;tibble [101 × 5]&gt; &lt;kablExtr [1]&gt; ## 6 10 lonely &lt;tibble [94 × 5]&gt; &lt;kablExtr [1]&gt; ## 7 103 lonely &lt;tibble [141 × 5]&gt; &lt;kablExtr [1]&gt; ## 8 103 prcrst &lt;tibble [166 × 5]&gt; &lt;kablExtr [1]&gt; ## 9 105 lonely &lt;tibble [97 × 5]&gt; &lt;kablExtr [1]&gt; ## 10 105 prcrst &lt;tibble [185 × 5]&gt; &lt;kablExtr [1]&gt; ## # … with 130 more rows All of these tables will be contained with the R shiny web app for readers to peruse at their leisure. An in detail description of each is a little beyond the goal and scope of the present study. However, the three sample participants from the manuscript are shown below. Notably (and by design), each of these participants differed in the feature set. Participant 169’s best model had the psychological feature set; Participant 43 had the situation feature set; and Participant 160 had the full feature set (psychological + situations). A few general notes on all these tables. ENR shows log odds coefficient weights, BISCWIT shows zero-order correlation weights, and Random Forest shows permutation-based variable importance metrics. Thus, these are not directly comparable, and observations like that the size of the coefficients tend to be smaller for biscwit are better thought of as relative. The direction magnitude relative to other coefficients / correlations / variable importance metrics tend to be similar across models relative to other variables in the same model. But due to differences in the estimation procedures of these models, they are not (and should not be expected to be) the same across models. 6.5.1.1.1 Participant 169 (px_coef %&gt;% filter(SID == &quot;169&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Procrastinating Model Coefficients for Participant 169 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0.12 0 &gt; -.01 Extraversion: Assertiveness -0.73 0 &lt; .01 Extraversion: Energy Level 0.68 0.12 &gt; -.01 Agreeableness: Compassion 0.50 0.13 &gt; -.01 Agreeableness: Respectfulness -0.15 0 &lt; .01 Agreeableness: Trust 0.15 0 &lt; .01 Conscientiousness: Organization 0.53 0 &gt; -.01 Conscientiousness: Productiveness -0.74 -0.17 &gt; -.01 Conscientiousness: Responsibility 0.62 0.15 &lt; .01 Neuroticism: Anxiety 0.13 0.12 &gt; -.01 Neuroticism: Depression -0.02 0 &gt; -.01 Neuroticism: Emotional Volatility 0.08 0 &lt; .01 Openness: Intellectual Curiosity -1.07 0 &gt; -.01 Openness: Aesthetic Sensitivity 0.14 0.15 &gt; -.01 Openness: Creative Imagination 1.80 0.29 &lt; .01 Negative: Angry -0.51 0 &lt; .01 Negative: Afraid 1.42 0.26 &lt; .01 Positive: Happy -0.38 0 &gt; -.01 Positive: Excited -0.54 0 &gt; -.01 Positive: Proud 0.33 0.11 &gt; -.01 Negative: Guilty 0.21 0.12 &lt; .01 Positive: Attentive -0.60 0 &gt; -.01 Positive: Content 0.33 0 &gt; -.01 Neutral: Purposeful 0.63 0 &gt; -.01 Neutral: Goal-directed 0.05 0 &lt; .01 Situations Duty 0 0 0 Intellect 0 0 0 Adversity 0 0 0 Mating 0 0 0 pOsitivity 0 0 0 Negativity 0 0 0 Deception 0 0 0 Sociality 0 0 0 Studying 0 0 0 Argument 0 0 0 Interacted 0 0 0 Lost something 0 0 0 Late 0 0 0 Forgot something 0 0 0 Bored with schoolwork 0 0 0 Excited about schoolwork 0 0 0 Anxious about schoolwork 0 0 0 Tired 0 0 0 Sick 0 0 0 Sleeping 0 0 0 In Class 0 0 0 Listening to music 0 0 0 On the internet 0 0 0 Watching TV 0 0 0 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.1.2 Participant 43 (px_coef %&gt;% filter(SID == &quot;43&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Lonely Model Coefficients for Participant 43 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0 0 0 Extraversion: Assertiveness 0 0 0 Extraversion: Energy Level 0 0 0 Agreeableness: Compassion 0 0 0 Agreeableness: Respectfulness 0 0 0 Agreeableness: Trust 0 0 0 Conscientiousness: Organization 0 0 0 Conscientiousness: Productiveness 0 0 0 Conscientiousness: Responsibility 0 0 0 Neuroticism: Anxiety 0 0 0 Neuroticism: Depression 0 0 0 Neuroticism: Emotional Volatility 0 0 0 Openness: Intellectual Curiosity 0 0 0 Openness: Aesthetic Sensitivity 0 0 0 Openness: Creative Imagination 0 0 0 Negative: Angry 0 0 0 Negative: Afraid 0 0 0 Positive: Happy 0 0 0 Positive: Excited 0 0 0 Positive: Proud 0 0 0 Negative: Guilty 0 0 0 Positive: Attentive 0 0 0 Positive: Content 0 0 0 Neutral: Purposeful 0 0 0 Neutral: Goal-directed 0 0 0 Situations Duty -5.45 -0.62 &lt; .01 Intellect 3.02 0 &gt; -.01 Adversity -0.37 -0.17 &lt; .01 Mating 0 0 0 pOsitivity 0.31 0 &gt; -.01 Negativity 0.40 -0.13 &gt; -.01 Deception 0 0 0 Sociality -1.86 -0.15 &lt; .01 Studying -0.49 -0.21 &gt; -.01 Argument -1.63 0 &lt; .01 Interacted -1.22 -0.45 &lt; .01 Lost something 0 0 &gt; -.01 Late 0.75 0.29 &lt; .01 Forgot something 0 -0.17 0 Bored with schoolwork 0.20 0 &gt; -.01 Excited about schoolwork -1.50 -0.20 &gt; -.01 Anxious about schoolwork 2.56 0 &gt; -.01 Tired 0 0 &gt; -.01 Sick 4.58 0.38 &lt; .01 Sleeping -1.58 0 &gt; -.01 In Class -3.03 -0.28 &lt; .01 Listening to music 0 0 &gt; -.01 On the internet -0.43 -0.25 &lt; .01 Watching TV -0.10 -0.20 &gt; -.01 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.1.3 Participant 160 (px_coef %&gt;% filter(SID == &quot;160&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Procrastinating Model Coefficients for Participant 160 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0.03 0 &lt; .01 Extraversion: Assertiveness 0.91 0.30 &gt; -.01 Extraversion: Energy Level -0.18 0 &gt; -.01 Agreeableness: Compassion 0.16 0 &gt; -.01 Agreeableness: Respectfulness 0.39 0.12 &gt; -.01 Agreeableness: Trust -0.39 0 &lt; .01 Conscientiousness: Organization 0.09 0 &gt; -.01 Conscientiousness: Productiveness -0.51 -0.17 &lt; .01 Conscientiousness: Responsibility 0.23 0 &lt; .01 Neuroticism: Anxiety 0.15 0 &gt; -.01 Neuroticism: Depression -0.17 -0.20 &lt; .01 Neuroticism: Emotional Volatility -0.13 -0.18 &lt; .01 Openness: Intellectual Curiosity 0.18 0 &gt; -.01 Openness: Aesthetic Sensitivity 0.76 0.28 &lt; .01 Openness: Creative Imagination 0.48 0 &lt; .01 Negative: Angry -0.19 0 &gt; -.01 Negative: Afraid -0.26 -0.12 &lt; .01 Positive: Happy -0.25 -0.13 &gt; -.01 Positive: Excited 0.11 0 &lt; .01 Positive: Proud -0.20 0 &lt; .01 Negative: Guilty 0.31 0 &lt; .01 Positive: Attentive -0.22 -0.27 &gt; -.01 Positive: Content 0.49 0 &gt; -.01 Neutral: Purposeful -0.06 0 &lt; .01 Neutral: Goal-directed -0.24 -0.16 &lt; .01 Situations Duty 0.29 0 &gt; -.01 Intellect -0.47 -0.25 &gt; -.01 Adversity 0 0 0 Mating -0.29 0 &gt; -.01 pOsitivity -0.07 0 &gt; -.01 Negativity -0.16 0 &lt; .01 Deception 0 0 0 Sociality -0.97 -0.42 &lt; .01 Studying -0.10 0 &lt; .01 Argument 0.34 0.17 &lt; .01 Interacted -0.05 -0.14 &lt; .01 Lost something -0.14 0 &gt; -.01 Late 0.25 0.12 &lt; .01 Forgot something 0.55 0.17 &gt; -.01 Bored with schoolwork 0 0 0 Excited about schoolwork -0.15 0 0 Anxious about schoolwork 0.11 0 &gt; -.01 Tired -0.06 0 &lt; .01 Sick -0.22 0 &gt; -.01 Sleeping 0.78 0.44 &lt; .01 In Class -0.14 -0.14 &lt; .01 Listening to music 0.13 0 &lt; .01 On the internet 0.27 0 &gt; -.01 Watching TV -0.47 -0.17 &lt; .01 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.2 Figure Now we’ll create some figures that display the same information but make relative comparisons within a model easier. coef_plot_fun &lt;- function(d, outcome, gr, SID, model){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) mod &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;), warn_missing = F) ttl &lt;- sprintf(&quot;Best %s Model (%s) Predicting \\n%s for Participant %s&quot;, mod, gr, o, SID) d &lt;- d %&gt;% mutate(neg = ifelse(sign(coef) == -1, &quot;(-)&quot;, NA), coef = abs(coef)) # Set a number of &#39;empty bar&#39; to add at the end of each group empty_bar &lt;- 4 to_add &lt;- data.frame(matrix(NA, empty_bar*nlevels(factor(d$category)), ncol(d)) ) colnames(to_add) &lt;- colnames(d) to_add$category &lt;- rep(levels(factor(d$category)), each=empty_bar) d &lt;- rbind(d, to_add) d &lt;- d %&gt;% arrange(category, desc(coef)) d$id &lt;- seq(1, nrow(d)) breaks &lt;- round(seq(0, max(d$coef, na.rm = T), length.out = 5),2) rng &lt;- c(-1*(breaks[5]-.01), breaks[5]+.01) # Get the name and the y position of each label label_data &lt;- d number_of_bar &lt;- nrow(label_data) angle &lt;- 90 - 360 * (label_data$id-0.5) /number_of_bar # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0) label_data$hjust &lt;- ifelse( angle &lt; -90, 1, 0) label_data$angle &lt;- ifelse(angle &lt; -90, angle+180, angle) label_data &lt;- label_data %&gt;% mutate(y = ifelse(is.na(coef) | coef &lt; 0, 0, coef + rng[2]/20) , lab = ifelse(!is.na(coef) &amp; coef &gt; 0, str_wrap(`short name`, 20), `short name`)) rng &lt;- c(round(-1*max(label_data$y),2)-.01, round(max(label_data$y),2)+.01) # prepare a data frame for base lines base_data &lt;- d %&gt;% group_by(category) %&gt;% summarize(start=min(id), end=max(id) - empty_bar) %&gt;% rowwise() %&gt;% mutate(title=mean(c(start, end))) breaks &lt;- breaks[1:4] # rng &lt;- c(-1*breaks[4], breaks[4]) # prepare a data frame for grid (scales) grid_data &lt;- base_data grid_data$end &lt;- grid_data$end[ c( nrow(grid_data), 1:nrow(grid_data)-1)] + 1 grid_data$start &lt;- grid_data$start - 1 grid_data &lt;- grid_data[-1,] # grid_data &lt;- grid_data %&gt;% crossing(breaks) p &lt;- d %&gt;% ggplot(aes( x = as.factor(id) , y = coef , fill = category )) + geom_bar(aes(na.rm = F), stat=&quot;identity&quot;, alpha=0.5) + scale_x_discrete(drop=FALSE) + scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Add a val=100/75/50/25 lines. I do it at the beginning to make sur barplots are OVER it. geom_segment(data = grid_data , aes(x = end, y = breaks[1], xend = start, yend = breaks[1]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[2], xend = start, yend = breaks[2]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[3], xend = start, yend = breaks[3]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[4], xend = start, yend = breaks[4]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + # # Add text showing the value of each 100/75/50/25 lines annotate(&quot;text&quot; , x = rep(max(d$id),4) , y = breaks , label = paste0(breaks, &quot;-&quot;) , color=&quot;grey&quot; , size=3 , angle=0 , fontface=&quot;bold&quot; , hjust=1) + ylim(rng[1], rng[2]) + theme_minimal() + theme( # legend.position = &quot;none&quot;, legend.position = &quot;bottom&quot;, axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank(), plot.title = element_text(hjust = .5, size = rel(1)) # plot.margin = unit(rep(-1,4), &quot;cm&quot;) ) + coord_polar() + labs(fill = &quot;Feature Category&quot;, title = ttl) + guides(fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0)) + geom_text(data = label_data , aes(x = id , y = y#coef , label = lab , hjust = hjust) , color=&quot;black&quot; , fontface=&quot;bold&quot; , lineheight = .6 , alpha=0.6 , size=2.5 , angle= label_data$angle , inherit.aes = FALSE) + geom_text(data = label_data , aes(x = id , y = coef - rng[2]/20 , label = neg) , color=&quot;black&quot; , hjust = .5 , fontface=&quot;bold&quot; , lineheight = .6 , size=2 , angle= label_data$angle , inherit.aes = FALSE) + # Add base line information geom_segment(data = base_data , aes(x = start , y = rng[1]/20 , xend = end , yend = rng[1]/20) , colour = &quot;black&quot; , alpha=0.8 , size=0.6 , inherit.aes = FALSE) ggsave(p, filename = sprintf(&quot;%s/05-results/05-figures/02-participant-coef/%s/%s_%s.pdf&quot; , local_path, model, SID, outcome) , width = 6, height = 8) ggsave(p, filename = sprintf(&quot;%s/05-results/05-figures/02-participant-coef/%s/png/%s_%s.png&quot; , local_path, model, SID, outcome) , width = 6, height = 8) return(p) } px_coef_fig &lt;- param_res %&gt;% right_join(best_mods %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(model:time)) %&gt;% select(-params) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;))) ) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;), group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) %&gt;% select(-set, -time) %&gt;% group_by(model, SID, outcome, group, Variable) %&gt;% summarize(coef = mean(coef)) %&gt;% ungroup() %&gt;% group_by(model, outcome, SID, group) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, ~(.) %&gt;% full_join(ftrs %&gt;% select(category = group, Variable = old_name, `short name`) ) %&gt;% mutate(coef = ifelse(coef == 0, NA_real_, coef) , category = factor(str_to_title(category)))), p = pmap(list(data, outcome, group, SID, model) , possibly(coef_plot_fun, NA_real_))) As before, we’ll show our three example participants. These figures are present in the manuscript as Figures 5-7. 6.5.1.2.1 Participant 169 (Figure 6) Participant 169’s best model for procrastination used the psychological feature set without time for each of the three methods (accuracy = 0.94; AUC = 0.80). Variable importance (log odds ratios) for the features in their ENR model are shown in the bar graph in Figure 5. Across all three methods, there were some differences selected features, but consensus in the direction and general magnitude of them. Across all three, the top feature was the Openness to Experience facet Creative Imagination, perhaps indicating that this participant tended to procrastinate when they were feeling more creative or imaginative previously. As in clear in Figure 5, they also tended to procrastinate less when they were Intellectually Curious (O) and more when they felt afraid. Thus, it seems like this participant’s procrastination may partially hinge upon competition between intellectual and creative pursuits, as well general fears. (px_coef_fig %&gt;% filter(outcome == &quot;prcrst&quot; &amp; SID == &quot;169&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] Figure 6.3: Figure 5. Variable Importance (absolute value of log odds) for Participant 169’s best model predicting Procrastination. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-)indicate negative effects (i.e. lower odds). ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-6-px-169.pdf&quot;, local_path) , width = 6, height = 8) 6.5.1.2.2 Participant 43 (Figure 7) participant 43’s best model for loneliness used the situation feature set (without time; accuracy = 0.91; AUC = 0.83). As in seen in the bar graph of their ENR variable importance in Figure 6, the situation characteristics and features seem to indicate that obligations (e.g., duty, in class), physical health (e.g., sick, sleeping), and social interactions (Sociability, argument) were predictive of future feelings of loneliness. For example, both the situation feature Duty and being in class predicted less loneliness, while feeling sick and getting in an argument predicted more. (px_coef_fig %&gt;% filter(outcome == &quot;lonely&quot; &amp; SID == &quot;43&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] Figure 6.4: Figure 6. Variable Importance (absolute value of log odds) for Participant 43’s best model predicting Loneliness. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-) indicates negative effects. ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-7-px-43.pdf&quot;, local_path) , width = 6, height = 8) 6.5.1.2.3 Participant 160 (Figure 8) Participant 160’s best models for procrastination utilized the full feature set (i.e. psychological and situational features) without time (see Figure 7; accuracy = 0.89, AUC = 0.94). ENR and BISCWIT agreed on the top three features: Sociability (DIAMONDS; negative), Sleeping (positive), and Depression (Neuroticism; negative) were each associated with future procrastination. Moreover, other related features, like attentiveness and Assertiveness (Extraversion), were also predictive of both outcomes. (px_coef_fig %&gt;% filter(outcome == &quot;prcrst&quot; &amp; SID == &quot;160&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] Figure 6.5: Figure 7. Variable Importance (absolute value of log odds) for Participant 160’s best model predicting Procrastination. Each bar is a different feature. Color indicates feature category (psychological, situational, or time). Height of bars indicates the magnitude of the effect. (-) indicates negative effects. ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-8-px-160.pdf&quot;, local_path) , width = 6, height = 8) 6.5.2 Profile Similarity Finally, when we looked at relative proportional frequencies of different features appearing in particiapnts’ top fives, it told us nothing about the tendency of features to co-occur. To look at this, we’ll opt for a simple visual depiction of the profile of coefficients/correlations/variable importance for each participant’s best model for each metric (accuracy, AUC), outcome (procrastination, loneliness), and model (ENR, BISCWIT, RF). procor_fun &lt;- function(d){ m &lt;- d %&gt;% select(-SID) %&gt;% mutate_all(~ifelse(is.na(.), 0, .)) %&gt;% as.matrix(); rownames(m) &lt;- d$SID r &lt;- cor(t(m)) diag(r) &lt;- NA rd &lt;- r %&gt;% data.frame() %&gt;% rownames_to_column(&quot;SID1&quot;) %&gt;% pivot_longer(cols = -SID1 , names_to = &quot;SID2&quot; , values_to = &quot;r&quot; , values_drop_na = T) %&gt;% mutate(SID2 = str_remove_all(SID2, &quot;X&quot;)) r %&gt;% mutate(r = fisherz(r)) %&gt;% group_by(SID1) %&gt;% summarize_at(vars(r), lst(mean, min, max)) } profile_sim &lt;- param_res %&gt;% select(-params) %&gt;% right_join(best_mods %&gt;% select(model:.metric)) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;))) ) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(Variable %in% ftrs$old_name) %&gt;% group_by(model, SID, outcome, .metric, Variable) %&gt;% summarize(coef = mean(coef)) %&gt;% ungroup() profile_plot_fun &lt;- function(d, model, metric, outcome){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) mod &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;), warn_missing = F) m &lt;- mapvalues(metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;), warn_missing = F) ttl &lt;- sprintf(&quot;%s Predicting Future %s Using Best %s Models&quot;, mod, o, m) min &lt;- if(model == &quot;glmnet&quot;) 5 else if (model == &quot;biscwit&quot;) 1 else max(abs(d$coef), na.rm = T) p &lt;- d %&gt;% mutate(coef = ifelse(coef == 0, NA, coef) , coef = ifelse(coef &gt; min, min, ifelse(coef &lt; -1*min, -1*min, coef)) , group = str_to_title(group) , new_name = factor(new_name, rev(ftrs$new_name))) %&gt;% drop_na() %&gt;% ggplot(aes(x = SID, y = new_name, color = coef)) + scale_color_gradient2(low = &quot;blue&quot; , mid = &quot;white&quot; , high = &quot;red&quot; # , limits = c(-5,5) ) + geom_point() + labs(x = &quot;Participant ID&quot;, y = NULL, color = &quot;Coefficient&quot; , title = ttl) + facet_grid(group ~ ., space = &quot;free&quot;, scale = &quot;free&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 90, face = &quot;bold&quot;) , axis.text.y = element_text(face = &quot;bold&quot;) , plot.title = element_text(hjust = .5, face = &quot;bold&quot;) , legend.position = &quot;bottom&quot; , strip.background = element_rect(fill = &quot;black&quot;) , strip.text = element_text(face = &quot;bold&quot;, color = &quot;white&quot;, size = rel(1.2)) ) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/03-px-profiles/%s_%s_%s.pdf&quot; , local_path, outcome, model, metric) , width = 12, height = 10) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/03-px-profiles/png/%s_%s_%s.png&quot; , local_path, outcome, model, metric) , width = 12, height = 10) return(p) } profile_sim_plots &lt;- profile_sim %&gt;% left_join(ftrs %&gt;% select(group, Variable = old_name, new_name)) %&gt;% group_by(model, .metric, outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = pmap(list(data, model, .metric, outcome), profile_plot_fun)) For parsimony, I’m just going to display the accuracy plots. You’ll find the others in the online materials and webapp. 6.5.2.1 Elastic Net Broadly there are a few takeaways from each of these figures. First, relative to other models (see below), ENR tended to include more features in any given model. However, likely due to the the combination of both soft and hard constraints via L1 and L2 regularization, the magnitudes were relatively low for some individuals than others. Second, only a relatively small number of participants’ best models had timing features. Third, even common features varied widely across people in presence, direction, and magnitude without exception. Finally, no two profiles are the same even just in which features were included, let alone in direction and magnitude of the associations. ##### Procrastination (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;glmnet&quot; &amp; outcome == &quot;prcrst&quot;))$p[[1]] 6.5.2.1.1 Loneliness (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;glmnet&quot; &amp; outcome == &quot;lonely&quot;))$p[[1]] 6.5.2.2 BISCWIT Broadly there are a few takeaways from each of these figures. First, the relative magnitude of the correlations tended to be stronger in the positive direction than the negative one. Second, only a relatively small number of participants’ best models had timing features. Third, even common features varied widely across people in presence, direction, and magnitude without exception. Finally, no two profiles are the same even just in which features were included, let alone in direction and magnitude of the associations. ##### Procrastination (Figure 5) (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;biscwit&quot; &amp; outcome == &quot;prcrst&quot;))$p[[1]] ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-5-px-coef-profiles.png&quot; , local_path) , width = 12, height = 10) 6.5.2.2.1 Loneliness (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;biscwit&quot; &amp; outcome == &quot;lonely&quot;))$p[[1]] 6.5.2.3 Random Forest 6.5.2.3.1 Procrastination (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;rf&quot; &amp; outcome == &quot;prcrst&quot;))$p[[1]] 6.5.2.3.2 Loneliness (profile_sim_plots %&gt;% filter(.metric == &quot;accuracy&quot; &amp; model == &quot;rf&quot; &amp; outcome == &quot;lonely&quot;))$p[[1]] "],["extended-discussion.html", "Chapter 7 Extended Discussion 7.1 On predicting more behaviors more of the time 7.2 The person situation debate revisited 7.3 Limitations and Conclusion", " Chapter 7 Extended Discussion The current study investigated personalized, idiographic prediction models for two behaviors, feeling lonely and procrastinating. Rather than assuming that antecedents of different outcomes were shared, our idiographic approach built N=1, personalized prediction models. Overall, three main conclusions emerged: First, psychological, situational, and time variables accurately predicted future everyday behaviors. Second, psychological and situational variables were both important, almost equally so, with neither being a predominant antecedent of behavior. Third, individual differences reigned supreme –people differed on how predictable outcomes were, which domains performed best, and which features were most important. These findings indicate the utility of an idiographic approach to psychological assessment relative to standard between-person approaches that are routinely used. 7.1 On predicting more behaviors more of the time We found accurate out-of-sample prediction of procrastination and feelings of loneliness when using a suite of personality and situational factors. While there are between-person individual differences in both loneliness and procrastination, there was also within-person variability in terms of how and when people experienced these behaviors. Typical prediction models within psychology have largely focused on which between-person features predict life outcomes or other aggregated behaviors (e.g., Beck &amp; Jackson, 2021a; Joel et al., 2020; Puterman et al., 2020). Here, in alignment with a growing emphasis on precision medicine approaches to improving physical health, well-being, and productivity, we demonstrate that within-person features are also predictable by psychological and situation features. These dynamic features tend to be less studied, which has resulted in little knowledge about why people vary within-person in these behaviors. Our findings suggest that from a fairly prescribed set of personality, situational, and time features, we can identify when someone is going to procrastinate or feel lonely at a future timepoint – not just if they tend to procrastinate or feel lonely in general. Notably, predictions were made assuming individuals have unique antecedents of each behavior. Although this equifinality is often described in theoretical models, it is rarely implemented in statistical models. Instead, statistical models use a circumscribed set of predictors that are assumed to impact people similarly, depending on their rank-order on the predictor (e.g., Borsboom et al., 2003). For example, procrastination is associated with Conscientiousness (Jackson et al., 2009). Typically, this suggests if people are feeling low in Conscientiousness markers (responsibility, organization) they would be more likely to procrastinate. However, we found that markers of Conscientiousness were not important antecedents of procrastinating for everyone, nor were they the most important in general (with 10-15% of the sample having Conscientiousness features as important predictors). People both procrastinate and feel lonely for many different reasons. As a result, prediction models that assume similar associations between predictors and outcomes for everyone may underestimate potential predictive validity. In general, we found individual differences in every aspect of the models – in accuracy, in feature sets, and in the importance of specific features. For some people, we could highly accurately predict future behaviors, while for others, we could not. Similarly, people differed in which and the degree to which the domains were important. Together these findings paint a picture of a psychological system that is highly unique to an individual. Although there is a longstanding consensus that behavior is the output of such highly unique dynamic psychological systems that are impacted by situational features (Mischel &amp; Shoda, 1995), these have remained elusive and often ignored in practice. Thus, the present study is an initial demonstrate of the empirical validity of such thinking. These participants demonstrated unique important situational and psychological features predicted future behavior. 7.2 The person situation debate revisited Half a century ago, the seeming limits of behavioral prediction that sparked the Person-Situation Debate and led to research being formulated around the question of whether person or situation features matter more. While most agree that both matter, there are few examples of demonstrating the joint importance of them for the same outcome (c.f., Sherman et al., 2015). We found evidence that person and situation features were both important for most individuals, with only a minority demonstrating that person or situation features alone were most predictive of future procrastination or loneliness. In other words, the Person-Situation Debate was always a false debate. The dynamic relations among person, situation, and behavior and indicate that attempts to understand behavior must incorporate both (Funder, 2006) – at least for most people. Not only are person and situation variables important, but they were also more important than time variables. Given that people have natural cycles of behavior that are regimented by time of day and day of week (Mathews, 1988; Larson, 1985), it would be natural to expect that behavior largely varies within and across people as a function of these cycles. For example, people work (behavior) less on the weekends and at night, which is a change their behavior. Similarly, time of day and day of week govern situations people can enter. Why were time variables not that important? It is likely that these time indices were already captured by the more proximal person or situational features. Time is likely important, but works through person and situation variables rather than being a separate factor. 7.3 Limitations and Conclusion This study is not without its limitations. First, relatively low variance in procrastination and loneliness led us to drop a number of participants from analyses. Thus, the participants in the present study are only representative of participants who experienced somewhat frequent loneliness and procrastination. Second, we examined prediction over a two week interval for most participants, so long-term prediction accuracy is unclear. Finally, we demonstrated high accuracy and AUC on average when predicting behavior four hours in the future, making it unclear how such models perform at different time intervals. However, given that processes unfold at different speeds both within- and between-person, model performance likely varies as a function of interval. The current study created personalized prediction models to help understand antecedents of future loneliness and procrastination. We found psychological and situational predictors did well in predicting within-person variations in these behaviors. However, in contrast to many years of methodological orthodoxy, the antecedents of these behaviors differed greatly across people. Thus, there is a need for more personalized assessments – not just longer assessments – but assessments that are tailored and important for the individual. Behavior appears to be highly predictable, so our next task is identifying personalized antecedents. "],["references.html", "References", " References "]]
